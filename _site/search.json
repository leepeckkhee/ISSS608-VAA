[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a Netlify site to store all my learnings and take-aways from ISS608-Visual Analytics and Applications"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\npacman::p_load(tidyverse)\n\n\n\n\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "In this chapter, you will be introduced to several ggplot2 extensions for creating more elegant and effective statistical graphics. By the end of this exercise, you will be able to:\n\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package,\ncreate professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages,\nplot composite figure by combining ggplot2 graphs by using patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "title": "Hands-on Exercise 2",
    "section": "2.2 Getting Started",
    "text": "2.2 Getting Started\n\n2.2.1 Install and loading the required libraries\nIn this exercise, beside tidyverse, four R packages will be used. They are:\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\nThe code chunk below uses p_load() of pacman package to check if ggrepel, patchwork, ggthemes, hrbrthemes and tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)\n\n\n\n2.2.2 Importing the data\nFor the purpose of this exercise, a data file called Exam_data will be used. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package. readr is one of the tidyverse package.\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThere are a total of seven attributes in the exam_data tibble data frame. Four of them are categorical data type and the other three are in continuous data type.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot-2-annotation-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot-2-annotation-ggrepel",
    "title": "Hands-on Exercise 2",
    "section": "2.3 Beyond ggplot 2 Annotation: ggrepel",
    "text": "2.3 Beyond ggplot 2 Annotation: ggrepel\nOne of the challenge in plotting statistical graph is annotation, especially with large number of data points.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nggrepel is an extension of ggplot2 package which provides geoms for ggplot2 to repel overlapping text as in our examples on the right.\nWe simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: ggrepel: 317 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 2",
    "section": "2.4 Beyond ggplot2 Themes",
    "text": "2.4 Beyond ggplot2 Themes\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n2.4.1 Working with ggtheme package\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others. In the example below, The Economist theme is used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n2.4.2 Working with hrbthemes package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#beyond-single-graph",
    "title": "Hands-on Exercise 2",
    "section": "2.5 Beyond Single Graph",
    "text": "2.5 Beyond Single Graph\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose figure with multiple graphs. In this section, we create composite plot by combining multiple graphs. First, we create three statistical graphics by using the code chunk below.\n\np1 <- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np2 <- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np3 <- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n2.5.1 Creating Composite Graphics: pathwork methods\nThere are several ggplot2 extension’s functions support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package. In this section, we will use a ggplot2 extension called patchwork which is specially designed for combining separate ggplot2 graphs into a single figure.\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\n\n2.5.2 Combining two ggplot2 graphs\nFigure in the tabset below shows a composite of two histograms created using patchwork.\n\np1 + p2\n\n\n\n\n\n\n2.5.3 Combining three ggplot2 graphs\nWe can plot more complex composite by using appropriate operators. For example, the composite figure below is plotted by using:\n\n“|” operator to stack two ggplot2 graphs,\n“/” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\n(p1 / p2) | p3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n2.5.4 Creating a composite figure with tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown in the figure below.\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n2.5.5 Creating figure with insert\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\np12 <- p1|p2\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n2.5.6 Creating a composite figure by using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\npatchwork <- (p1 / p2) | p3\npatchwork & theme_economist()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to create interactive data visualisation by using functions provided by ggiraph and plotlyr packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "title": "Hands-on Exercise 3",
    "section": "3.2 Getting started",
    "text": "3.2 Getting started\n\n3.2.1 Install and loading the required libraries\nFirst, write a code chunk to check, install and launch the following R packages:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\nThe code chunk below will be used to accomplish the task.\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse, readr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-data",
    "title": "Hands-on Exercise 3",
    "section": "3.3 Importing Data",
    "text": "3.3 Importing Data\nIn this section, Exam_data.csv provided will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on Exercise 3",
    "section": "3.4 Interactive Data Visualisation - ggiraph methods",
    "text": "3.4 Interactive Data Visualisation - ggiraph methods\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements. If it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\n\n3.4.1 Tooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page. By hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\n\n3.4.2 Displaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\n\nexam_data$tooltip <- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7. By hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\n\n3.4.3 Customising Tooltip style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css <- \"background-color:white; #<<\nfont-style:bold; color:black;\" #<<\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #<<\n    opts_tooltip(    #<<\n      css = tooltip_css)) #<<\n)                                       \n\n\n\n\n\n\n\n3.4.4 Displaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip <- function(y, ymax, accuracy = .01) {\n  mean <- scales::number(y, accuracy = accuracy)\n  sem <- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point <- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n3.4.5 Hover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id. Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. Note that the default value of the hover css is hover_css = “fill:orange;”.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                        \n\n\n\n\n\n\n\n3.4.6 Styling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)      \n\n\n\n\n\n\n\n3.4.7 Combining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below. Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)              \n\n\n\n\n\n\n\n3.4.8 Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick. Web document link with a data object will be displayed on the web browser upon mouse click.\n\nexam_data$onclick <- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                           \n\n\n\n\n\n\n\n3.4.9 Coordinated Multiple Views with ggiraph\nCoordinated multiple views methods has been implemented in the data visualisation below.\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point.\n\np1 <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 <- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---plotly-methods",
    "title": "Hands-on Exercise 3",
    "section": "3.5 Interactive Data Visualisation - plotly methods",
    "text": "3.5 Interactive Data Visualisation - plotly methods\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are: - by using plot_ly(), and - by using ggplotly()\n\n3.5.1 Creating an interactive scatter plot: plot_ly() method\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n3.5.2 Working with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n3.5.3 Creating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly(). Notice that the only extra line you need to include in the code chunk is ggplotly().\n\np <- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n3.5.4 Coordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data. highlight_key() simply creates an object of class crosstalk::SharedData.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nd <- highlight_key(exam_data)\np1 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Exercise 3",
    "section": "3.6 Interactive Data Visualisation - crosstalk methods",
    "text": "3.6 Interactive Data Visualisation - crosstalk methods\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n3.6.1 Interactive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n3.6.2 Linked brushing: crosstalk method\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!\n\n\nd <- highlight_key(exam_data) \np <- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nSetting the `off` event (i.e., 'plotly_deselect') to match the `on` event (i.e., 'plotly_selected'). You can change this default via the `highlight()` function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#programming-animated-statistical-graphics-with-r-overview",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#programming-animated-statistical-graphics-with-r-overview",
    "title": "Hands-on Exercise 3",
    "section": "4.1 Programming Animated Statistical Graphics with R Overview",
    "text": "4.1 Programming Animated Statistical Graphics with R Overview\nWhen telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, we will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, we will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\n4.1.1 Basic concepts of animation\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n4.1.2 Terminology\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, we can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started-1",
    "title": "Hands-on Exercise 3",
    "section": "4.2 Getting Started",
    "text": "4.2 Getting Started\n\n4.2.1 Loading the R packages\nFirst, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n4.2.2 Importing the data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nWrite a code chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol <- c(\"Country\", \"Continent\")\nglobalPop <- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %>%\n  mutate_each_(funs(factor(.)), col) %>%\n  mutate(Year = as.integer(Year))\n\nWarning: `mutate_each_()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Exercise 3",
    "section": "4.3 Animated Data Visualisation: gganimate methods",
    "text": "4.3 Animated Data Visualisation: gganimate methods\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_()/exit_() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n4.3.1 Building a static population bubble plot\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n4.3.2 Building the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#animated-data-visualisation-plotly",
    "title": "Hands-on Exercise 3",
    "section": "4.4 Animated Data Visualisation: plotly",
    "text": "4.4 Animated Data Visualisation: plotly\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n4.4.1 Building an animated bubble plot: ggplotly() method\nIn this sub-section, we will learn how to create an animated bubble plot by using ggplotly() method. ggplotly() is used to convert the R graphic object into an animated svg object.\n\ngg <- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nWarning in geom_point(aes(size = Population, frame = Year), alpha = 0.7, :\nIgnoring unknown aesthetics: frame\n\nggplotly(gg)\n\nWarning in p$x$data[firstFrame] <- p$x$frames[[1]]$data: number of items to\nreplace is not a multiple of replacement length\n\n\n\n\n\n\n\n\n4.4.2 Building an animated bubble plot: plot_ly() method\nIn this sub-section, we will learn how to create an animated bubble plot by using plot_ly() method.\n\nbp <- globalPop %>%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent, \n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          )\nbp\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values.\n\nWarning: `line.width` does not currently support multiple values."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "pacman::p_load(tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#plotting-a-horizontal-bar-chart-with-defined-theme-and-colour-scheme",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#plotting-a-horizontal-bar-chart-with-defined-theme-and-colour-scheme",
    "title": "In-class Exercise 1",
    "section": "Plotting a horizontal bar chart with defined theme and colour scheme",
    "text": "Plotting a horizontal bar chart with defined theme and colour scheme\n\nggplot(data = exam_data, aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"lightblue\", colour = \"lightblue\", \n                                    size = 0.5, linetype = \"solid\"),\n    panel.grid.major = element_line(size = 0.5, linetype = 'solid', colour = \"white\"), \n    panel.grid.minor = element_line(size = 0.25, linetype = 'solid', colour = \"white\"))\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#leveraging-on-fct_infreq-of-forcats-package",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#leveraging-on-fct_infreq-of-forcats-package",
    "title": "In-class Exercise 1",
    "section": "Leveraging on fct_infreq() of forcats package",
    "text": "Leveraging on fct_infreq() of forcats package\n\nexam_data %>%\n  mutate(RACE = fct_infreq(RACE)) %>%\n  ggplot(aes(x = RACE)) + \n  geom_bar()+\n  ylim(0,220) +\n  geom_text(stat=\"count\", \n      aes(label=paste0(..count.., \", \", \n      round(..count../sum(..count..)*100,\n            1), \"%\")),\n      vjust=-1) +\n  xlab(\"Race\") +\n  ylab(\"No. of\\nPupils\") +\n  theme(axis.title.y=element_text(angle = 0))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#adding-mean-and-median-lines-in-red-and-grey-respectively-on-the-histogram-plot",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#adding-mean-and-median-lines-in-red-and-grey-respectively-on-the-histogram-plot",
    "title": "In-class Exercise 1",
    "section": "Adding mean and median lines in red and grey respectively on the histogram plot",
    "text": "Adding mean and median lines in red and grey respectively on the histogram plot\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(MATHS, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=1) +\n  geom_vline(aes(xintercept=median(MATHS, na.rm=T)),\n             color=\"grey30\",\n             linetype=\"dashed\", \n             size=1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#displaying-a-background-histogram-showing-the-distribution-of-english-scores-for-all-pupils",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#displaying-a-background-histogram-showing-the-distribution-of-english-scores-for-all-pupils",
    "title": "In-class Exercise 1",
    "section": "Displaying a background histogram showing the distribution of English scores for all pupils",
    "text": "Displaying a background histogram showing the distribution of English scores for all pupils\n\nd <- exam_data   \nd_bg <- d[, -3]  \n\nggplot(d, aes(x = ENGLISH, fill = GENDER)) +\n  geom_histogram(data = d_bg, fill = \"grey\", alpha = .5) +\n  geom_histogram(colour = \"black\") +\n  facet_wrap(~ GENDER) +\n  guides(fill = FALSE) +  \n  theme_bw()\n\nWarning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications. In this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "pacman:: p_load(rstatix, gt, patchwork, tidyverse)\n\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\n\nVisualizing Normal Distribution\n\nA Q-Q plot, short for “quantile-quantile” plot, is used to assess whether or not a set of data potentially came from some theoretical distribution. In most cases, this type of plot is used to determine whether or not a data set follows a normal distribution.\nIf the data is normally distributied, the points in a Q-Q plot will lie on a straight diagonal line. Conversely, if the points deviate significantly from the straight diagonal line, then it’s less likely that the data is normally distributed.\n\n\nggplot(exam_data,\n       aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\nIn the section below, we present the information in via panel tabset to show two main tabs: “The plot” and “The code chunk”.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, \n       aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the points deviate significantly from the straight diagonal line. This is a clear indication that the set of data is not normally distributed.\n\n\n\n\nCombining statistical graph and analysis table\nIn the section below, we combine the statistical graph and analysis table into a single view for easy visualization.\n\nThe plotThe code chunk\n\n\n\n\n\n\n\n\n\n\nqq <- ggplot(exam_data,\n             aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line()\n\nsw_t <- exam_data %>%\n  shapiro_test(ENGLISH) %>%\n  gt()\n\ntmp <- tempfile(fileext = '.png')\ngtsave(sw_t,tmp)\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq + table_png"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The task is to apply the concepts and methods learned in Lesson 1-4 to reveal the demographic and financial characteristics of the city of Engagement by using appropriate static and interactive statistical graphics methods. This exercise requires a user-friendly and interactive solution that helps city managers and planners to explore the complex data in an engaging way and reveal hidden patterns."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#the-plot",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#the-plot",
    "title": "In-class Exercise 4",
    "section": "The plot",
    "text": "The plot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#the-code-chunk",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#the-code-chunk",
    "title": "In-class Exercise 4",
    "section": "The code chunk",
    "text": "The code chunk\n\nggplot(exam_data, \n       aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#the-plot-1",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#the-plot-1",
    "title": "In-class Exercise 4",
    "section": "The plot",
    "text": "The plot\n#| echo:false qq <- ggplot(exam_data, aes(sample=ENGLISH)) + stat_qq() + stat_qq_line()\nsw_t <- exam_data %>% shapiro_test(ENGLISH) %>% gt()\ntmp <- tempfile(fileext = ‘.png’) gtsave(sw_t,tmp) table_png <- png:: readPNG(tmp, native = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "In this hands-on exercise, we will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Hands-on Exercise 4",
    "section": "4.2 Visual Statistical Analysis with ggstatsplot",
    "text": "4.2 Visual Statistical Analysis with ggstatsplot\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves. - To provide alternative statistical inference methods by default. - To follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "title": "Hands-on Exercise 4",
    "section": "4.3 Getting started",
    "text": "4.3 Getting started\n\n4.3.1 Installing and launching R packages\nIn this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n4.3.2 Importing data\n\nexam <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n4.3.3 One-sample test: gghistostats() method\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n4.3.4 Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n4.3.5 How to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n4.3.6 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n4.3.7 Oneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n4.3.7.1 ggbetweenstats - Summary of tests\n\n\n\n\n\n\n\n\n\n\n4.3.8 Significant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n4.3.9 Significant Test of Association (Depedence) : ggbarstats() methods\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 <- exam %>% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#unpacking-the-bayes-factor",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#unpacking-the-bayes-factor",
    "title": "Hands-on Exercise 4",
    "section": "4.3.4 Unpacking the Bayes Factor",
    "text": "4.3.4 Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#how-to-interpret-bayes-factor",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#how-to-interpret-bayes-factor",
    "title": "Hands-on Exercise 4",
    "section": "4.3.5 How to interpret Bayes Factor",
    "text": "4.3.5 How to interpret Bayes Factor"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-models",
    "title": "Hands-on Exercise 4",
    "section": "4.4 visualising Models",
    "text": "4.4 visualising Models\nIn this section, we will learn how to visualize model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started-1",
    "title": "Hands-on Exercise 4",
    "section": "4.5 Getting Started",
    "text": "4.5 Getting Started"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 4",
    "section": "4.6 Installing and loading the required libraries",
    "text": "4.6 Installing and loading the required libraries\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n4.6.1 Importing Excel file: readxl methods\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale <- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   <dbl> <chr>    <dbl>     <dbl>     <dbl>    <dbl>  <dbl>         <dbl>  <dbl>\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period <dbl>, HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>, …\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\n4.6.2 Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n4.6.3 Model Diagnostic: checking for multicolinearity:\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c <- check_collinearity(model)\nplot(check_c)\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n4.6.4 Model Diagnostic: checking normality assumption\nIn the code chunk, check_normality() of performance package.\n\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n <- check_normality(model1)\n\n\nplot(check_n)\n\n\n\n\n\n\n4.6.5 Model Diagnostic: Check model for homogeneity of variances\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h <- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\n\n\n4.6.6 Model Diagnostic: Complete check\nWe can also perform the complete check by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n4.6.7 Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n4.6.8 Visualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome",
    "title": "Hands-on Exercise 4",
    "section": "4.1 Learning Outcome",
    "text": "4.1 Learning Outcome\nIn this hands-on exercise, we will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#b.-visualizing-uncertainty",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#b.-visualizing-uncertainty",
    "title": "Hands-on Exercise 4",
    "section": "4b. Visualizing Uncertainty",
    "text": "4b. Visualizing Uncertainty"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome-1",
    "title": "Hands-on Exercise 4",
    "section": "4.1 Learning Outcome",
    "text": "4.1 Learning Outcome"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates",
    "title": "Hands-on Exercise 4",
    "section": "4.2 Visualizing the uncertainty of point estimates",
    "text": "4.2 Visualizing the uncertainty of point estimates\n\nA point estimate is a single number, such as a mean.\nUncertainty is expressed as standard error, confidence interval, or credible interval\nImportant: Don’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\npacman::p_load(tidyverse, plotly, crosstalk, DT, ggdist, gganimate)\n\n\nexam <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n4.2.1 Visualizing the uncertainty of point estimates: ggplot2 methods\nThe code chunk below performs the followings:\n\ngroup the observation by RACE,\ncomputes the count of observations, mean, standard deviation and standard error of Maths by RACE, and\nsave the output as a tibble data table called my_sum.\n\n\nmy_sum <- exam %>%\n  group_by(RACE) %>%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n \n  \n    RACE \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    Chinese \n    193 \n    76.50777 \n    15.69040 \n    1.132357 \n  \n  \n    Indian \n    12 \n    60.66667 \n    23.35237 \n    7.041005 \n  \n  \n    Malay \n    108 \n    57.44444 \n    21.13478 \n    2.043177 \n  \n  \n    Others \n    9 \n    69.66667 \n    10.72381 \n    3.791438 \n  \n\n\n\n\n\n\n\n4.2.2 Visualizing the uncertainty of point estimates: ggplot2 methods\nThe code chunk below is used to reveal the standard error of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean \n          maths score by rac\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n4.2.3 Visualizing the uncertainty of point estimates: ggplot2 methods\n\n\n\n\n\n\nNote\n\n\n\nPlot the 95% confidence interval of mean maths score by race. The error bars should be sorted by the average maths scores.\n\n\n\n\n\n\n\n\n\n4.2.4 Visualizing the uncertainty of point estimates with interactive error bars\n\n\n\n\n\n\nNote\n\n\n\nPlot interactive error bars for the 99% confidence interval of mean maths score by race.\n\n\n\n\nWarning in geom_point(aes(x = RACE, y = mean, text = paste(\"Race:\", RACE, :\nIgnoring unknown aesthetics: text"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-ggdist-package",
    "title": "Hands-on Exercise 4",
    "section": "4.3 Visualising Uncertainty: ggdist package",
    "text": "4.3 Visualising Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\n4.3.1 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +   #<<\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n4.3.2 Visualizing the uncertainty of point estimates: ggdist methods\nThe below chart shows the confidence interval.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n4.3.3 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\nWarning: fill_type = \"gradient\" is not supported by the current graphics device.\n - Falling back to fill_type = \"segments\".\n - If you believe your current graphics device *does* support\n   fill_type = \"gradient\" but auto-detection failed, set that option\n   explicitly and consider reporting a bug.\n - See help(\"geom_slabinterval\") for more information."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4",
    "section": "4.4 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "4.4 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\nStep 1: Installing ungeviz package\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nSkipping install of 'ungeviz' from a github remote, the SHA1 (aeae12b0) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\nStep 2: Launch the application in R\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\nWarning in geom_hpline(data = sampler(25, group = RACE), height = 0.6, color =\n\"#D55E00\"): Ignoring unknown parameters: `height`\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "title": "Hands-on Exercise 4",
    "section": "4.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "4.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)\n\nWarning in geom_hpline(data = sampler(25, group = RACE), height = 0.6, color =\n\"#D55E00\"): Ignoring unknown parameters: `height`"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#c.-funnel-plots-for-fair-comparisons",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#c.-funnel-plots-for-fair-comparisons",
    "title": "Hands-on Exercise 4",
    "section": "4c. Funnel Plots for Fair Comparisons",
    "text": "4c. Funnel Plots for Fair Comparisons"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "title": "Hands-on Exercise 4",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nFunnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-launching-r-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-launching-r-packages-1",
    "title": "Hands-on Exercise 4",
    "section": "4.2 Installing and Launching R Packages",
    "text": "4.2 Installing and Launching R Packages\nIn this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-data-1",
    "title": "Hands-on Exercise 4",
    "section": "4.3 Importing Data",
    "text": "4.3 Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 <- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %>%\n  mutate_if(is.character, as.factor)\n\nRows: 267 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): City, District, Sub-district\ndbl (4): Sub-district ID, Positive, Recovered, Death\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4",
    "section": "4.4 FunnelPlotR methods",
    "text": "4.4 FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n4.4.1 FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n4.4.2 FunnelPlotR methods: Makeover 1\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #<<\n  xrange = c(0, 6500),  #<<\n  yrange = c(0, 0.05)   #<<\n)\n\nWarning: The `xrange` argument deprecated; please use the `x_range` argument\ninstead. For more options, see the help: `?funnel_plot`\n\n\nWarning: The `yrange` argument deprecated; please use the `y_range` argument\ninstead. For more options, see the help: `?funnel_plot`\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n4.4.3 FunnelPlotR methods: Makeover 2\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #<<           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #<<\n  y_label = \"Cumulative Fatality Rate\"  #<<\n)\n\nWarning: The `xrange` argument deprecated; please use the `x_range` argument\ninstead. For more options, see the help: `?funnel_plot`\n\n\nWarning: The `yrange` argument deprecated; please use the `y_range` argument\ninstead. For more options, see the help: `?funnel_plot`\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4",
    "section": "4.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "4.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n4.5.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf <- covid19 %>%\n  mutate(rate = Death / Positive) %>%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %>%\n  filter(rate > 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean <- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n4.5.2 Calculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq <- seq(1, max(df$Positive), 1)\nnumber.ll95 <- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 <- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 <- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 <- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI <- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n4.5.3 Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np <- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\n\nWarning in geom_point(aes(label = `Sub-district`), alpha = 0.4): Ignoring\nunknown aesthetics: label\n\np\n\n\n\n\n\n\n4.5.4 Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly <- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#description-of-dataset",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#description-of-dataset",
    "title": "Take-home Exercise 1",
    "section": "2. Description of Dataset",
    "text": "2. Description of Dataset\nFor the purpose of this study, two data sets are provided. They are:\nParticipants.csv: Contains information about the residents of City of Engagement that have agreed to participate in this study.\n\nparticipantId (integer): unique ID assigned to each participant.\nhouseholdSize (integer): the number of people in the participant’s household\nhaveKids (boolean): whether there are children living in the participant’s household.\nage (integer): participant’s age in years at the start of the study.\neducationLevel (string factor): the participant’s education level, one of: {“Low”, “HighSchoolOrCollege”, “Bachelors”, “Graduate”}\ninterestGroup (char): a char representing the participant’s stated primary interest group, one of {“A”, “B”, “C”, “D”, “E”, “F”, “G”, “H”, “I”, “J”}. Note: specific topics of interest have been redacted to avoid bias.\njoviality (float): a value ranging from [0,1] indicating the participant’s overall happiness level at the start of the study.\n\nFinancialJournal.csv: Contains information about financial transactions.\n\nparticipantId (integer): unique ID corresponding to the participant affected\ntimestamp (datetime): the time when the check-in was logged\ncategory (string factor): a string describing the expense category, one of {“Education”, “Food”, “Recreation”, “RentAdjustment”, “Shelter”, “Wage”}\namount (double): the amount of the transaction"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling-and-preparation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling-and-preparation",
    "title": "Take-home Exercise 1",
    "section": "3. Data Wrangling and Preparation",
    "text": "3. Data Wrangling and Preparation\n\n3.1 Installing Requisite R packages\n\npacman::p_load(plotly, lubridate, tidyverse, psych, ggridges, gganimate, ggrepel, tidyr, ggstatsplot, ggside)\n\n\n\n3.2 Loading the Dataset\n\nfinancialjournal <- read_csv(\"data/financialjournal.csv\")\nparticipants <- read_csv(\"data/participants.csv\")\n\n\n\n3.3 Data Preparation\n\n\n\n\n\n\n\n\nIssues\nDescription\nResolution\n\n\n\n\nDuplicated records\nThere are duplicated records within financialjournal dataset.\nDuplicated records were removed\n\n\nLack of additional useful features\nAdditional features such as Savings, and Spending to aggregate how much an individual spends and saved relative to their wage can be useful in gaining a deeper understanding of each person’s financial habits\nNew variables were created, specifically in relation to Savings (Wage - Education - Food - Recreation - Shelter) and Spending (Education + Food + Recreation + Shelter).\n\n\nInaccuracies within the Shelter recorded amount\nThere is a column called RentAdjustment in the original dataset which if we do not net off with the Shelter recorded amount, it will artificially inflate the individual’s spending amount\nThe updated Shelter values was calculated to incorporate RentAdjustment where applicable.\n\n\nMissing monthly values for 10 participants\nParticipant IDs: 44, 127, 142, 154, 161, 256, 262, 267, 279, 285, only have one month of data in March 2022. Hence, we need to be cautious when using their data in section 4.1 and 4.2\nThese Participant IDs were excluded from the analysis in section 4.1 and 4.2\n\n\nLack of an average metric in relation to Wage, Savings, Spending and its associated sub-categories (such as Education, Food, Recreation etc)\nGiven that there are two dataset, participants and financialjournal for further analysis, it is useful to merge both dataset together for further analysis. However, financialjournal dataset presents the spending habits of individual at too granular a level for it to be effectively merged with the participants dataset.\nIt is also noted that the data spans across twelve months and hence an average metric is suitable as it implies the average yearly financial habits of individuals.\nAn average metric was computed for Wage, Savings, Spending and its associated sub categories (such as Education, Food, Recreation etc).\n\n\n\n\n3.3.1 Check for missing and duplicate records\nisna() function is used to confirm that there are no missing values in both dataset.\n\n\nShow code\nany(is.na(participants))\n\n\n[1] FALSE\n\n\nShow code\nany(is.na(financialjournal))\n\n\n[1] FALSE\n\n\nduplicated() function is used to check for duplicated records in both dataset.\n\n\nShow code\nduplicated_check_participants <- participants %>% filter(duplicated(participants))\nduplicated_check_financialjournal <- financialjournal %>% filter(duplicated(financialjournal))\nduplicated_check_participants\n\n\n# A tibble: 0 × 7\n# ℹ 7 variables: participantId <dbl>, householdSize <dbl>, haveKids <lgl>,\n#   age <dbl>, educationLevel <chr>, interestGroup <chr>, joviality <dbl>\n\n\nShow code\nduplicated_check_financialjournal\n\n\n# A tibble: 1,113 × 4\n   participantId timestamp           category   amount\n           <dbl> <dttm>              <chr>       <dbl>\n 1             0 2022-03-01 00:00:00 Shelter    -555. \n 2             0 2022-03-01 00:00:00 Education   -38.0\n 3             1 2022-03-01 00:00:00 Shelter    -555. \n 4             1 2022-03-01 00:00:00 Education   -38.0\n 5             2 2022-03-01 00:00:00 Shelter    -557. \n 6             2 2022-03-01 00:00:00 Education   -12.8\n 7             3 2022-03-01 00:00:00 Shelter    -555. \n 8             3 2022-03-01 00:00:00 Education   -38.0\n 9             4 2022-03-01 00:00:00 Shelter   -1556. \n10             4 2022-03-01 00:00:00 Education   -12.8\n# ℹ 1,103 more rows\n\n\nUpon finding out that there are duplicated entries, we proceed to exclude the duplicated records as shown in the code below.\n\n\nShow code\nfinancialjournal <- distinct(financialjournal)\n\n\n\n\n3.3.2 Creation of new additional useful variables and cleaning of “Shelter” column\nThereafter, we performed the below steps via the code below:\n\nLeveraged on mutate() function to create a new column called timestamp, to set it to the start of the month for each observation. Ten new columns were also created using mutate(): savings, spending, and YearMonth. category and Wage, which contains the mean value for Education, Food, Recreation, Shelter, Wage, Savings and Spending. The mutate_all() function is used to convert all numeric columns to absolute values via abs().\nData is then grouped by participantId, timestamp, and category using group_by() before using the summarize() function to calculate the sum of amount for each group.\nThe pivot_wider() function is used to pivot the data into one column for each category and its corresponding amount values. values_fill is set as 0 to cater for scenario where categories may have a missing value.\nThe Shelter column is adjusted by adding RentAdjustment to it.\nThe final output is then grouped by participantId and YearMonth.\n\n\n\nShow code\nfinancialjournal_new <- financialjournal %>%\n   mutate(timestamp = lubridate::floor_date(timestamp, unit = \"month\")) %>%\n  group_by(participantId, timestamp, category) %>%\n  summarize(amount = sum(amount)) %>%\n  pivot_wider(names_from = category, \n              values_from = amount,\n              values_fill = 0) %>%\n  mutate(Shelter = Shelter + RentAdjustment,\n         savings = Wage - Education - Food - Recreation - Shelter,\n         spending = Education + Food + Recreation + Shelter) %>%\n  select(participantId, timestamp, Education:Shelter, Wage, savings, spending, -RentAdjustment) %>%\n  mutate_all(~ifelse(is.numeric(.), abs(.), .)) %>%\n  mutate(YearMonth = format(timestamp, \"%b %Y\")) %>%\n  group_by(participantId, YearMonth) %>%\n  mutate(avg_education = mean(Education),\n         avg_food = mean(Food),\n         avg_recreation = mean(Recreation),\n         avg_shelter = mean(Shelter),\n         avg_wage = mean(Wage),\n         avg_savings = mean(savings),\n         avg_spending = mean(spending))\n\n\nWe then gain a more detailed insight into the updated Wage, Savings, SpendingWage, Savings, Spending dataset via psych.\n\n\nShow code\npsych::describe(financialjournal_new)\n\n\n               vars     n    mean      sd  median trimmed     mad     min\nparticipantId     1 10691  483.30  294.17  466.00  477.84  370.65    0.00\ntimestamp         2 10691     NaN      NA      NA     NaN      NA     Inf\nEducation         3 10691   13.10   25.90    0.00    6.18    0.00    0.00\nFood              4 10691  346.41   85.25  308.58  342.51   67.67   32.00\nRecreation        5 10691  387.55  249.97  377.40  373.71  201.59    0.00\nShelter           6 10691  628.67  222.03  649.70  622.51  233.47    0.00\nWage              7 10691 4265.05 2436.30 3613.88 3867.84 1786.00 1600.00\nsavings           8 10691 5640.78 2481.72 5016.76 5267.59 1864.46 2133.62\nspending          9 10691 1375.73  425.81 1391.53 1379.97  368.60   32.00\nYearMonth*       10 10691    6.52    3.43    7.00    6.52    4.45    1.00\navg_education    11 10691   13.10   25.90    0.00    6.18    0.00    0.00\navg_food         12 10691  346.41   85.25  308.58  342.51   67.67   32.00\navg_recreation   13 10691  387.55  249.97  377.40  373.71  201.59    0.00\navg_shelter      14 10691  628.67  222.03  649.70  622.51  233.47    0.00\navg_wage         15 10691 4265.05 2436.30 3613.88 3867.84 1786.00 1600.00\navg_savings      16 10691 5640.78 2481.72 5016.76 5267.59 1864.46 2133.62\navg_spending     17 10691 1375.73  425.81 1391.53 1379.97  368.60   32.00\n                    max    range  skew kurtosis    se\nparticipantId   1010.00  1010.00  0.13    -1.17  2.85\ntimestamp          -Inf     -Inf    NA       NA    NA\nEducation         91.14    91.14  2.00     2.80  0.25\nFood             590.42   558.42  0.13     0.41  0.82\nRecreation      1962.10  1962.10  0.90     2.50  2.42\nShelter         1556.36  1556.36  0.25     0.74  2.15\nWage           21334.65 19734.65  1.95     5.24 23.56\nsavings        22856.94 20723.33  1.82     4.74 24.00\nspending        3463.11  3431.12 -0.04     0.92  4.12\nYearMonth*        12.00    11.00 -0.02    -1.20  0.03\navg_education     91.14    91.14  2.00     2.80  0.25\navg_food         590.42   558.42  0.13     0.41  0.82\navg_recreation  1962.10  1962.10  0.90     2.50  2.42\navg_shelter     1556.36  1556.36  0.25     0.74  2.15\navg_wage       21334.65 19734.65  1.95     5.24 23.56\navg_savings    22856.94 20723.33  1.82     4.74 24.00\navg_spending    3463.11  3431.12 -0.04     0.92  4.12\n\n\n\n\n3.3.3 Check on Participants’ record to ensure they all have 12 months of data\nThereafter, we group the data by the YearMonth variable to calculate the total amount on each segment (e.g. Education, Food, Recreation, Shelter and Wage). It appears that in the month of Mar 2022, there is a huge spike in the total Wage amount. This can potentially be a case of bonus payout period. However, we will take a closer look in the subsequent section.\n\n\nShow code\nfinancialjournal_monthly <- financialjournal_new %>%\n  group_by(YearMonth) %>%\n  summarize(total_Education = sum(Education),\n            total_Food = sum(Food),\n            total_Recreation = sum(Recreation),\n            total_Shelter = sum(Shelter),\n            total_Wage = sum(Wage))\n\nfinancialjournal_monthly\n\n\n# A tibble: 12 × 6\n   YearMonth total_Education total_Food total_Recreation total_Shelter\n   <chr>               <dbl>      <dbl>            <dbl>         <dbl>\n 1 Apr 2022           11423.    304297.          389695.       558489.\n 2 Aug 2022           11423.    312841.          306963.       558451.\n 3 Dec 2022           11423.    313103.          316621.       558451.\n 4 Feb 2023           11423.    282478.          271127.       558451.\n 5 Jan 2023           11423.    313272.          311580.       558451.\n 6 Jul 2022           11423.    313819.          329613.       558451.\n 7 Jun 2022           11423.    302909.          314808.       558451.\n 8 Mar 2022           14354.    327843.          649591.       578120.\n 9 May 2022           11423.    313556.          336418.       558451.\n10 Nov 2022           11423.    302847.          288290.       558451.\n11 Oct 2022           11423.    313577.          326722.       558451.\n12 Sep 2022           11423.    302878.          301910.       558451.\n# ℹ 1 more variable: total_Wage <dbl>\n\n\nGiven that we will perform a Ridgeplot analysis subsequently, we want to confirm that all participants indeed have 12 months of records. Running the code below shows us that there are ten individuals (Participant IDs: 44, 127, 142, 154, 161, 256, 262, 267, 279, 285) who do not have 12 months of records. Interestingly, they all only have one month of record which falls on Mar 2022.\n\n\nShow code\n# Get the list of participantIds with only one month of data\nparticipants_check <- financialjournal_new %>%\n  group_by(participantId) %>%\n  summarize(num_months = n_distinct(YearMonth)) %>%\n  filter(num_months != 12) %>%\n  pull(participantId)\n\n# Filter financialjournal_new to sieve out rows with participantIds that have only one month of data\nfinancialjournal_new %>%\n  filter((participantId %in% participants_check))\n\n\n# A tibble: 131 × 17\n# Groups:   participantId, YearMonth [131]\n   participantId timestamp           Education  Food Recreation Shelter  Wage\n           <dbl> <dttm>                  <dbl> <dbl>      <dbl>   <dbl> <dbl>\n 1            44 2022-03-01 00:00:00      12.8  38.5          0       0 2236.\n 2           127 2022-03-01 00:00:00       0    55.8          0       0 2764.\n 3           142 2022-03-01 00:00:00       0    64.7          0       0 2845.\n 4           154 2022-03-01 00:00:00       0    58.6          0       0 2595.\n 5           161 2022-03-01 00:00:00       0    41.0          0       0 2679.\n 6           256 2022-03-01 00:00:00      91.1  73.4          0       0 2505.\n 7           262 2022-03-01 00:00:00      91.1  60.1          0       0 2740.\n 8           267 2022-03-01 00:00:00      91.1  74.7          0       0 2642.\n 9           279 2022-03-01 00:00:00      91.1  51.8          0       0 2396.\n10           285 2022-03-01 00:00:00      91.1  68.2          0       0 2692.\n# ℹ 121 more rows\n# ℹ 10 more variables: savings <dbl>, spending <dbl>, YearMonth <chr>,\n#   avg_education <dbl>, avg_food <dbl>, avg_recreation <dbl>,\n#   avg_shelter <dbl>, avg_wage <dbl>, avg_savings <dbl>, avg_spending <dbl>"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-visualisation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-visualisation",
    "title": "Take-home Exercise 1",
    "section": "4. Data Visualisation",
    "text": "4. Data Visualisation\n\n4.1 Expenditure of Participants over time period of March 2022 to March 2023\nGiven the above findings, we will proceed to exclude the ten Participant IDs from our ridgeplot analysis. Ridgeline plot consists of a set of overlapped density plots to help with comparing of multiple distributions in the dataset, and is especially useful to visualise the changes in our key parameters in distribution over time (12 month span).\n\nEducation, Food, Recreation and Shelter amounts are extracted and arranged in a long format using pivot_longer()\nggplot() function is used to create a plot with the amount on the x-axis, category on the y-axis\nstat_density_ridges() function adds density ridges to the plot to show the distribution of spending amounts\ntransition_time() function is used to animate the plot from March 2022 to March 2023\nscale_x_continuous() was utilised to set the x-axis limits to be in line with the range of spending amounts within the dataset\n\nFrom the first Ridgeline graph, we can observe that Recreation and Food expenses tend to have a larger fluctuation across a twelve month period from March 2022 to March 2023 as compared to other spend categories.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nfinancialjournal_expendituredetails <- financialjournal_new %>%\n  pivot_longer(cols = c(\"Education\", \"Food\", \"Recreation\", \"Shelter\"),\n               names_to = \"category\", values_to = \"amount\") %>%\n  arrange(participantId, timestamp) %>%\n  filter(!participantId %in% c(44, 127, 142, 154, 161, 256, 262, 267, 279, 285))\n\nfinancialjournal_expendituredetails %>%\n  mutate(timestamp = floor_date(timestamp, unit = \"day\")) %>%\n  ggplot(aes(x = amount, y = category, fill = category)) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) + scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges() +\n  labs(title = \"Amounts by Category from Mar 2022 to Mar 2023: {frame_time}\",\n       x = \"Amount\",\n       y = \"\") +\n  transition_time(timestamp) +\n  ease_aes(\"linear\") +\n  scale_x_continuous(limits = range(financialjournal_expendituredetails$amount))\n\n\n\n\n\n\n4.2 Wage, Overall Spending and Savings of Participants over time period of March 2022 to March 2023\nFor the second Ridgeline graph below, the approach is very similar with that outlined in section 4.1. Specifically, here we want to gain a closer insight into an individuals’ Spending, Savings and Wage. We can observe that an individuals’ Spending is largely center around a common range despite the huge variation in Wage. Savings and Wage tend to have a wider spread as compared to Spending.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nfinancialjournal_wagesavings <- financialjournal_new %>%\n  pivot_longer(cols = c(\"Wage\",\"spending\",\"savings\"),\n               names_to = \"category\", values_to = \"amount\") %>%\n  arrange(participantId, timestamp) %>%\n  filter(!participantId %in% c(44, 127, 142, 154, 161, 256, 262, 267, 279, 285))\n\nfinancialjournal_wagesavings %>%\n  mutate(timestamp = floor_date(timestamp, unit = \"day\")) %>%\n  ggplot(aes(x = amount, y = category, fill = category)) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) + scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges() +\n  labs(title = \"Amounts by Category from Mar 2022 to Mar 2023: {frame_time}\",\n       x = \"Amount\",\n       y = \"\") +\n  transition_time(timestamp) +\n  ease_aes(\"linear\") +\n  scale_x_continuous(limits = range(financialjournal_wagesavings$amount))\n\n\n\n\nNext, we carve out a dataframe called financialjournal_avg which contains the average values for Education, Food, Recreation, Shelter, Wage, Savings, Spending for each ParticiapntId.\n\n\nShow code\nfinancialjournal_avg <- financialjournal_new %>%\n  group_by(participantId) %>%\n  summarize(avg_education = mean(Education),\n            avg_food = mean(Food),\n            avg_recreation = mean(Recreation),\n            avg_shelter = mean(Shelter),\n            avg_wage = mean(Wage),\n            avg_savings = mean(savings),\n            avg_spending = mean(spending))\n\nfinancialjournal_avg\n\n\n# A tibble: 1,011 × 8\n   participantId avg_education avg_food avg_recreation avg_shelter avg_wage\n           <dbl>         <dbl>    <dbl>          <dbl>       <dbl>    <dbl>\n 1             0          38.0     262.           365.        555.    9151.\n 2             1          38.0     264.           553.        555.    8031.\n 3             2          12.8     289.           348.        557.    7092.\n 4             3          38.0     283.           392.        555.    6856.\n 5             4          12.8     272.           526.       1004.    8838.\n 6             5          12.8     345.           428.        600.    1934.\n 7             6          12.8     318.           631.        581.    1951.\n 8             7          12.8     505.           481.       1013.    6073.\n 9             8          12.8     497.           442.       1180.    3712.\n10             9          91.1     286.           376.        558.   12715.\n# ℹ 1,001 more rows\n# ℹ 2 more variables: avg_savings <dbl>, avg_spending <dbl>\n\n\nWe then proceed to merge the above dataset with that of participants dataset for further analysis.\n\n\nShow code\nmerged_data <- left_join(financialjournal_avg, participants, by = \"participantId\")\n\n\nisna() function was utilised to confirm that there are no missing values post the merge.\n\n\nShow code\nany(is.na(merged_data))\n\n\n[1] FALSE\n\n\n\n\n4.3 Age Bucket Amongst Participants\nWe sliced the age groups into three main buckets, 1) 18 - 30 years old, 2) 31 - 55 years old and 3) >55 years old. This was done to delineate the young adults, middle aged adults, and senior adults.\n\n\nShow code\nmerged_data$age_group<- cut(merged_data$age, breaks = c(17, 30, 55, Inf), \n                              labels = c(\"18-30\", \"31-55\", \">55\"))\n\n\nWe delve deeper into the age group of the participants to better understand the demographics on hand. We perform the below steps:\n\nsummarized the data via group_by() and summarize() functions to calculate the number of participants in each age bucket before arriving at the percentage values via mutate() function\nused geom_col() to create a bar chart for each age group\nused coord_polar() to convert the bar chart to a pie chart\nused geom_label_repel() to add percentage labels to each segment of the pie chart\nused guides() to add titles to the legend\nused labs() to add a title to the chart\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nage_count <- merged_data %>%\n  group_by(age_group) %>%\n  summarize(count = n()) %>%\n  mutate(age_pct = round(count/sum(count)*100)) %>%\n  mutate(ypos_p = rev(cumsum(rev(age_pct))),\n         pos_p = age_pct/2 + lead(ypos_p, 1),\n         pos_p = if_else(is.na(pos_p), age_pct/2, pos_p))\n\nggplot(age_count, aes(x = \"\", y = age_pct, fill = fct_inorder(age_group))) +\n  geom_col(width = 1, color = 1) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"YlGnBu\") +\n  geom_label_repel(data = age_count,\n                   aes(y = pos_p, label = paste0(age_pct, \"%\"), color = age_group),\n                   size = 4.5, nudge_x = 1, show.legend = FALSE) +\n  guides(fill = guide_legend(title = \"Age Group\"), color = guide_legend(title = \"Age Group\")) +\n  labs(title = \"Proportion of Age Group amongst Participants\") +\n  theme(legend.position = \"bottom\") +\n  theme_minimal() +\n  theme(axis.title = element_blank())\n\n\n\n\n\n\n4.4 Education Level Amongst Participants\nWe utilised the same method as outlined in section 4.3 to delve deeper into the education background level of the participants to better understand the demographics on hand.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\neducation_count <- merged_data %>%\n  group_by(educationLevel) %>%\n  summarize(\n    count = n()) %>% \n  mutate(educationlevel_pct = round(count/sum(count)*100)) %>% \n  mutate(ypos_p = rev(cumsum(rev(educationlevel_pct))),\n         pos_p = educationlevel_pct/2 + lead(ypos_p,1),\n         pos_p = if_else(is.na(pos_p), educationlevel_pct/2, pos_p))\n\nggplot(education_count, aes(x = \"\" , y = educationlevel_pct, fill = fct_inorder(educationLevel))) +\n  geom_col(width = 1, color = 1) +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"YlGnBu\") +\n  geom_label_repel(data = education_count,\n                   aes(y = pos_p, label = paste0(educationlevel_pct, \"%\"), color = educationLevel),\n                   size = 4.5, nudge_x = 1, show.legend = FALSE) +\n  guides(fill = guide_legend(title = \"Education Level\"), color = guide_legend(title = \"Education Level\")) +\n  labs(title = \"Proportion of Education Levels amongst Participants\") +\n  theme(legend.position = \"bottom\") +\n  theme_minimal() +\n  theme(axis.title = element_blank())\n\n\n\n\n\n\n4.5 Violin Plot across Education Level\nNext, we leverage on violin plot to take a closer look at how spending (overall and across various sub categories), savings, wage and joviality differs across education level. Violin plots are useful especially when we want to observe the distribution, peaks, valleys and tails of each group’s density curve to identify how similar or dissimilar the various groups are.\nThe plot is created via:\n\nplot_ly() function to create the violin plot. Within the plot_ly() function, the x-axis of the plot is specified as educationlevel and the y-axis is set to avg_wage initially but will dynamically update thereafter upon the specified selection.\nlayout() function to define the title of the plot, x-axis labels, y-axis labels, and the dropdown menu via the updatemenus argument.\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = merged_data,\n        x = ~educationLevel,\n        y = ~avg_wage,\n        line = list(width = 1),\n        type = \"violin\",\n        marker = list(opacity = 0.5,\n                      line = list(width = 2)),\n        box = list(visible = T),\n        meanline = list(visible = T,\n                        color = \"rgb(231, 99, 250)\")) |>\n  layout(title = \"Distribution of key factors by Education Level\",\n         xaxis = list(title = \"Education Level\"),\n         yaxis = list(title = \"Average Ëducation Expenses\"),\n         updatemenus = list(list(type = 'dropdown',\n                                 xref = \"paper\",\n                                 yref = \"paper\",\n                                 xanchor = \"left\",\n                                 x = 0.04, \n                                 y = 0.95,\n                                 buttons = list(\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_education)),\n                                                    list(yaxis = list(title = \"Average Education\"))),\n                                        label = \"Average Education Expenses\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_food)),\n                                                    list(yaxis = list(title = \"Average Food\"))),\n                                        label = \"Average Food Expenses\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_recreation)),\n                                                    list(yaxis = list(title = \"Average Recreation\"))),\n                                        label = \"Average Recreation Expenses\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_shelter)),\n                                                    list(yaxis = list(title = \"Average Shelter\"))),\n                                        label = \"Average Shelter Expenses\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_wage)),\n                                                    list(yaxis = list(title = \"Average Wage\"))),\n                                        label = \"Average Wage\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_savings)),\n                                                    list(yaxis = list(title = \"Average Savings\"))),\n                                        label = \"Average Savings\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_spending)),\n                                                    list(yaxis = list(title = \"Average Spending\"))),\n                                        label = \"Average Spending\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$joviality)),\n                                                    list(yaxis = list(title = \"Joviality\"))),\n                                        label = \"Joviality\")\n                              \n                                   )\n                                 )\n                            )\n         )\n\n\n\n\n\n\n4.6 Violin Plot across Interest Group\nNext, we leverage on violin plot to take a closer look at how spending (overall and across various sub categories), savings, wage and joviality differs across Interest Group.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = merged_data,\n        x = ~interestGroup,\n        y = ~avg_wage,\n        line = list(width = 1),\n        type = \"violin\",\n        marker = list(opacity = 0.5,\n                      line = list(width = 2)),\n        box = list(visible = T),\n        meanline = list(visible = T,\n                        color = \"rgb(231, 99, 250)\")) |>\n  layout(title = \"Distribution of key factors by Interest Group\",\n         xaxis = list(title = \"Interest Group\"),\n         yaxis = list(title = \"Average Ëducation Expenses\"),\n         updatemenus = list(list(type = 'dropdown',\n                                 xref = \"paper\",\n                                 yref = \"paper\",\n                                 xanchor = \"left\",\n                                 x = 0.04, \n                                 y = 0.95,\n                                 buttons = list(\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_education)),\n                                                    list(yaxis = list(title = \"Average Education Expenses\"))),\n                                        label = \"Average Education\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_food)),\n                                                    list(yaxis = list(title = \"Average Food Expenses\"))),\n                                        label = \"Average Food\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_recreation)),\n                                                    list(yaxis = list(title = \"Average Recreation Expenses\"))),\n                                        label = \"Average Recreation\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_shelter)),\n                                                    list(yaxis = list(title = \"Average Shelter Expenses\"))),\n                                        label = \"Average Shelter\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_wage)),\n                                                    list(yaxis = list(title = \"Average Wage\"))),\n                                        label = \"Average Wage\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_savings)),\n                                                    list(yaxis = list(title = \"Average Savings\"))),\n                                        label = \"Average Savings\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_spending)),\n                                                    list(yaxis = list(title = \"Average Spending\"))),\n                                        label = \"Average Spending\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$joviality)),\n                                                    list(yaxis = list(title = \"Joviality\"))),\n                                        label = \"Joviality\")\n                                   )\n                                 )\n                            )\n         )\n\n\n\n\n\n\n4.7 Violin Plot across Age Group\nNext, we leverage on violin plot to take a closer look at how spending (overall and across various sub categories), savings, wage and joviality differs across age groups.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = merged_data,\n        x = ~age_group,\n        y = ~avg_wage,\n        line = list(width = 1),\n        type = \"violin\",\n        marker = list(opacity = 0.5,\n                      line = list(width = 2)),\n        box = list(visible = T),\n        meanline = list(visible = T,\n                        color = \"rgb(231, 99, 250)\")) |>\n  layout(title = \"Distribution of key factors by Age Group\",\n         xaxis = list(title = \"Age Group\"),\n         yaxis = list(title = \"Average Ëducation Expenses\"),\n         updatemenus = list(list(type = 'dropdown',\n                                 xref = \"paper\",\n                                 yref = \"paper\",\n                                 xanchor = \"left\",\n                                 x = 0.04, \n                                 y = 0.95,\n                                 buttons = list(\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_education)),\n                                                    list(yaxis = list(title = \"Average Education\"))),\n                                        label = \"Average Education Expenses\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_food)),\n                                                    list(yaxis = list(title = \"Average Food\"))),\n                                        label = \"Average Food Expenses\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_recreation)),\n                                                    list(yaxis = list(title = \"Average Recreation\"))),\n                                        label = \"Average Recreation Expenses\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_shelter)),\n                                                    list(yaxis = list(title = \"Average Shelter\"))),\n                                        label = \"Average Shelter Expenses\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_wage)),\n                                                    list(yaxis = list(title = \"Average Wage\"))),\n                                        label = \"Average Wage\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_savings)),\n                                                    list(yaxis = list(title = \"Average Savings\"))),\n                                        label = \"Average Savings\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$avg_spending)),\n                                                    list(yaxis = list(title = \"Average Spending\"))),\n                                        label = \"Average Spending\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(merged_data$joviality)),\n                                                    list(yaxis = list(title = \"Joviality\"))),\n                                        label = \"Joviality\")\n                              \n                                   )\n                                 )\n                            )\n         )\n\n\n\n\n\n\n4.8 Correlation between wage and spending\nIn this section, we are keen to explore if there is a correlation between wage and spending. We utilised ggstatsplot and ggscatterstats function to calculate the correlation and plot a chart. Interestingly, there is a weak positive correlation at 0.14 between wage and spending.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nggstatsplot::ggscatterstats(data = merged_data, x = \"avg_wage\", y = \"avg_spending\")\n\n\n\n\n\n\n4.9 Correlation between wage and joviality\nWe are also keen to explore if there is a correlation between wage and joviality. Interestingly, there is a moderately weak negative correlation at -0.33 between wage and spending.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\nggstatsplot::ggscatterstats(data = merged_data, x = \"avg_wage\", y = \"joviality\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#future-work",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#future-work",
    "title": "Take-home Exercise 1",
    "section": "5. Future Work",
    "text": "5. Future Work\nFor future work, we can consider to further enhance on the testing of whether for example avg_education, avg_food, avg_recreation, avg_shelter, avg_wage, avg_savings, avg_spending or joviality are statistically different across the various interest groups, education levels or age groups. This can be done via first checking for normality assumptions before determining which tests (e.g. Anova) to use.\nFurther analysis such as whether individuals who have kids have a higher joviality can also be investigated and tested accordingly."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "title": "Hands-on Exercise 5",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nIn this hands-on exercise, we will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "title": "Hands-on Exercise 5",
    "section": "5.2 Getting Started",
    "text": "5.2 Getting Started\n\n5.2.1 Installing and launching R packages\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-data",
    "title": "Hands-on Exercise 5",
    "section": "5.3 The Data",
    "text": "5.3 The Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n5.3.1 The edges data\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondences between 55 employees.\n\n\n\n\n5.3.2 The nodes data\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\n5.3.3 Importing network data from files\nIn this step, we will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n5.3.4 Reviewing the imported data\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n5.3.5 Wrangling time\nThe code chunk below will be used to perform the changes to update “SentDate” field as date data type instead of character data type.\n\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the above code chunk\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the day spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n5.3.6 Reviewing the revised date fields\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     <ord> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n5.3.7 Wrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the above code chunk\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n5.3.8 Reviewing the revised edges file\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday <ord> Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  <int> 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 5",
    "section": "5.4 Creating network objects using tidygraph",
    "text": "5.4 Creating network objects using tidygraph\nIn this section, we will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\n\n5.4.1 The tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n5.4.2 The dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n5.4.3 Using tbl_graph() to build tidygraph data model\nIn this section, we will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\n\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n5.4.4 Reviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,372 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n5.4.5 Reviewing the output tidygraph’s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n5.4.6 Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\nGAStech_graph %>%\n  activate(edges) %>%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 1,372 × 4\n   from    to Weekday  Weight\n  <int> <int> <ord>     <int>\n1    40    41 Saturday     13\n2    41    43 Monday       11\n3    35    31 Tuesday      10\n4    40    41 Monday       10\n5    40    43 Monday       10\n6    36    32 Sunday        9\n# ℹ 1,366 more rows\n#\n# A tibble: 54 × 4\n     id label           Department     Title           \n  <dbl> <chr>           <chr>          <chr>           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 5",
    "section": "5.5 Plotting Static Network Graphs with ggraph package",
    "text": "5.5 Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs. As in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\n\n5.5.1 Plotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the above code chunk\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\n5.5.2 Changing the default network graph theme\nIn this section, we will use theme_graph() to remove the x and y axes.\n\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the above code chunk\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n5.5.3 Changing the coloring of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n5.5.4 Working with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n \n\n\n5.5.5 Fruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng <- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the above code chunk:\n\nlayout argument is used to define the layout to be used\n\n\n\n\n\n5.5.6 Modifying network nodes\nIn this section, we will colour each node by referring to their respective departments.\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the above code chunk:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\n\n5.5.7 Modifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the above code chunk:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-facet-graphs",
    "title": "Hands-on Exercise 5",
    "section": "5.6 Creating facet graphs",
    "text": "5.6 Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are: - facet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here, - facet_edges() whereby nodes are always drawn in all panels even if the node data contains an attribute named the same as the one used for the edge facetting, and - facet_graph() faceting on two variables simultaneously.\n\n5.6.1 Working with facet_edges()\nIn the code chunk below, facet_edges() is used.\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n5.6.2 Working with facet_edges()\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n5.6.3 A framed facet graph\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n5.6.4 Working with facet_nodes()\nIn the code chunk below, facet_nodes() is used.\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#network-metrics-analysis",
    "title": "Hands-on Exercise 5",
    "section": "5.7 Network Metrics Analysis",
    "text": "5.7 Network Metrics Analysis\n\n5.7.1 Computing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector.\n\ng <- GAStech_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the above code chunk:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\n\n5.7.2 Visualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n5.7.3 Visualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it.\nIn the code chunk below group_edge_betweenness() is used.\n\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 5",
    "section": "5.8 Building Interactive Network Graph with visNetwork",
    "text": "5.8 Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n5.8.1 Data preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %>%\n  rename(from = id) %>%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %>%\n  rename(to = id) %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 1) %>%\n  ungroup()\n\n\n\n5.8.2 Plotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n5.8.3 Working with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n5.8.4 Working with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes <- GAStech_nodes %>%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n5.8.5 Working with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n\nThe argument arrows is used to define where to place the arrow.\nThe smooth argument is used to plot the edges using a smooth curve.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n5.8.6 Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#creating-ternary-plot-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#creating-ternary-plot-with-r",
    "title": "Hands-on Exercise 6",
    "section": "6. Creating Ternary Plot with R",
    "text": "6. Creating Ternary Plot with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "title": "Hands-on Exercise 6",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nTernary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, we will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps: - Install and launch tidyverse and ggtern packages. - Derive three new measures using mutate() function of dplyr package. - Build a static ternary plot using ggtern() function of ggtern package. - Build an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 6",
    "section": "6.2 Installing and launching R packages",
    "text": "6.2 Installing and launching R packages\nFor this exercise, two main R packages will be used in this hands-on exercise, they are: - ggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots. - Plotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load('plotly', 'tidyverse')\n\n\n\n\n\n\n\nNote\n\n\n\nDue to some technical issue, ggtern is currently not available for downloading via cran. We need to download ggtern from the archive by using the code chunk below. The latest archive version is 3.4.1.\n\n\n\nrequire(devtools)\ninstall_version(\"ggtern\", version = \"3.4.1\", repos = \"http://cran.us.r-project.org\")\n\ncolorspace (2.0-3 -> 2.1-0 ) [CRAN]\nstringi    (1.7.8 -> 1.7.12) [CRAN]\nrlang      (1.1.0 -> 1.1.1 ) [CRAN]\ncli        (3.4.1 -> 3.6.1 ) [CRAN]\nRcpp       (1.0.9 -> 1.0.10) [CRAN]\nutf8       (1.2.2 -> 1.2.3 ) [CRAN]\nfansi      (1.0.3 -> 1.0.4 ) [CRAN]\n\n\nNext, load ggtern package into R environment by using the code chunk below.\n\nlibrary(ggtern)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "title": "Hands-on Exercise 6",
    "section": "6.3 Data Preparation",
    "text": "6.3 Data Preparation\n\n6.3.1 The data\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n6.3.2 Importing Data\nTo import respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data <- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n6.3.3 Preparing the Data\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated <- pop_data %>%\n  mutate(`Year` = as.character(Year))%>%\n  spread(AG, Population) %>%\n  mutate(YOUNG = rowSums(.[4:8]))%>%\n  mutate(ACTIVE = rowSums(.[9:16]))  %>%\n  mutate(OLD = rowSums(.[17:21])) %>%\n  mutate(TOTAL = rowSums(.[22:24])) %>%\n  filter(Year == 2018)%>%\n  filter(TOTAL > 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 6",
    "section": "6.4 Plotting Ternary Diagram with R",
    "text": "6.4 Plotting Ternary Diagram with R\n\n6.4.1 Plotting a static ternary diagram\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n6.4.2 Plotting an interactive ternary diagram\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel <- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis <- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes <- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %>%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "5.1 Loading the required packages\n\npacman::p_load(jsonlite,igraph, tidygraph, ggraph, \n               visNetwork,tidyverse)\n\n\n\n5.2 Loading the dataset\n\nMC1 <- fromJSON(\"data/MC1.json\")\n\n\nMC1_nodes<- as_tibble(MC1$nodes) %>%\n  select(id,type,country)\n\n\nMC1_edges<- as_tibble(MC1$links) %>%\n  select(source,target, type, weight,key)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "Seafood is the largest traded food commodity, providing sustenance to more than 3 billion people worldwide. The United Nations Food and Agricultural Organization estimates that 85% of fish stocks are either overfished or fully exploited.\nUnfortunately, illegal, unreported, and unregulated fishing is a major contributor to overfishing worldwide. According to a study conducted by the Financial Transparency Coalition, it found that IUU fishing accounts for one-fifth of the global fisheries’ catches (equivalent to approximately $23.5bn) annually. Global losses arising from IUU fishing are also estimated to be approximately $50bn.\nTherefore, as part of Mini-Challenge 2 of VAST Challenge 2023, NGO FishEye International (a nonpartisan organization) is charged with understanding the social, political, and economic forces that drive the illegal fishing trade and I am addressing question 4 of MC Challenge 2. I first begin by outlining the series of steps I took to eventually arrive at my response titled “Mini-Case 2 Challenge Writeup & 4 Images” within Section 7 below."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#description-of-dataset",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#description-of-dataset",
    "title": "Take-home Exercise 2",
    "section": "2. Description of Dataset",
    "text": "2. Description of Dataset\nThe dataset utilised for the below analysis consists of a total of 34,552 nodes and 5,464,092 direct edges (shipper to receiver). It is a directed multi-graph, with multiple edges between the same two nodes being a possible scenario. The full details can be found on: https://vast-challenge.github.io/2023/MC2.html"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling-and-preparation",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-wrangling-and-preparation",
    "title": "Take-home Exercise 2",
    "section": "3. Data Wrangling and Preparation",
    "text": "3. Data Wrangling and Preparation\n\n3.1 Installing Requisite R packages\nThe code chunk below uses p_load() of pacman package to check if the said packages are installed in the computer. If they are, then they will be launched into R.\n\n\nShow code\npacman::p_load(jsonlite, igraph, tidygraph, ggraph, \n               visNetwork, tidyverse, readxl, ggplot2, plotly, patchwork)\n\n\n\n\n3.2 Loading the MC2 Challenge Dataset\nWe first start off by loading the mc2_challenge_graph.json dataset into “MC2” via the code below:\n\n\nShow code\nMC2 <- fromJSON(\"data/mc2_challenge_graph.json\")\n\n\n\n\n3.3 Loading the Connect2India Harmonized System Code Dataset\nWe load an external “Connect2India Harmonized System Code to enable for subsequent understanding of HS Codes within MC2.\n\n\nShow code\nhscode_excel_file <- \"data/hsn-codes-list.xlsx\"\nhscode_map <- read_excel(hscode_excel_file)\nhscode_map <- hscode_map %>%\n  rename_with(~ gsub(\" \", \"_\", .), everything()) %>%\n  mutate(HS_Code = as.character(HS_Code))\n\n\nLet’s take a look at the parent hscode mapping description via the code chunk below\n\n\nShow code\nhscode_map_parent <- hscode_map %>%\n  filter(nchar(HS_Code) == 1)\n\nhscode_map_parent\n\n\n# A tibble: 9 × 3\n  S.No. HS_Code Description                                          \n  <dbl> <chr>   <chr>                                                \n1     1 1       Live animals                                         \n2   939 2       Meat and edible meat offal                           \n3  3836 3       Fish, crustaceans, molluscs, aquatic invertebrates ne\n4  6196 4       Dairy products, eggs, honey, edible animal product ne\n5  7658 5       Products of animal origin, nes                       \n6  9582 6       Live trees, plants, bulbs, roots, cut flowers etc    \n7 11073 7       Edible vegetables and certain roots and tubers       \n8 13226 8       Edible fruit, nuts, peel of citrus fruit, melons     \n9 17010 9       Coffee, tea, mate and spices                         \n\n\nIt appears that HS_Code starting with 3 is relevant for our use case.\n\n\nShow code\nhscode_map_secondparent <- hscode_map %>%\n  mutate(HS_Code = as.character(HS_Code)) %>%\n  filter(startsWith(HS_Code, \"3\") & nchar(HS_Code) == 2)\n\nhscode_map_secondparent\n\n\n# A tibble: 10 × 3\n   S.No. HS_Code Description                                          \n   <dbl> <chr>   <chr>                                                \n 1  3837 30      Pharmaceutical products                              \n 2  4481 31      Fertilizers                                          \n 3  4539 32      Tanning, dyeing extracts, tannins, derivs,pigments et\n 4  4915 33      Essential oils, perfumes, cosmetics, toileteries     \n 5  5081 34      Soaps, lubricants, waxes, candles, modelling pastes  \n 6  5169 35      Albuminoids, modified starches, glues, enzymes       \n 7  5230 36      Explosives, pyrotechnics, matches, pyrophorics, etc  \n 8  5268 37      Photographic or cinematographic goods                \n 9  5370 38      Miscellaneous chemical products                      \n10  5663 39      Plastics and articles thereof                        \n\n\nMore specifically, HS_Code starting with 301, 302, 303, 304, 305, 306, 307 and 308 are applicable for our analysis.\n\n\nShow code\nhscode_map_thirdparent <- hscode_map %>%\n  mutate(HS_Code = as.character(HS_Code)) %>%\n  filter(startsWith(HS_Code, \"3\") & nchar(HS_Code) == 3)\n\nhscode_map_thirdparent\n\n\n# A tibble: 8 × 3\n  S.No. HS_Code Description                                                     \n  <dbl> <chr>   <chr>                                                           \n1  4091 301     Live fish                                                       \n2  4105 302     Fish, fresh or chilled, whole                                   \n3  4192 303     Fish, frozen, whole                                             \n4  4285 304     Fish fillets, fish meat, mince except liver, roe                \n5  4366 305     Fish,cured, smoked, fish meal for human consumption             \n6  4402 306     Crustaceans                                                     \n7  4435 307     Molluscs                                                        \n8  4471 308     Fish and crustaceans, molluscs and other aquatic invertebrates …\n\n\n\n\n3.4 Initialising MC2_nodes and MC2_edges\nWe begin by initialising the nodes and edges based off the dataset “MC2_challenge_graph” - MC2_nodes consists of id, shpcountry and rcvcountry - MC2_edges consists of source, target, arrivaldate, hscode, valueofgoods_omu, volumeteu, weightkg and valueofgoodsusd.\nWe utilised the mutate function to derive a new variable called “year” from “arrivaldate”. Additionally, we ensured that all MC2_edges records are distinct with no duplicates via the distinct() function.\n\n\nShow code\nMC2_nodes<- as_tibble(MC2$nodes) %>%\n  select(id, shpcountry, rcvcountry)\n\nMC2_edges<- as_tibble(MC2$links) %>%\n  mutate(arrivaldate = ymd(arrivaldate)) %>%\n  mutate(year = year(arrivaldate)) %>% \n  select(source, target, arrivaldate, year, hscode, \n         valueofgoods_omu, volumeteu, weightkg, valueofgoodsusd) %>%\n  distinct()\n\n\n\n\n3.5 Joining of HSCodes to MC2_edges\nThereafter, we weaved in the Description from “Connect2India Harmonized System Code” and populated the Description information within MC2_edges to gain a better understanding of what each hscode means.\nFrom the below, we can see that there are several hscode that are not related to “fishery” products. For example, hscode 630630 refers to sails.\n\n\nShow code\nMC2_edges <- left_join(MC2_edges, hscode_map, by = c(\"hscode\" = \"HS_Code\")) %>%\n  select(source, target, arrivaldate, year, hscode, Description, valueofgoods_omu, volumeteu, weightkg, valueofgoodsusd)\n\nMC2_edges\n\n\n# A tibble: 5,309,087 × 10\n   source target arrivaldate  year hscode Description valueofgoods_omu volumeteu\n   <chr>  <chr>  <date>      <dbl> <chr>  <chr>                  <dbl>     <dbl>\n 1 AquaD… Barin… 2034-02-12   2034 630630 Sails                 141015         0\n 2 AquaD… Barin… 2034-03-13   2034 630630 Sails                 141015         0\n 3 AquaD… -15045 2028-02-07   2028 470710 Waste or s…               NA         0\n 4 AquaD… -15045 2028-02-23   2028 470710 Waste or s…               NA         0\n 5 AquaD… -15045 2028-09-11   2028 470710 Waste or s…               NA         0\n 6 AquaD… -15045 2028-10-09   2028 470710 Waste or s…               NA         0\n 7 AquaD… Océan… 2028-04-12   2028 304890 <NA>                      NA         0\n 8 AquaD… Olas … 2028-06-04   2028 304890 <NA>                      NA         0\n 9 AquaD… Shou … 2028-09-03   2028 303890 <NA>                      NA         0\n10 AquaD… Shou … 2028-09-08   2028 306170 <NA>                      NA         0\n# ℹ 5,309,077 more rows\n# ℹ 2 more variables: weightkg <int>, valueofgoodsusd <dbl>\n\n\nFrom the below chart, we can see the top 30 hscode within MC2_edges. Upon further investigation, it seems that HS Codes starting with 1604 and 1605 are relevant for our analysis. 1604 refers to “prepared or preserved fish, fish eggs, caviar”, while 1605 refers to Crustaceans, molluscs, etc, prepared or preserved”\n\n\nShow code\ntop30_hscode <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(n = n()) %>%\n  arrange(desc(n)) %>%\n  top_n(30)\n\ntop30_hscode <- top30_hscode %>%\n  plot_ly(x = ~reorder(hscode, n), y = ~n, type = \"bar\") %>%\n  layout(xaxis = list(title = \"Parent HSCode\"), yaxis = list(title = \"Frequency\"))\n\ntop30_hscode\n\n\n\n\n\n\n\n\n3.6 Refining MC2_edges to fish related HSCodes\nTo ensure a more effective analysis, we tidied up MC2_edges to only contain hscode that begins with 301 to 308 or 1604 and 1605 to scope MC2_edges towards fish related products. However, as seen from the below output, it seems like there are not sufficient information within the description columns to inform us of the specific product tied to the hscode.\n\n\nShow code\nMC2_edges <- MC2_edges %>%\n  filter(str_detect(hscode, \"^(301|302|303|304|305|306|307|308|1604|1605)\"))\n\nMC2_edges\n\n\n# A tibble: 739,363 × 10\n   source target arrivaldate  year hscode Description valueofgoods_omu volumeteu\n   <chr>  <chr>  <date>      <dbl> <chr>  <chr>                  <dbl>     <dbl>\n 1 AquaD… Océan… 2028-04-12   2028 304890 <NA>                      NA         0\n 2 AquaD… Olas … 2028-06-04   2028 304890 <NA>                      NA         0\n 3 AquaD… Shou … 2028-09-03   2028 303890 <NA>                      NA         0\n 4 AquaD… Shou … 2028-09-08   2028 306170 <NA>                      NA         0\n 5 AquaD… Costa… 2029-02-04   2029 304790 <NA>                      NA         0\n 6 AquaD… Costa… 2029-02-04   2029 304790 <NA>                      NA         0\n 7 AquaD… Elara… 2029-04-22   2029 304620 <NA>                      NA         0\n 8 AquaD… Elara… 2029-04-22   2029 304620 <NA>                      NA         0\n 9 AquaD… Fishe… 2030-03-10   2030 304990 <NA>                      NA         0\n10 AquaD… Fishe… 2033-09-23   2033 304990 <NA>                      NA         0\n# ℹ 739,353 more rows\n# ℹ 2 more variables: weightkg <int>, valueofgoodsusd <dbl>\n\n\nTherefore, we created a new column to store the hscode_parent where it will contain only the first three digit of the hscode. With this, we then map the description from Connect2India Harmonized System Code to gleam further insights into the worded description.\n\n\nShow code\nMC2_edges <- MC2_edges %>%\n  mutate(hscode_parent = substr(hscode, 1, 3))\n\nMC2_edges <- MC2_edges %>%\n  left_join(hscode_map_thirdparent, by = c(\"hscode_parent\" = \"HS_Code\")) %>%\n  select(-S.No.)%>%\n  rename(hscode_des = Description.x, hscode_parent_des = Description.y)\n\nMC2_edges\n\n\n# A tibble: 739,363 × 12\n   source  target arrivaldate  year hscode hscode_des valueofgoods_omu volumeteu\n   <chr>   <chr>  <date>      <dbl> <chr>  <chr>                 <dbl>     <dbl>\n 1 AquaDe… Océan… 2028-04-12   2028 304890 <NA>                     NA         0\n 2 AquaDe… Olas … 2028-06-04   2028 304890 <NA>                     NA         0\n 3 AquaDe… Shou … 2028-09-03   2028 303890 <NA>                     NA         0\n 4 AquaDe… Shou … 2028-09-08   2028 306170 <NA>                     NA         0\n 5 AquaDe… Costa… 2029-02-04   2029 304790 <NA>                     NA         0\n 6 AquaDe… Costa… 2029-02-04   2029 304790 <NA>                     NA         0\n 7 AquaDe… Elara… 2029-04-22   2029 304620 <NA>                     NA         0\n 8 AquaDe… Elara… 2029-04-22   2029 304620 <NA>                     NA         0\n 9 AquaDe… Fishe… 2030-03-10   2030 304990 <NA>                     NA         0\n10 AquaDe… Fishe… 2033-09-23   2033 304990 <NA>                     NA         0\n# ℹ 739,353 more rows\n# ℹ 4 more variables: weightkg <int>, valueofgoodsusd <dbl>,\n#   hscode_parent <chr>, hscode_parent_des <chr>\n\n\n\n\n3.7 Cleaning of NA records within MC2_nodes and MC2_edges\nThereafter, we checked MC2_nodes and MC2_edges for NA records via the code chunk below.\n\n\nShow code\nany(is.na(MC2_nodes))\n\n\n[1] TRUE\n\n\nShow code\nany(is.na(MC2_edges))\n\n\n[1] TRUE\n\n\nAs seen from the results above, there are NA records. Therefore, we performed cleaning on the MC2_nodes to remove shpcountry that has na records or rcvcountry that has na records. Additionally, we also removed duplicated IDs within MC2_nodes. The cleaned data is then stored in MC2_nodes_cleaned.\n\n\nShow code\n#keeps distinct id and removes NA records within nodes\nMC2_nodes_cleaned <- MC2_nodes %>%\n  distinct(id, .keep_all = TRUE) %>%\n  filter(!is.na(shpcountry) | !is.na(rcvcountry))\n\n\nNext, we want to determine what are the NA records within MC2_edges, and the extent of missing records within each columns. We can see that valueofgoods_omu has a total of 99.97% missing records, followed by valueofgoodsusd with 3.10% and volumeteu with 0.19%. We also note that hscode_des has 100% missing records as the specific hscodes are not found in Connect2India Harmonized System Code records.\nDespite the below results, we will continue to keep the records at this stage, as we are looking to derive a new column called “weight” in the subsequent stage.\n\n\nShow code\nna_percentages <- MC2_edges %>%\n  summarise(across(everything(), ~ mean(is.na(.)) * 100, .names = \"NA_{.col}\"))\n\nna_percentages\n\n\n# A tibble: 1 × 12\n  NA_source NA_target NA_arrivaldate NA_year NA_hscode NA_hscode_des\n      <dbl>     <dbl>          <dbl>   <dbl>     <dbl>         <dbl>\n1         0         0              0       0         0          74.1\n# ℹ 6 more variables: NA_valueofgoods_omu <dbl>, NA_volumeteu <dbl>,\n#   NA_weightkg <dbl>, NA_valueofgoodsusd <dbl>, NA_hscode_parent <dbl>,\n#   NA_hscode_parent_des <dbl>"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#exploratory-data-analysis",
    "title": "Take-home Exercise 2",
    "section": "4. Exploratory Data Analysis",
    "text": "4. Exploratory Data Analysis\n\n4.1 Shipping volume (in terms of weight kg) over the years\nWe begin our overall exploratory data analysis by understanding the volume of fishing shipments in terms of weight (kg) over the years. We utilise ggplot to plot a bar chart by weight-kg (y-axis) over the years (x-axis). We also utilised scale_y_continuous to format the labels to “billions”. Additionally, scale_x_discrete was used to enable the x-axis to be unique values of the year variable from MC2_edges dataframe.\nAs seen from the plot below, we can see an overall increasing trend in terms of shipment volume. This makes managing IUU fishing even more pressing and urgent.\n\n\nShow code\nggplot(MC2_edges, aes(x = as.factor(year), y = weightkg)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Year\", y = \"Weight (kg)\", title = \"Weight over the Years\") +\n  scale_y_continuous(labels = function(x) paste0(x/1e9, \"B\")) +\n  scale_x_discrete(breaks = unique(MC2_edges$year)) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n4.2 Top 10 HSCodes and Parent HSCodes Frequency\nNext, we want to better understand what are the top hscode and parent hscode within MC2_edges. We leveraged on summarize() and arrange() to obtain the top 10 most frequent occurrences of hscode and hscode_parent within MC2_edges dataset.\n\n4.2.1 Top 10 Parent HSCodes\nParent HSCodes starting with 304, 160 and 306 are the top three in terms of frequecy counts.\n\n\nShow code\ntop10_by_hscodeparent <- MC2_edges %>%\n  group_by(hscode_parent) %>%\n  summarize(n = n()) %>%\n  arrange(desc(n)) %>%\n  top_n(10)\n\ntop10_by_hscodeparent_plot <- top10_by_hscodeparent %>%\n  plot_ly(x = ~reorder(hscode_parent, n), y = ~n, type = \"bar\") %>%\n  layout(xaxis = list(title = \"Parent HSCode\"), yaxis = list(title = \"Frequency\"))\n\ntop10_by_hscodeparent_plot\n\n\n\n\n\n\n\n\n4.2.2 Top 10 HSCodes\nNext, we want to better understand what are the top hscode within MC2_edges. The top three HSCodes are 306170, 304620 and 160414.\n\n\nShow code\ntop10_by_hscode <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(n = n()) %>%\n  arrange(desc(n)) %>%\n  top_n(10)\n\ntop10_by_hscode_plot <- top10_by_hscode %>%\n  plot_ly(x = ~reorder(hscode, n), y = ~n, type = \"bar\") %>%\n  layout(xaxis = list(title = \"HSCode\"), yaxis = list(title = \"Frequency\"))\n\ntop10_by_hscode_plot\n\n\n\n\n\n\n\n\n\n4.3 Top 10 HSCodes total valueofgoodsusd and weightkg\nWe calculate the top 10 HSCodes for total value of goodsusd and weightkg before combining the separate bar plots for the said two metrics into a single subplot for comparison via subplot().\n\n\nShow code\nvalueofgoodsusd_by_hscode_top10 <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(total_valueofgoodsusd = sum(valueofgoodsusd, na.rm = TRUE)) %>%\n  arrange(total_valueofgoodsusd) %>%\n  top_n(10)\n\nvalueofgoodsusd_by_hscode_top10_plot <- plot_ly(\n  data = valueofgoodsusd_by_hscode_top10,\n  x = ~hscode,\n  y = ~total_valueofgoodsusd,\n  type = \"bar\",\n  marker = list(color = \"rgba(0, 0, 255, 0.7)\"),\n  hovertemplate = \"HSCode: %{x}<br>Total Value of Goods USD: %{y}\",\n  name = \"Value of Goods USD\"\n)\n\nweightkg_by_hscode_top10 <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(total_weight = sum(weightkg, na.rm = TRUE)) %>%\n  arrange(total_weight) %>%\n  top_n(10)\n\nweightkg_by_hscode_top10_plot <- plot_ly(\n  data = weightkg_by_hscode_top10,\n  x = ~hscode,\n  y = ~total_weight,\n  type = \"bar\",\n  marker = list(color = \"rgba(255, 0, 0, 0.7)\"),\n  hovertemplate = \"HSCode: %{x}<br>Total Weight: %{y}\",\n  name = \"Total Weight\"\n)\n\ncombined_plot_top10_valueofgoodsusd_weightkg <- subplot(\n  valueofgoodsusd_by_hscode_top10_plot,\n  weightkg_by_hscode_top10_plot,\n  nrows = 1,\n  shareX = TRUE,\n  shareY = FALSE\n) %>%\n  layout(\n    title = \"Top 10 Value of Goods USD and Total Weight (kg) by HSCode\",\n    xaxis = list(title = \"HSCode\"),\n    yaxis = list(title = \"Total Value of Goods USD / Total Weight (kg)\"),\n    legend = list(title = \"Metrics\")\n  )\n\ncombined_plot_top10_valueofgoodsusd_weightkg\n\n\n\n\n\n\n\n\n4.4 Bottom 10 HSCodes total valueofgoodsusd and weightkg\nWe calculate the bottom 10 HSCodes for total value of goodsusd and weightkg before combining the separate bar plots for the said two metrics into a single subplot for comparison via subplot().\n\n\nShow code\nvalueofgoodsusd_by_hscode_bottom10 <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(total_valueofgoodsusd = sum(valueofgoodsusd, na.rm = TRUE)) %>%\n  arrange(total_valueofgoodsusd) %>%\n  head(10)\n\nvalueofgoodsusd_by_hscode_bottom10_plot <- plot_ly(\n  data = valueofgoodsusd_by_hscode_bottom10,\n  x = ~hscode,\n  y = ~total_valueofgoodsusd,\n  type = \"bar\",\n  marker = list(color = \"rgba(0, 0, 255, 0.7)\"),\n  hovertemplate = \"HSCode: %{x}<br>Total Value of Goods USD: %{y}\",\n  name = \"Value of Goods USD\"\n)\n\nweightkg_by_hscode_bottom10 <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(total_weight = sum(weightkg, na.rm = TRUE)) %>%\n  arrange(total_weight) %>%\n  head(10)\n\nweightkg_by_hscode_bottom10_plot <- plot_ly(\n  data = weightkg_by_hscode_bottom10,\n  x = ~hscode,\n  y = ~total_weight,\n  type = \"bar\",\n  marker = list(color = \"rgba(255, 0, 0, 0.7)\"),\n  hovertemplate = \"HSCode: %{x}<br>Total Weight: %{y}\",\n  name = \"Total Weight\"\n)\n\ncombined_plot_bottom10_valueofgoodsusd_weightkg <- subplot(\n  valueofgoodsusd_by_hscode_bottom10_plot,\n  weightkg_by_hscode_bottom10_plot,\n  nrows = 1,\n  shareX = TRUE,\n  shareY = FALSE\n) %>%\n  layout(\n    title = \"Bottom 10 Value of Goods USD and Total Weight (kg) by HSCode\",\n    xaxis = list(title = \"HSCode\"),\n    yaxis = list(title = \"Total Value of Goods USD / Total Weight (kg)\"),\n    legend = list(title = \"Metrics\")\n  )\n\ncombined_plot_bottom10_valueofgoodsusd_weightkg\n\n\n\n\n\n\n\n\n4.5 Top 8 Sources (per year) across the Years by varying metrics\nWithin this section, we take a look a the top 8 sources (per year) across the years by weightkg and value of goods usd. We looped over the years to calculate the top 8 sources by the relevant metrics for each year, before leveraging on plot_ly() to create a line chart to visualise the trends over the years.\n\n4.5.1 Weightkg\n\n\nShow code\n#Define years\nstart_year <- 2028\nend_year <- 2034\n\n#Create empty data frame to store the data for each year\ndf_weightkg_sources <- data.frame(year = numeric(),\n                   source = character(),\n                   weightkg = numeric(),\n                   stringsAsFactors = FALSE)\n\n#Loop over the years\nfor (year_filter in start_year:end_year) {\n  # Filter the data for the current year\n  MC2_edges_filtered <- filter(MC2_edges, year == year_filter)\n  \n  #Group the filtered data by source and calculate the total weight\n  top8_by_source <- MC2_edges_filtered %>%\n    group_by(source) %>%\n    summarize(weightkg = sum(weightkg)) %>%\n    arrange(desc(weightkg)) %>%\n    top_n(8)\n  \n  #Add the year to the data frame\n  top8_by_source$year <- year_filter\n  \n  #Append the data for the current year to the overall data frame\n  df_weightkg_sources <- bind_rows(df_weightkg_sources, top8_by_source)\n}\n\nline_chart <- df_weightkg_sources %>%\n  plot_ly(x = ~year, y = ~weightkg, color = ~source, type = \"scatter\", mode = \"lines+markers\",\n          colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\", \"#A6761D\", \"#666666\"),\n          text = ~paste(\"Source: \", source, \"<br>Weight: \", weightkg, \" kg\")) %>%\n  layout(xaxis = list(title = \"Year\"), yaxis = list(title = \"Weight (kg)\"),\n         title = \"Top 8 Sources by Weight (kg)\")\n\nline_chart\n\n\n\n\n\n\n\n\n4.5.2 Value of Goods USD\n\n\nShow code\n#Define years\nstart_year <- 2028\nend_year <- 2034\n\n#Create empty data frame to store the data for each year\ndf_valueofgoodsusd_sources <- data.frame(year = numeric(),\n                   source = character(),\n                   valueofgoodsusd = numeric(),\n                   stringsAsFactors = FALSE)\n\n#Loop over the years\nfor (year_filter in start_year:end_year) {\n  #Filter the data for the current year\n  MC2_edges_filtered <- filter(MC2_edges, year == year_filter)\n  \n  #Group the filtered data by source and calculate the total weight\n  top8_by_source <- MC2_edges_filtered %>%\n    group_by(source) %>%\n    summarize(valueofgoodsusd = sum(valueofgoodsusd)) %>%\n    arrange(desc(valueofgoodsusd)) %>%\n    top_n(8)\n  \n  #Add the year to the data frame\n  top8_by_source$year <- year_filter\n  \n  #Append the data for the current year to the overall data frame\n  df_valueofgoodsusd_sources <- bind_rows(df_valueofgoodsusd_sources, top8_by_source)\n}\n\nline_chart <- df_valueofgoodsusd_sources %>%\n  plot_ly(x = ~year, y = ~valueofgoodsusd, color = ~source, type = \"scatter\", mode = \"lines+markers\",\n          colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\", \"#A6761D\", \"#666666\"),\n          text = ~paste(\"Source: \", source, \"<br>Weight: \", valueofgoodsusd, \" kg\")) %>%\n  layout(xaxis = list(title = \"Year\"), yaxis = list(title = \"valueofgoodsusd\"),\n         title = \"Top 8 Sources by valueofgoodsusd\")\n\nline_chart\n\n\n\n\n\n\n\n\n\n4.6 Top 8 Targets (per year) across the Years by varying metrics\nWithin this section, we take a look a the top 8 targets (per year) across the years by weightkg and value of goods usd. We looped over the years to calculate the top 8 targets by the relevant metrics for each year, before leveraging on plot_ly() to create a line chart to visualise the trends over the years.\n\n4.6.1 Weightkg\n\n\nShow code\n#Define years\nstart_year <- 2028\nend_year <- 2034\n\n#Create empty data frame to store the data for each year\ndf_weightkg_targets <- data.frame(year = numeric(),\n                   target = character(),\n                   weightkg = numeric(),\n                   stringsAsFactors = FALSE)\n\n#Loop over the years\nfor (year_filter in start_year:end_year) {\n  #Filter the data for the current year\n  MC2_edges_filtered <- filter(MC2_edges, year == year_filter)\n  \n  #Group the filtered data by target and calculate the total weight\n  top8_by_target <- MC2_edges_filtered %>%\n    group_by(target) %>%\n    summarize(weightkg = sum(weightkg)) %>%\n    arrange(desc(weightkg)) %>%\n    top_n(8)\n  \n  #Add the year to the data frame\n  top8_by_target$year <- year_filter\n  \n  #Append the data for the current year to the overall data frame\n  df_weightkg_targets <- bind_rows(df_weightkg_targets, top8_by_target)\n}\n\nline_chart <- df_weightkg_targets %>%\n  plot_ly(x = ~year, y = ~weightkg, color = ~target, type = \"scatter\", mode = \"lines+markers\",\n          colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\", \"#A6761D\", \"#666666\"),\n          text = ~paste(\"Target: \", target, \"<br>Weight: \", weightkg, \" kg\")) %>%\n  layout(xaxis = list(title = \"Year\"), yaxis = list(title = \"Weight (kg)\"),\n         title = \"Top 8 Targets by Weight (kg)\")\n\nline_chart\n\n\n\n\n\n\n\n\n4.6.2 Value of Goods USD\n\n\nShow code\n#Define years\nstart_year <- 2028\nend_year <- 2034\n\n#Create empty data frame to store the data for each year\ndf_valueofgoodsusd_targets <- data.frame(year = numeric(),\n                   target = character(),\n                   valueofgoodsusd = numeric(),\n                   stringsAsFactors = FALSE)\n\n#Loop over the years\nfor (year_filter in start_year:end_year) {\n  # Filter the data for the current year\n  MC2_edges_filtered <- filter(MC2_edges, year == year_filter)\n  \n  #Group filtered data by source and calculate the total weight\n  top8_by_target <- MC2_edges_filtered %>%\n    group_by(target) %>%\n    summarize(valueofgoodsusd = sum(valueofgoodsusd)) %>%\n    arrange(desc(valueofgoodsusd)) %>%\n    top_n(8)\n  \n  #Add the year to the data frame\n  top8_by_target$year <- year_filter\n  \n  #Append the data for the current year to the overall data frame\n  df_valueofgoodsusd_targets <- bind_rows(df_valueofgoodsusd_targets, top8_by_target)\n}\n\nline_chart <- df_valueofgoodsusd_targets %>%\n  plot_ly(x = ~year, y = ~valueofgoodsusd, color = ~target, type = \"scatter\", mode = \"lines+markers\",\n          colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\", \"#A6761D\", \"#666666\"),\n          text = ~paste(\"Target: \", target, \"<br>Weight: \", valueofgoodsusd, \" kg\")) %>%\n  layout(xaxis = list(title = \"Year\"), yaxis = list(title = \"valueofgoodsusd\"),\n         title = \"Top 8 Target by valueofgoodsusd\")\n\nline_chart\n\n\n\n\n\n\nWith the above Exploratory Data Analysis setting a good foundation for further visualisation on the network graph, we first begin by preparing the edges and nodes properly per code chunks below:\nThe below code chunk creates a new MC2_edges_cleaned factoring in the below:\n\ncreation of new column called weight that counts the number of times the source and target connects in a year\ncreation of new column called noofunique_targets that counts the number of unique source target relationship\nfilter away records where source is not equal to target\nselect all records where weights existing in the top 95 percentile\n\n\n\nShow code\nMC2_edges_weight <- MC2_edges %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  #filter(weights > 20) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.95))\n\n\nThereafter, the below code chunk creates “MC2_nodes_extracted” data table to ensure that the nodes in nodes data table includes all the source and target values. Therefore, a join is performed to populate shpcountry and rcvcountery back to the “MC2_nodes_extracted” data table.\n\n\nShow code\nid1_weight <- MC2_edges_weight %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight <- MC2_edges_weight %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight <- rbind(id1_weight, id2_weight) %>%\n  distinct()\n\nMC2_nodes_extracted_weight <- left_join(MC2_nodes_extracted_weight, MC2_nodes_cleaned, by = \"id\")\n\n\nA check can also be performed to ensure that all source and targets within MC2_edges_cleaned are found in MC2_nodes_extracted.\n\n\nShow code\n# Check if sources and targets within MC2_edges_cleaned are found in MC2_nodes_extracted$id\nsources_check_weight <- MC2_edges_weight$source %in% MC2_nodes_extracted_weight$id\ntargets_check_weight <- MC2_edges_weight$target %in% MC2_nodes_extracted_weight$id\nsources_found_weight <- all(sources_check_weight)\ntargets_found_weight <- all(targets_check_weight)\n\ncat(\"All sources found in MC2_nodes_extracted_weight 'id':\", sources_found_weight, \"\\n\")\n\n\nAll sources found in MC2_nodes_extracted_weight 'id': TRUE \n\n\nShow code\ncat(\"All targets found in MC2_nodes_extracted_weight 'id':\", targets_found_weight, \"\\n\")\n\n\nAll targets found in MC2_nodes_extracted_weight 'id': TRUE"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#hscode-exploration-within-mc2_edges",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#hscode-exploration-within-mc2_edges",
    "title": "Take-home Exercise 2",
    "section": "4.1 HSCode Exploration within MC2_edges",
    "text": "4.1 HSCode Exploration within MC2_edges\n\n4.1.1 Top parent HS Codes within MC2_edges\nNext, we want to better understand what are the top parent hscode within MC2_edges.\n\ntop10_by_segment <- MC2_edges %>%\n  group_by(hscode_parent) %>%\n  summarize(n = n()) %>%\n  arrange(desc(n)) %>%\n  top_n(10)\n\ntop10_by_segment_plot <- top10_by_segment %>%\n  plot_ly(x = ~reorder(hscode_parent, n), y = ~n, type = \"bar\") %>%\n  layout(xaxis = list(title = \"Parent HSCode\"), yaxis = list(title = \"Frequency\"))\n\ntop10_by_segment_plot\n\n\n\n\n\n\n\n4.1.2 Top HS Codes within MC2_edges\nNext, we want to better understand what are the top hscode within MC2_edges.\n\ntop10_by_hscode <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(n = n()) %>%\n  arrange(desc(n)) %>%\n  top_n(10)\n\ntop10_by_hscode_plot <- top10_by_hscode %>%\n  plot_ly(x = ~reorder(hscode, n), y = ~n, type = \"bar\") %>%\n  layout(xaxis = list(title = \"HSCode\"), yaxis = list(title = \"Frequency\"))\n\ntop10_by_hscode_plot\n\n\n\n\n\n\n#hide\nweightkg_by_hscode_top10 <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(total_weight = sum(weightkg, na.rm = TRUE)) %>%\n  arrange(total_weight) %>%\n  top_n(10)\n  #top_n(10, wt = -total_weight)\n\nweightkg_by_hscode_top10_plot <- plot_ly(data = weightkg_by_hscode_top10, x = ~hscode, y = ~total_weight, type = \"bar\") %>%\n  layout(title = \"Total Weight by HSCode\",\n         xaxis = list(title = \"HSCode\"),\n         yaxis = list(title = \"Total Weight\"))\n\nweightkg_by_hscode_top10_plot\n\n\n\n\n\n\n#hide\nweightkg_by_hscode_bottom10 <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(total_weight = sum(weightkg, na.rm = TRUE)) %>%\n  arrange(total_weight) %>%\n  head(10)\n  #top_n(10, wt = -total_weight)\n\nweightkg_by_hscode_bottom10_plot <- plot_ly(data = weightkg_by_hscode_bottom10, x = ~hscode, y = ~total_weight, type = \"bar\") %>%\n  layout(title = \"Total Weight by HSCode\",\n         xaxis = list(title = \"HSCode\"),\n         yaxis = list(title = \"Total Weight\"))\n\nweightkg_by_hscode_bottom10_plot\n\n\n\n\n\n\n#hide\nweightkg_by_hscode_parent_top10 <- MC2_edges %>%\n  group_by(hscode_parent) %>%\n  summarize(total_weight = sum(weightkg, na.rm = TRUE)) %>%\n  arrange(total_weight) %>%\n  top_n(10)\n  #top_n(10, wt = -total_weight)\n\nweightkg_by_hscode_parent_top10_plot <- plot_ly(data = weightkg_by_hscode_parent_top10, x = ~hscode_parent, y = ~total_weight, type = \"bar\") %>%\n  layout(title = \"Total Weight by HSCode_Parent\",\n         xaxis = list(title = \"HSCode_Parent\"),\n         yaxis = list(title = \"Total Weight\"))\n\nweightkg_by_hscode_parent_top10_plot\n\n\n\n\n\n\n#hide\nvalueofgoodsusd_by_hscode_top10 <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(total_valueofgoodsusd = sum(valueofgoodsusd, na.rm = TRUE)) %>%\n  arrange(total_valueofgoodsusd) %>%\n  top_n(10)\n  #top_n(10, wt = -total_weight)\n\nvalueofgoodsusd_by_hscode_top10_plot <- plot_ly(data = valueofgoodsusd_by_hscode_top10, x = ~hscode, y = ~total_valueofgoodsusd, type = \"bar\") %>%\n  layout(title = \"Total valueofgoodsusd by HSCode\",\n         xaxis = list(title = \"HSCode\"),\n         yaxis = list(title = \"Total valueofgoodsusd\"))\n\nvalueofgoodsusd_by_hscode_top10_plot\n\n\n\n\n\n\n#hide\nvalueofgoodsusd_by_hscode_bottom10 <- MC2_edges %>%\n  group_by(hscode) %>%\n  summarize(total_valueofgoodsusd = sum(valueofgoodsusd, na.rm = TRUE)) %>%\n  arrange(total_valueofgoodsusd) %>%\n  head(10)\n  #top_n(10, wt = -total_weight)\n\nvalueofgoodsusd_by_hscode_bottom10_plot <- plot_ly(data = valueofgoodsusd_by_hscode_bottom10, x = ~hscode, y = ~total_valueofgoodsusd, type = \"bar\") %>%\n  layout(title = \"Total valueofgoodsusd by HSCode\",\n         xaxis = list(title = \"HSCode\"),\n         yaxis = list(title = \"Total valueofgoodsusd\"))\n\nvalueofgoodsusd_by_hscode_bottom10_plot\n\n\n\n\n\n\n#hide\nvalueofgoodsusd_by_hscode_parent_top10 <- MC2_edges %>%\n  group_by(hscode_parent) %>%\n  summarize(total_valueofgoodsusd = sum(valueofgoodsusd, na.rm = TRUE)) %>%\n  arrange(total_valueofgoodsusd) %>%\n  top_n(10)\n  #top_n(10, wt = -total_weight)\n\nvalueofgoodsusd_by_hscode_parent_top10_plot <- plot_ly(data = valueofgoodsusd_by_hscode_parent_top10, x = ~hscode_parent, y = ~total_valueofgoodsusd, type = \"bar\") %>%\n  layout(title = \"Total valueofgoodsusd by HSCode_Parent\",\n         xaxis = list(title = \"HSCode_Parent\"),\n         yaxis = list(title = \"Total valueofgoodsusd\"))\n\nvalueofgoodsusd_by_hscode_parent_top10_plot\n\n\n\n\n\n\n\n4.1.3 Top 10 total valueofgoodsusd and weightkg by HS Codes\n\nvalueofgoodsusd_by_hscode_top10_plot <- plot_ly(\n  data = valueofgoodsusd_by_hscode_top10,\n  x = ~hscode,\n  y = ~total_valueofgoodsusd,\n  type = \"bar\",\n  marker = list(color = \"rgba(0, 0, 255, 0.7)\"),\n  hovertemplate = \"HSCode: %{x}<br>Total Value of Goods USD: %{y}\",\n  name = \"Value of Goods USD\"\n)\n\nweightkg_by_hscode_top10_plot <- plot_ly(\n  data = weightkg_by_hscode_top10,\n  x = ~hscode,\n  y = ~total_weight,\n  type = \"bar\",\n  marker = list(color = \"rgba(255, 0, 0, 0.7)\"),\n  hovertemplate = \"HSCode: %{x}<br>Total Weight: %{y}\",\n  name = \"Total Weight\"\n)\n\ncombined_plot_top10 <- subplot(\n  valueofgoodsusd_by_hscode_top10_plot,\n  weightkg_by_hscode_top10_plot,\n  nrows = 1,\n  shareX = TRUE,\n  shareY = FALSE\n) %>%\n  layout(\n    title = \"Total Value of Goods USD and Total Weight by HSCode\",\n    xaxis = list(title = \"HSCode\"),\n    yaxis = list(title = \"Total Value of Goods USD / Total Weight\"),\n    legend = list(title = \"Metrics\")\n  )\n\ncombined_plot_top10\n\n\n\n\n\n\n\n4.1.4 Bottom 10 total valueofgoodsusd and weightkg by HS Codes\n\nvalueofgoodsusd_by_hscode_bottom10_plot <- plot_ly(\n  data = valueofgoodsusd_by_hscode_bottom10,\n  x = ~hscode,\n  y = ~total_valueofgoodsusd,\n  type = \"bar\",\n  marker = list(color = \"rgba(0, 0, 255, 0.7)\"),\n  hovertemplate = \"HSCode: %{x}<br>Total Value of Goods USD: %{y}\",\n  name = \"Value of Goods USD\"\n)\n\nweightkg_by_hscode_bottom10_plot <- plot_ly(\n  data = weightkg_by_hscode_bottom10,\n  x = ~hscode,\n  y = ~total_weight,\n  type = \"bar\",\n  marker = list(color = \"rgba(255, 0, 0, 0.7)\"),\n  hovertemplate = \"HSCode: %{x}<br>Total Weight: %{y}\",\n  name = \"Total Weight\"\n)\n\ncombined_plot_bottom10 <- subplot(\n  valueofgoodsusd_by_hscode_bottom10_plot,\n  weightkg_by_hscode_bottom10_plot,\n  nrows = 1,\n  shareX = TRUE,\n  shareY = FALSE\n) %>%\n  layout(\n    title = \"Total Value of Goods USD and Total Weight by HSCode\",\n    xaxis = list(title = \"HSCode\"),\n    yaxis = list(title = \"Total Value of Goods USD / Total Weight\"),\n    legend = list(title = \"Metrics\")\n  )\n\ncombined_plot_bottom10\n\n\n\n\n\n\n\n4.1.5 Top 20 Sources by weightkg across the Years\n\n#Define years\nstart_year <- 2028\nend_year <- 2034\n\n#Create empty data frame to store the data for each year\ndata <- data.frame(year = numeric(),\n                   source = character(),\n                   weightkg = numeric(),\n                   stringsAsFactors = FALSE)\n\n#Loop over the years\nfor (year_filter in start_year:end_year) {\n  # Filter the data for the current year\n  MC2_edges_filtered <- filter(MC2_edges, year == year_filter)\n  \n  #Group the filtered data by source and calculate the total weight\n  top20_by_source <- MC2_edges_filtered %>%\n    group_by(source) %>%\n    summarize(weightkg = sum(weightkg)) %>%\n    arrange(desc(weightkg)) %>%\n    top_n(20)\n  \n  #Add the year to the data frame\n  top20_by_source$year <- year_filter\n  \n  #Append the data for the current year to the overall data frame\n  data <- bind_rows(data, top20_by_source)\n}\n\n#Create the line chart with hover text\nline_chart <- data %>%\n  plot_ly(x = ~year, y = ~weightkg, color = ~source, type = \"scatter\", mode = \"lines+markers\",\n          colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\", \"#A6761D\", \"#666666\"),\n          text = ~paste(\"Source: \", source, \"<br>Weight: \", weightkg, \" kg\")) %>%\n  layout(xaxis = list(title = \"Year\"), yaxis = list(title = \"Weight (kg)\"),\n         title = \"Top 20 Sources by Weight (kg)\")\n\n#Display the line chart\nline_chart\n\n\n\n\n\n\n\n4.1.6 Top 20 Sources by valueofgoodsusd across the Years\n\n#Define years\nstart_year <- 2028\nend_year <- 2034\n\n#Create empty data frame to store the data for each year\ndata <- data.frame(year = numeric(),\n                   source = character(),\n                   valueofgoodsusd = numeric(),\n                   stringsAsFactors = FALSE)\n\n#Loop over the years\nfor (year_filter in start_year:end_year) {\n  #Filter the data for the current year\n  MC2_edges_filtered <- filter(MC2_edges, year == year_filter)\n  \n  #Group the filtered data by source and calculate the total weight\n  top20_by_source <- MC2_edges_filtered %>%\n    group_by(source) %>%\n    summarize(valueofgoodsusd = sum(valueofgoodsusd)) %>%\n    arrange(desc(valueofgoodsusd)) %>%\n    top_n(20)\n  \n  #Add the year to the data frame\n  top20_by_source$year <- year_filter\n  \n  #Append the data for the current year to the overall data frame\n  data <- bind_rows(data, top20_by_source)\n}\n\n#Create the line chart with hover text\nline_chart <- data %>%\n  plot_ly(x = ~year, y = ~valueofgoodsusd, color = ~source, type = \"scatter\", mode = \"lines+markers\",\n          colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\", \"#A6761D\", \"#666666\"),\n          text = ~paste(\"Source: \", source, \"<br>Weight: \", valueofgoodsusd, \" kg\")) %>%\n  layout(xaxis = list(title = \"Year\"), yaxis = list(title = \"valueofgoodsusd\"),\n         title = \"Top 20 Sources by valueofgoodsusd\")\n\n#Display the line chart\nline_chart\n\n\n\n\n\n\n\n4.1.7 Top 20 Targets by weightkg across the Years\n\n#Define years\nstart_year <- 2028\nend_year <- 2034\n\n#Create empty data frame to store the data for each year\ndata <- data.frame(year = numeric(),\n                   target = character(),\n                   weightkg = numeric(),\n                   stringsAsFactors = FALSE)\n\n#Loop over the years\nfor (year_filter in start_year:end_year) {\n  #Filter the data for the current year\n  MC2_edges_filtered <- filter(MC2_edges, year == year_filter)\n  \n  #Group the filtered data by target and calculate the total weight\n  top20_by_target <- MC2_edges_filtered %>%\n    group_by(target) %>%\n    summarize(weightkg = sum(weightkg)) %>%\n    arrange(desc(weightkg)) %>%\n    top_n(20)\n  \n  #Add the year to the data frame\n  top20_by_target$year <- year_filter\n  \n  #Append the data for the current year to the overall data frame\n  data <- bind_rows(data, top20_by_target)\n}\n\n#Create the line chart with hover text\nline_chart <- data %>%\n  plot_ly(x = ~year, y = ~weightkg, color = ~target, type = \"scatter\", mode = \"lines+markers\",\n          colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\", \"#A6761D\", \"#666666\"),\n          text = ~paste(\"Target: \", target, \"<br>Weight: \", weightkg, \" kg\")) %>%\n  layout(xaxis = list(title = \"Year\"), yaxis = list(title = \"Weight (kg)\"),\n         title = \"Top 20 Targets by Weight (kg)\")\n\n#Display the line chart\nline_chart\n\n\n\n\n\n\n\n4.1.8 Top 20 Targets by valueofgoodsusd across the Years\n\n#Define years\nstart_year <- 2028\nend_year <- 2034\n\n#Create empty data frame to store the data for each year\ndata <- data.frame(year = numeric(),\n                   target = character(),\n                   valueofgoodsusd = numeric(),\n                   stringsAsFactors = FALSE)\n\n#Loop over the years\nfor (year_filter in start_year:end_year) {\n  # Filter the data for the current year\n  MC2_edges_filtered <- filter(MC2_edges, year == year_filter)\n  \n  #Group filtered data by source and calculate the total weight\n  top20_by_target <- MC2_edges_filtered %>%\n    group_by(target) %>%\n    summarize(valueofgoodsusd = sum(valueofgoodsusd)) %>%\n    arrange(desc(valueofgoodsusd)) %>%\n    top_n(20)\n  \n  #Add the year to the data frame\n  top20_by_target$year <- year_filter\n  \n  #Append the data for the current year to the overall data frame\n  data <- bind_rows(data, top20_by_target)\n}\n\n#Create the line chart with hover text\nline_chart <- data %>%\n  plot_ly(x = ~year, y = ~valueofgoodsusd, color = ~target, type = \"scatter\", mode = \"lines+markers\",\n          colors = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\", \"#A6761D\", \"#666666\"),\n          text = ~paste(\"Target: \", target, \"<br>Weight: \", valueofgoodsusd, \" kg\")) %>%\n  layout(xaxis = list(title = \"Year\"), yaxis = list(title = \"valueofgoodsusd\"),\n         title = \"Top 20 Target by valueofgoodsusd\")\n\n#Display the line chart\nline_chart"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualisation-and-network-insights",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualisation-and-network-insights",
    "title": "Take-home Exercise 2",
    "section": "7. Visualisation and Network Insights",
    "text": "7. Visualisation and Network Insights"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#marker-check-here-for-hscode-filter",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#marker-check-here-for-hscode-filter",
    "title": "Take-home Exercise 2",
    "section": "*** Marker Check here for HSCode Filter ***",
    "text": "*** Marker Check here for HSCode Filter ***\n\n#create a new column called weight that counts the number of times the source and target connects\nMC2_edges_weight <- MC2_edges %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  #filter(weights > 20) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.95))\n\nThereafter, the below code chunk creates “MC2_nodes_extracted” data table to ensure that the nodes in nodes data table includes all the source and target values. Therefore, a join is performed to populate shpcountry and rcvcountery back to the “MC2_nodes_extracted” data table.\n\nid1_weight <- MC2_edges_weight %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight <- MC2_edges_weight %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight <- rbind(id1_weight, id2_weight) %>%\n  distinct()\n\nMC2_nodes_extracted_weight <- left_join(MC2_nodes_extracted_weight, MC2_nodes_cleaned, by = \"id\")\n\nA check can also be performed to ensure that all source and targets within MC2_edges_cleaned are found in MC2_nodes_extracted.\n\n# Check if sources and targets within MC2_edges_cleaned are found in MC2_nodes_extracted$id\nsources_check_weight <- MC2_edges_weight$source %in% MC2_nodes_extracted_weight$id\ntargets_check_weight <- MC2_edges_weight$target %in% MC2_nodes_extracted_weight$id\nsources_found_weight <- all(sources_check_weight)\ntargets_found_weight <- all(targets_check_weight)\n\ncat(\"All sources found in MC2_nodes_extracted_weight 'id':\", sources_found_weight, \"\\n\")\n\nAll sources found in MC2_nodes_extracted_weight 'id': TRUE \n\ncat(\"All targets found in MC2_nodes_extracted_weight 'id':\", targets_found_weight, \"\\n\")\n\nAll targets found in MC2_nodes_extracted_weight 'id': TRUE \n\n\n\n#can be updated to glimpse whatever we want to see\n#glimpse(MC2_edges_cleaned)\n#glimpse(MC2_nodes_extracted)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plotting-of-basic-static-graph",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plotting-of-basic-static-graph",
    "title": "Take-home Exercise 2",
    "section": "5.1 Plotting of basic static graph",
    "text": "5.1 Plotting of basic static graph\nWe first begin by plotting a basic static graph to gain an overall view of the node edge connection. “fr” (Fruchterman-Reingold) layout was utilised as it is a special force directed layout where nodes are drawn closer to nodes they are connected to.\n\n5.1.1 Overall graph by valueofgoodsusd, volumeteu, weightkg and weight\n###!!!modularity\n\nset.seed(123)\n\n# Create the graph with filtered edges\nMC2_graph_undirected <- tbl_graph(nodes = MC2_nodes_extracted_weight, edges = MC2_edges_weight, directed = FALSE)\n\n# Calculate modularity using the Louvain algorithm\nmodularity <- cluster_louvain(MC2_graph_undirected, resolution = 0.5)\n\n# Assign categories based on modularity\nMC2_nodes_extracted_weight$modularitycategory <- as.character(modularity$membership)\n\n# Get top nodes based on modularity\ntop_nodes <- MC2_nodes_extracted_weight %>%\n  filter(id %in% names(which.max(modularity$membership)))\n\n# Visualize the graph using ggraph\nggraph(MC2_graph_undirected, layout = \"fr\") +\n  geom_edge_link(color = \"grey\", alpha = 0.7, arrow = arrow(length = unit(0.015, \"npc\"))) +\n  geom_node_point(aes(color = MC2_nodes_extracted_weight$modularitycategory)) +\n  scale_color_manual(values = rainbow(length(unique(modularity$membership)))) +\n  theme(legend.position = \"right\") +\n  labs(title = \"Graph with Group Components based on Modularity Maximization\")\n\n\n\n\n###!!!edgebetweenness\n\nset.seed(123)\n\n# Create the graph with filtered edges\nMC2_graph_undirected <- tbl_graph(nodes = MC2_nodes_extracted_weight, edges = MC2_edges_weight, directed = FALSE)\n\n# Calculate edge betweenness clustering\nclusters <- cluster_edge_betweenness(MC2_graph_undirected)\n\n# Assign categories based on the clusters\nMC2_nodes_extracted_weight$edge_betweenness_category <- as.character(clusters$membership)\n\n# Get top nodes based on the clusters\ntop_nodes <- MC2_nodes_extracted_weight %>%\n  filter(id %in% names(which.max(clusters$membership)))\n\n# Visualize the graph using ggraph\nggraph(MC2_graph_undirected, layout = \"fr\") +\n  geom_edge_link(color = \"grey\", alpha = 0.7, arrow = arrow(length = unit(0.015, \"npc\"))) +\n  geom_node_point(aes(color = MC2_nodes_extracted_weight$edge_betweenness_category)) +\n  scale_color_manual(values = rainbow(length(unique(clusters$membership)))) +\n  theme(legend.position = \"right\") +\n  labs(title = \"Graph with Group Components based on Edge Betweenness Clustering\")\n\n\n\n\n\n# Convert cluster counts and percentages into a tibble\n#cluster_summary <- tibble(\n#  Cluster = as.character(names(cluster_counts)),\n#  Count = as.integer(cluster_counts),\n#  Percentage = cluster_percentages,\n#)\n\n# Sort the cluster summary table by descending count\n#cluster_summary_sorted <- cluster_summary %>%\n#  arrange(desc(Count))\n\n#cluster_summary_sorted\n\n###!!!clusterleadingeigen\n\nset.seed(123)\n\n# Create the graph with filtered edges\nMC2_graph <- tbl_graph(nodes = MC2_nodes_extracted_weight, edges = MC2_edges_weight, directed = FALSE)\n\n# Convert the tbl_graph to an igraph object\nMC2_igraph <- as.igraph(MC2_graph)\n\n# Extract the edge weights from the MC2_edges_weight dataset\nedge_weights <- MC2_edges_weight$weights\n\n# Weave the edge weights into the graph\nE(MC2_igraph)$weight <- edge_weights\n\n# Perform leading eigenvector community detection\ngroups <- cluster_leading_eigen(MC2_igraph)\n\n# Assign categories based on the groups\nMC2_nodes_extracted_weight$category <- as.character(groups$membership)\n\n# Get top nodes based on the groups\ntop_nodes <- MC2_nodes_extracted_weight %>%\n  filter(id %in% names(which.max(groups$membership)))\n\n# Visualize the graph using ggraph\nggraph(MC2_graph, layout = \"fr\") +\n  geom_edge_link(color = \"grey\", alpha = 0.7, arrow = arrow(length = unit(0.015, \"npc\"))) +\n  geom_node_point(aes(color = MC2_nodes_extracted_weight$category)) +\n  scale_color_manual(values = rainbow(length(unique(groups$membership)))) +\n  theme(legend.position = \"right\") +\n  labs(title = \"Graph with Group Components based on Leading Eigenvector Centrality\")\n\n\n\n\n###!!!group components!!\n\n\n\n\n\n\n\n5.1.2 MC2_edges_weight by years (Degree Centrality)\n\nThe Code ChunkWeights_2028Weights_2029Weights_2030Weights_2031Weights_2032Weights_2033Weights_2034\n\n\nThe below shows a sample code chunk of how we can plot for the year 2028. Users need to take note to configure it to the respective years they are looking to analyse.\n\nset.seed(123)\n\nMC2_edges_weight_2028 <- MC2_edges %>%\n  filter(year == 2028) %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.80)) #or filter for unique target\n\nid1_weight_2028 <- MC2_edges_weight_2028 %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight_2028 <- MC2_edges_weight_2028 %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight_2028 <- rbind(id1_weight_2028, id2_weight_2028) %>%\n  distinct()\n\nMC2_nodes_extracted_weight_2028 <- left_join(MC2_nodes_extracted_weight_2028, MC2_nodes_cleaned, by = \"id\")\n\n# Create the graph with filtered edges\nMC2_graph_weights_2028 <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028, edges = MC2_edges_weight_2028, directed = TRUE)\n\n# Calculate degree centrality\ndegree_centrality <- degree(MC2_graph_weights_2028, mode = \"in\")\n# Define thresholds or criteria for category assignment\ndegree_threshold <- 2\n\n# Set size to degree centrality\nV(MC2_graph_weights_2028)$size <- degree_centrality\n\n# Assign categories based on centrality measures\nMC2_nodes_extracted_weight_2028$category <- ifelse(degree_centrality > degree_threshold, \"Central Carrier\", \"Fishing Vessel\")\n\n# Get top nodes based on degree centrality\ntop_nodes_2028 <- MC2_nodes_extracted_weight_2028 %>%\n  top_n(50, degree_centrality)\n\n# Visualize the filtered graph using ggraph\nggraph(MC2_graph_weights_2028, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(0.02, \"npc\"))) +\n  geom_node_point(aes(color = MC2_nodes_extracted_weight_2028$category, size = size)) +\n  scale_color_manual(values = c(\"Central Carrier\" = \"red\", \"Fishing Vessel\" = \"grey\")) +\n  scale_size_continuous(range = c(1, 4)) +\n  #geom_node_text(aes(label = ifelse(!is.na(MC2_nodes_extracted_weight_2028$category), MC2_nodes_extracted_weight_2028$category, \"\")), vjust = -1, hjust = 0, size = 3) +\n  theme(legend.position = \"right\") +\n  labs(title = \"Graph with Group Components and Size based on Degree Centrality\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmissing nodes\n\n# Create an empty data frame to store the disappeared nodes\nall_disappeared_nodes <- data.frame(id = character(), disappeared_year = numeric())\n\n# Iterate over the data frames for each year starting from 2029\nfor (year in 2029:2034) {\n  # Get the current year's top nodes\n  nodes_current <- get(paste0(\"top_nodes_\", year))\n  \n  # Get the previous year's top nodes\n  nodes_previous <- get(paste0(\"top_nodes_\", year - 1))\n  \n  # Find nodes that disappeared between years\n  disappeared_nodes <- data.frame(\n    id = nodes_previous$id[!nodes_previous$id %in% nodes_current$id],\n    disappeared_year = rep(year - 1, sum(!nodes_previous$id %in% nodes_current$id))\n  )\n  \n  # Append the disappeared nodes to the all_disappeared_nodes data frame\n  all_disappeared_nodes <- rbind(all_disappeared_nodes, disappeared_nodes)\n}\n\n# Sort the all_disappeared_nodes data frame by ID\nall_disappeared_nodes <- all_disappeared_nodes[order(all_disappeared_nodes$id), ]\n\n# Print the all_disappeared_nodes data frame\nprint(all_disappeared_nodes)\n\n                                                         id disappeared_year\n13                                                    -1203             2028\n39                                                     -481             2031\n10                                                     -533             2028\n15                           3 Oceanography ОАО Marine life             2029\n18                      Adriatic Mackerel Ges.m.b.H. Family             2029\n43                                Adriatic Tuna Pic Express             2031\n12 Agua Oceanfront Limited Liability Company Transportation             2028\n65 Agua Oceanfront Limited Liability Company Transportation             2033\n28                              AquaDelight N.V. Coral Reef             2030\n25                                Arunachal Pradesh s Brine             2030\n54                        Assam  Market Incorporated Tanker             2032\n2      AtlanticAppetite Oyster  Ltd. Corporation Investment             2028\n46     AtlanticAppetite Oyster  Ltd. Corporation Investment             2031\n64     AtlanticAppetite Oyster  Ltd. Corporation Investment             2033\n67                                 Belgian Cod BV Solutions             2033\n52                            Bihar  GmbH & Co. KG Carriers             2031\n68                           Caracola Azul Ltd. Corporation             2033\n62                                      Caracola Azul Oyj &             2032\n72                                Caracola del Mar A/S Line             2033\n40             Coastal Castaway Coast Corporation Worldwide             2031\n14                  Costa del Mar Ges.m.b.H. Marine biology             2029\n9                                         Costa del Sol A/S             2028\n66                                       Costa del Sol N.V.             2033\n36                        David Ltd. Liability Co Forwading             2031\n27                              Dubingiai Ltd. Liability Co             2030\n42                     Dutch  Ltd. Liability Co Consultants             2031\n71                                     Estrella de Mar Sagl             2033\n1                       Estrella del Mar Tilapia Oyj Marine             2028\n37                       Estrella del Sol Incorporated Aqua             2031\n74            Faroe Islands Halibut Sea S.p.A. Distribution             2033\n21                                 Finest Co Marine ecology             2029\n29                                  Great Ruaha River  GmbH             2030\n69                         Greek ermen Plc Marine sanctuary             2033\n8                                        hǎi dǎn AG Express             2028\n22                            Himachal Pradesh  BV Holdings             2030\n57                                Isla del Mar NV Forwading             2032\n44                                   Kerala  Market Sp Buoy             2031\n63                                    Kerala s S.A. de C.V.             2032\n24                    Lake Turkana  Charter Boat  NV Marine             2030\n23                             lǐyú GmbH & Co. KG Logistics             2030\n7                   Maharashtra  Ltd. Liability Co Shipping             2028\n45                  Maharashtra  Ltd. Liability Co Shipping             2031\n35                         Mandalore S.A. de C.V. Logistics             2031\n73                                         Mar de Plata Ltd             2033\n58                              Náutica del Sol Corporation             2032\n3     Niger Bend   Limited Liability Company Marine ecology             2028\n4                      Océano Azul Port S.A. de C.V. United             2028\n31          Océano de Coral Corporation Marine conservation             2030\n48                                   Olas del Mar Worldwide             2031\n41                               páng xiè S.A. de C.V. Line             2031\n53                         Panope Limited Liability Company             2032\n51                                      Punjab s A/S Marine             2031\n70                                      Punjab s A/S Marine             2033\n50                             Punjab s Marine conservation             2031\n19                                 Punjab Sea  CJSC Estuary             2029\n38                                  Rift Valley fishery Inc             2031\n55                         Saltwater Sisters AS Marine life             2032\n20                          Sea Breezes GmbH & Co. KG Shark             2029\n49                          Sea Breezes GmbH & Co. KG Shark             2031\n56                                     SeaBass &  SE Tanker             2032\n16              Seafaring Souls Ges.m.b.H. Marine sanctuary             2029\n17                           SeaFusion Ges.m.b.H. and Son's             2029\n11                       SeaTree Ltd. Liability Co Holdings             2028\n47                       SeaTree Ltd. Liability Co Holdings             2031\n5                     SilverStream CJSC Marine conservation             2028\n59                    SilverStream CJSC Marine conservation             2032\n60                                Spanish Anchovy SE Marine             2032\n61                 TideTable Limited Liability Company Wave             2032\n6                                   Turkish Mussels LLC and             2028\n32   Uttarakhand  Market Limited Liability Company Nautical             2030\n33                              Volga River LLC Enterprises             2030\n34                                         Yu er he  S.p.A.             2031\n26                        Yu gan  Sea spray GmbH Industrial             2030\n30                                  Yu xian  SRL Industrial             2030\n\n\n\n\n5.1.3 MC2_edges_weight by years (Eigenvector Centrality)\n\nThe Code ChunkWeights_2028Weights_2029 not changed yetWeights_2030Weights_2031Weights_2032Weights_2033Weights_2034\n\n\nThe below shows a sample code chunk of how we can plot for the year 2028. Users need to take note to configure it to the respective years they are looking to analyse.\n\n\n\nset.seed(123)\n\nMC2_edges_weight_2028 <- MC2_edges %>%\n  filter(year == 2028) %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.8)) #or filter for unique target\n\nid1_weight_2028 <- MC2_edges_weight_2028 %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight_2028 <- MC2_edges_weight_2028 %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight_2028 <- rbind(id1_weight_2028, id2_weight_2028) %>%\n  distinct()\n\nMC2_nodes_extracted_weight_2028 <- left_join(MC2_nodes_extracted_weight_2028, MC2_nodes_cleaned, by = \"id\")\n\n# Create the graph with filtered edges\nMC2_graph_weights_2028 <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028, edges = MC2_edges_weight_2028, directed = TRUE)\n\n# Calculate eigenvector centrality\neigenvector_centrality <- eigen_centrality(MC2_graph_weights_2028)$vector\n\n# Define thresholds or criteria for category assignment based on eigenvector centrality values\neigenvector_threshold <- 0.3\n\n# Set size to eigenvector centrality\nV(MC2_graph_weights_2028)$size <- eigenvector_centrality\n\n# Assign categories based on eigenvector centrality measures\nMC2_nodes_extracted_weight_2028$category <- ifelse(eigenvector_centrality > eigenvector_threshold, \"Central Carrier\", \"Fishing Vessel\")\n\n# Get top nodes based on eigenvector centrality\ntop_nodes_2028 <- MC2_nodes_extracted_weight_2028 %>%\n  top_n(50, eigenvector_centrality)\n\n# Visualize the filtered graph using ggraph\nggraph(MC2_graph_weights_2028, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(0.02, \"npc\"))) +\n  geom_node_point(aes(color = MC2_nodes_extracted_weight_2028$category, size = size)) +\n  scale_color_manual(values = c(\"Central Carrier\" = \"red\", \"Fishing Vessel\" = \"grey\")) +\n  scale_size_continuous(range = c(1, 4)) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#dnu too squeezy\nset_graph_style()\nMC2_graph_weight <- tbl_graph(nodes = MC2_nodes_extracted_weight, edges = MC2_edges_weight, directed = TRUE)\ng <- ggraph(MC2_graph_weight, \n            layout = \"fr\") + \n  geom_edge_link() +\n  geom_node_point()\n\ng + facet_edges(~year) \n\n\n\n#make into animated graph or stay with one graph per view and flip it (keep toggling from one to another)\n#use panel tabset function in quarto, so you can have multiple tabs across years\n#examine those with larger group components, or further filter\n\n\nset_graph_style()\n\ng <- ggraph(MC2_graph_weight, \n            layout = \"fr\") + \n  geom_edge_link() +\n  geom_node_point()\n\ng + facet_edges(~hscode)\n\n\n\n\n\nset_graph_style()\n\ng <- ggraph(MC2_graph_weight, \n            layout = \"fr\") + \n  geom_edge_link() +\n  geom_node_point()\n  \ng + facet_nodes(~id)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#network-metrics-analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#network-metrics-analysis",
    "title": "Take-home Exercise 2",
    "section": "5.2 Network Metrics Analysis",
    "text": "5.2 Network Metrics Analysis\n\n5.2.1 Centrality Indices Computation\n\n#MC2_graph_weight <- tbl_graph(nodes = MC2_nodes_extracted_weight, edges = MC2_edges_weight, directed = TRUE)\n# Compute the degree centrality for our graph G. \ndegr_cent <- centr_degree(MC2_graph_weight, mode = 'all')\ndegr_cent <- degr_cent$res\n\n# Compute the eigenvector centrality of our network\neign_cent <- eigen_centrality(MC2_graph_weight)\neign_cent <- eign_cent$vector\n\n# Compute betweeness centrality\nbetw_cent <- igraph::betweenness(MC2_graph_weight)\n\n\ndata <- data.frame(vertex = MC2_nodes_extracted_weight,\n                   label = MC2_nodes_extracted_weight$id,\n                   degree = degr_cent, \n                   eigen = eign_cent, \n                   betweeness = betw_cent)\n\n# Order the data by degree centrality\ndata <- data %>% arrange(desc(degree))\n\n# Rename the columns in the data frame\ndata <- data %>%\n  rename(\n    id = vertex.id,\n    shpcountry = vertex.shpcountry,\n    rcvcountry = vertex.rcvcountry,\n    label_2 = label\n  )\n\ndata <- data[, -which(names(data) == \"label_2\")]\n\n# Calculate the threshold for the top 30% of nodes based on degree\nthreshold <- quantile(data$degree, probs = 0.9)\n\n# Filter the nodes based on the threshold\ntop_nodes <- data[data$degree >= threshold, ]\n\n# Store the top nodes in the data data frame\ndata <- top_nodes\n\n# Print the updated data frame\ndata\n\n                                                       id     shpcountry\n1                               Caracola del Sol Services     Puerto Sol\n2                                       Mar del Este CJSC       Merigrad\n3                       Sea Breezes S.A. de C.V. Freight       Isliandor\n4                               hǎi dǎn Corporation Wharf        Marebak\n5                                         Pao gan SE Seal     Alverossia\n6                          Costa de la Felicidad Shipping     Alverossia\n7                             AquaDelight N.V. Coral Reef    Coralmarica\n8                                   Blue Horizon Family &    Osterivaria\n9                           Madagascar Coast  AG Freight         Oceanus\n10                      Selous Game Reserve  S.A. de C.V.      Isliandor\n11                               nián yú Ltd. Corporation       Merigrad\n12                               Estrella de la Costa SRL        Mawazam\n13                    Caracola del Este Ltd. Liability Co    Brindivaria\n14                               Balkan Cat ОАО Transport     Vesperanda\n15                                   SeaBass &  SE Tanker   Arreciviento\n16                                  Mar de la Vida S.p.A.     Vesperanda\n17                              Spanish Shrimp A/S Marine        Marebak\n18                              Daniel Ferry N.V. Transit    Osterivaria\n19                                     Lake Tana  & Son's    Osterivaria\n20                           Neptune's Harvest A/S Hijiki        Mawazam\n21                                 Shou gan  Oyj Overseas        Mawazam\n22                           The Sea Turtle GmbH & Co. KG        Marebak\n23                         Kong zhong diao yu  Ges.m.b.H.        Oceanus\n24                     Diao er  Limited Liability Company        Marebak\n25                            Playa de Arena OJSC Express     Vesperanda\n26                                 Mar del Placer Manatee Puerto del Mar\n27                                    Pao gan LC Freight          -22004\n28                            Volga River LLC Enterprises       Quornova\n29                               Chuan gou  N.V. Delivery       Merigrad\n30                                  Dutch Eel AB Holdings     Vesperanda\n31                                      Olas del Mar N.V.        Mawazam\n32                      Manipur  Market Corporation Cargo     Vesperanda\n33                         Ocean Quest Inc Marine ecology        Mawazam\n34                        Wave Watchers Ltd. Liability Co       Merigrad\n35 Uttarakhand  Market Limited Liability Company Nautical        Helixia\n36                              Costa de Coral SRL United        Mawazam\n37                             Madhya Pradesh  Market LLC       Merigrad\n38                Sea Breeze Corporation Marine sanctuary       Coralada\n39                          Michael Ltd. Corporation Seal        Mawazam\n40                           Oceanfront Oases Co Shipping     Vesperanda\n41                      OceanicOrigin Foods Co Consulting   Arreciviento\n42                                 Perla del Mar Deep-sea    Osterivaria\n43                           Tripura  Market S.A. de C.V.     Vesperanda\n44                    Estrella del Mar Tilapia Oyj Marine    Coralmarica\n45                           lǐyú GmbH & Co. KG Logistics           <NA>\n    rcvcountry vertex.modularitycategory vertex.edge_betweenness_category\n1      Oceanus                         3                                5\n2      Oceanus                         4                                6\n3  Coralmarica                         3                                3\n4      Oceanus                         3                                3\n5      Oceanus                         2                                2\n6      Oceanus                         2                                4\n7      Oceanus                         2                                4\n8         <NA>                         2                                4\n9      Oceanus                         2                                4\n10     Oceanus                         1                                1\n11     Oceanus                         2                                2\n12     Oceanus                         4                               19\n13     Oceanus                         1                                7\n14     Oceanus                         1                                7\n15     Oceanus                         4                                6\n16        <NA>                         1                                1\n17        <NA>                         2                                4\n18     Oceanus                         2                                4\n19     Oceanus                         2                                4\n20     Oceanus                         4                               20\n21     Oceanus                         4                                6\n22     Oceanus                         5                               15\n23     Oceanus                         5                               15\n24     Oceanus                         2                               18\n25     Oceanus                         3                                5\n26     Oceanus                         1                                1\n27     Oceanus                         2                                4\n28     Oceanus                         4                               19\n29     Oceanus                         1                                7\n30     Oceanus                         5                               15\n31     Oceanus                         4                                6\n32     Oceanus                         1                                6\n33        <NA>                         4                                6\n34    Merigrad                         4                                6\n35     Oceanus                         4                               19\n36    Rio Isla                         4                                6\n37     Oceanus                         3                                3\n38     Oceanus                         1                                6\n39        <NA>                         2                                4\n40        <NA>                         1                                7\n41   Khamseena                         7                               17\n42     Oceanus                         1                                7\n43     Oceanus                         5                               15\n44     Oceanus                         4                                6\n45     Oceanus                         4                                6\n   vertex.category degree      eigen betweeness\n1               20    111 0.79846490          0\n2               22    109 1.00000000          0\n3               20     65 0.74093203          0\n4               20     61 0.37195958          0\n5               19     56 0.38843299          0\n6               19     53 0.50426436          0\n7               23     48 0.22400393          0\n8               23     37 0.57545678          0\n9               23     37 0.30021461          0\n10               1     34 0.23123207          0\n11              19     33 0.27996553          0\n12              66     29 0.16209153          0\n13              22     29 0.19935494          0\n14              23     28 0.21302006          0\n15              25     25 0.08682652          0\n16              20     23 0.35465120          0\n17              20     23 0.19529845          0\n18              23     22 0.26900871          0\n19              23     22 0.15186532          0\n20               1     22 0.12496250          0\n21               1     22 0.29062264          0\n22              23     21 0.04818342          0\n23              19     21 0.12305269          0\n24              30     20 0.07190415          0\n25              20     20 0.36436460          0\n26              67     20 0.10572920          0\n27              30     19 0.06537834          0\n28              66     19 0.05646727          0\n29              22     18 0.32176751          0\n30              23     18 0.07504389          0\n31              25     18 0.18904099          0\n32              67     16 0.17444565          0\n33               1     16 0.16127632          0\n34              22     16 0.32697833          0\n35              66     16 0.05353294          0\n36              25     15 0.28486416          0\n37              20     15 0.14572525          0\n38              67     15 0.11761432          0\n39              19     14 0.19084825          0\n40              22     14 0.09601903          0\n41               1     14 0.00362491          0\n42              23     14 0.06226664          0\n43              20     14 0.27999220          0\n44              25     14 0.06410447          0\n45              25     14 0.08171178          0\n\n\n\n# Get the unique names from the \"id\" column of the data data frame\nvalid_names <- unique(data$id)\n\n# Filter the edges data frame to include only the valid names in the \"source\" column\nMC2_edges_weight <- MC2_edges_weight[MC2_edges_weight$source %in% valid_names, ]\n\n# Filter the edges data frame to include only the valid names in the \"target\" column\nMC2_edges_weight <- MC2_edges_weight[MC2_edges_weight$target %in% valid_names, ]\n\n# Print the updated edges data frame\nprint(MC2_edges_weight)\n\n# A tibble: 358 × 6\n   source                   target       hscode  year weights noofunique_targets\n   <chr>                    <chr>        <chr>  <dbl>   <int>              <int>\n 1 Balkan Cat ОАО Transport \"AquaDeligh… 306170  2028     126                 53\n 2 Balkan Cat ОАО Transport \"AquaDeligh… 306170  2029      54                 53\n 3 Balkan Cat ОАО Transport \"Caracola d… 306170  2031      71                 53\n 4 Balkan Cat ОАО Transport \"Caracola d… 306170  2033      60                 53\n 5 Balkan Cat ОАО Transport \"Caracola d… 306170  2034      82                 53\n 6 Balkan Cat ОАО Transport \"Costa de l… 306170  2034      76                 53\n 7 Balkan Cat ОАО Transport \"Madagascar… 306170  2033      48                 53\n 8 Balkan Cat ОАО Transport \"Mar del Es… 306170  2033      66                 53\n 9 Balkan Cat ОАО Transport \"Selous Gam… 306170  2029      70                 53\n10 Balkan Cat ОАО Transport \"Selous Gam… 306170  2031      70                 53\n# ℹ 348 more rows\n\n\n\nMC2_graph_weight <- tbl_graph(nodes = MC2_nodes_extracted_weight, edges = MC2_edges_weight, directed = TRUE)\n# Find communites using the edge betweeness algorithm\nbtw_groups <- cluster_edge_betweenness(MC2_graph_weight)\nbtw_groups <- btw_groups$membership\n\n\n# Plot the actual modularity groupings\nplot(MC2_graph_weight, vertex.color = groups, # Changes node color\n     edge.color = 'black',     # Changes edge color\n     vertex.size = 20,         # Changes vertex size\n     vertex.shape = 'circle',  # Changes vertex shape\n     asp = 0,\n     layout = layout_in_circle, \n     main = 'Actual')\n\n\n\n\n\nMC2_graph <- tbl_graph(nodes = MC2_nodes_extracted_weight, edges = MC2_edges_weight, directed = TRUE)\ng <- MC2_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=weights), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = shpcountry, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n5.2.2 Community Visualisation"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-network-graph",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-network-graph",
    "title": "Take-home Exercise 2",
    "section": "7.1 Interactive Network Graph",
    "text": "7.1 Interactive Network Graph\n::: panel-tabset ### The Code Chunk\nThe below code chunk shows a sample of how we can plot an interactive network graph leveraging on the computed in-degree centrality over the years. We first begin by converting the graph into a tibble the interactive network graph requires us to work with numerical identifiers.\n\nMC2_graph_weights_2028_indegree <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028_indegree, edges = MC2_edges_weight_2028_indegree, directed = TRUE)\nnodes_df_weights_2028_indegree <- MC2_graph_weights_2028_indegree %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2028_indegree <- MC2_graph_weights_2028_indegree %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2028_indegree <- MC2_nodes_extracted_weight_2028_indegree %>%\n  rename(label = id)\n\nnodes_df_weights_2028_indegree <- inner_join(nodes_df_weights_2028_indegree, MC2_nodes_extracted_weight_2028_indegree, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2028_indegree <- nodes_df_weights_2028_indegree %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2028_indegree <- sort(nodes_df_weights_2028_indegree$id) # for the ID nodes dropdown box\n\nvis_plot_2028_indegree <- visNetwork(nodes = nodes_df_weights_2028_indegree, edges = edges_df_weights_2028_indegree) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", # or use igraph's `layout_*`s in quotes\n                  smooth = FALSE,            # set to F when bogged by bigger graphs\n                  physics = TRUE           # set to F when bogged by bigger graphs\n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2028_indegree)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), # usually will take some tweaking\n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2028_indegree\n\n\n[2028] In-Degree Centrality Interactive Network Graph\n\n\n\n\n\n\n\n\n[2029] In-Degree Centrality Interactive Network Graph\n\n\n\n\n\n\n\n\n[2030] In-Degree Centrality Interactive Network Graph\n\n\n\n\n\n\n\n\n\n[2031] In-Degree Centrality Interactive Network Graph\n\nMC2_graph_weights_2031_indegree <- tbl_graph(nodes = MC2_nodes_extracted_weight_2031_indegree, edges = MC2_edges_weight_2031_indegree, directed = TRUE)\nnodes_df_weights_2031_indegree <- MC2_graph_weights_2031_indegree %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2031_indegree <- MC2_graph_weights_2031_indegree %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2031_indegree <- MC2_nodes_extracted_weight_2031_indegree %>%\n  rename(label = id)\n\nnodes_df_weights_2031_indegree <- inner_join(nodes_df_weights_2031_indegree, MC2_nodes_extracted_weight_2031_indegree, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2031_indegree <- nodes_df_weights_2031_indegree %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2031_indegree <- sort(nodes_df_weights_2031_indegree$id) # for the ID nodes dropdown box\n\nvis_plot_2031_indegree <- visNetwork(nodes = nodes_df_weights_2031_indegree, edges = edges_df_weights_2031_indegree) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", # or use igraph's `layout_*`s in quotes\n                  smooth = FALSE,            # set to F when bogged by bigger graphs\n                  physics = TRUE           # set to F when bogged by bigger graphs\n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2031_indegree)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), # usually will take some tweaking\n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2031_indegree\n\n\n\n\n\n\n\n\n[2032] In-Degree Centrality Interactive Network Graph\n\n\n\n\n\n\n\n\n\n[2033] In-Degree Centrality Interactive Network Graph\n\n\n\n\n\n\n\n\n\n[2034] In-Degree Centrality Interactive Network Graph"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#joining-with-data-bundles",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#joining-with-data-bundles",
    "title": "Take-home Exercise 2",
    "section": "6. Joining with data bundles",
    "text": "6. Joining with data bundles\n\n6.1 Define a list of bundle names and their corresponding file paths\n\n#Defining bundle_files file path\nbundle_files <- list(\n  carp = \"data/bundles/carp.json\",\n  catfish = \"data/bundles/catfish.json\",\n  chub_mackerel = \"data/bundles/chub_mackerel.json\",\n  cod2 = \"data/bundles/cod2.json\",\n  herring = \"data/bundles/herring.json\",\n  lichen = \"data/bundles/lichen.json\",\n  mackerel = \"data/bundles/mackerel.json\",\n  pollock = \"data/bundles/pollock.json\",\n  salmon = \"data/bundles/salmon.json\",\n  salmon_wgl = \"data/bundles/salmon_wgl.json\",\n  shark = \"data/bundles/shark.json\",\n  tuna = \"data/bundles/tuna.json\"\n)\n\n#Creating empty list to store the filtered edges and nodes\nfiltered_edges_list <- list()\nfiltered_nodes_list <- list()\n\n#Define selected columns\nedge_columns <- c(\"source\", \"target\", \"arrivaldate\", \"year\", \"hscode\")\nadditional_columns <- c(\"valueofgoods_omu\", \"volumeteu\", \"weightkg\", \"valueofgoodsusd\")\n\n#Loop over each bundle files\nfor (bundle_name in names(bundle_files)) {\n  # Read the JSON file\n  bundle <- fromJSON(bundle_files[[bundle_name]])\n  \n  #Filter the edges\n  edges <- as_tibble(bundle$links)\n  if (\"arrivaldate\" %in% names(edges)) {\n    edges <- edges %>%\n      mutate(arrivaldate = ymd(arrivaldate)) %>%\n      mutate(year = year(arrivaldate))\n  }\n  \n  #Select relevant columns only if they exist\n  if (all(c(edge_columns, additional_columns) %in% names(edges))) {\n    edges <- edges %>%\n      select(all_of(c(edge_columns, additional_columns))) %>%\n      distinct() %>%\n      filter(str_detect(hscode, \"^(301|302|303|304|305|306|307|308|1604|1605)\"))\n  }\n  \n  #Filter the nodes\n  nodes <- as_tibble(bundle$nodes) %>%\n    select(id, shpcountry, rcvcountry)\n  \n  #Store filtered edges and nodes in the respective lists\n  filtered_edges_list[[bundle_name]] <- edges\n  filtered_nodes_list[[bundle_name]] <- nodes\n}\n\ncarp_edges <- filtered_edges_list$carp\ncarp_nodes <- filtered_nodes_list$carp\ncatfish_edges <- filtered_edges_list$catfish\ncatfish_nodes <- filtered_nodes_list$catfish\nchub_mackerel_edges <- filtered_edges_list$chub_mackerel\nchub_mackerel_nodes <- filtered_nodes_list$chub_mackerel\ncod2_edges <- filtered_edges_list$cod2\ncod2_nodes <- filtered_nodes_list$cod2\nherring_edges <- filtered_edges_list$herring\nherring_nodes <- filtered_nodes_list$herring\nlichen_edges <- filtered_edges_list$lichen\nlichen_nodes <- filtered_nodes_list$lichen\nmackerel_edges <- filtered_edges_list$mackerel\nmackerel_nodes <- filtered_nodes_list$mackerel\npollock_edges <- filtered_edges_list$pollock\npollock_nodes <- filtered_nodes_list$pollock\nsalmon_edges <- filtered_edges_list$salmon\nsalmon_nodes <- filtered_nodes_list$salmon\nsalmon_wgl_edges <- filtered_edges_list$salmon_wgl\nsalmon_wgl_nodes <- filtered_nodes_list$salmon_wgl\nshark_edges <- filtered_edges_list$shark\nshark_nodes <- filtered_nodes_list$shark\ntuna_edges <- filtered_edges_list$tuna\ntuna_nodes <- filtered_nodes_list$tuna\n\n\n\n6.2 Map visual of sub bundles\n\n6.2.1 Tuna\n\nid1_tuna <- tuna_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_tuna <- tuna_edges %>%\n  select(target) %>%\n  rename(id = target)\ntuna_nodes <- rbind(id1_tuna, id2_tuna) %>%\n  distinct()\n\ntuna_edges_cleaned <- tuna_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\ntuna_graph_weight <- tbl_graph(nodes = tuna_nodes, edges = tuna_edges_cleaned, directed = TRUE)\nggraph(tuna_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.2 Shark\n\nid1_shark <- shark_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_shark <- shark_edges %>%\n  select(target) %>%\n  rename(id = target)\nshark_nodes <- rbind(id1_shark, id2_shark) %>%\n  distinct()\n\nshark_edges_cleaned <- shark_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\nshark_graph_weight <- tbl_graph(nodes = shark_nodes, edges = shark_edges_cleaned, directed = TRUE)\nggraph(shark_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.3 Cod2\n\nid1_cod2 <- cod2_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_cod2 <- cod2_edges %>%\n  select(target) %>%\n  rename(id = target)\ncod2_nodes <- rbind(id1_cod2, id2_cod2) %>%\n  distinct()\n\ncod2_edges_cleaned <- cod2_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\ncod2_graph_weight <- tbl_graph(nodes = cod2_nodes, edges = cod2_edges_cleaned, directed = TRUE)\nggraph(cod2_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.4 Carp\n\nid1_carp <- carp_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_carp <- carp_edges %>%\n  select(target) %>%\n  rename(id = target)\ncarp_nodes <- rbind(id1_carp, id2_carp) %>%\n  distinct()\n\ncarp_edges_cleaned <- carp_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\ncarp_graph_weight <- tbl_graph(nodes = carp_nodes, edges = carp_edges_cleaned, directed = TRUE)\nggraph(carp_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.5 Mackerel\n\nid1_mackerel <- mackerel_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_mackerel <- mackerel_edges %>%\n  select(target) %>%\n  rename(id = target)\nmackerel_nodes <- rbind(id1_mackerel, id2_mackerel) %>%\n  distinct()\n\nmackerel_edges_cleaned <- mackerel_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\nmackerel_graph_weight <- tbl_graph(nodes = mackerel_nodes, edges = mackerel_edges_cleaned, directed = TRUE)\nggraph(mackerel_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.7 Catfish\n\nid1_catfish <- catfish_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_catfish <- catfish_edges %>%\n  select(target) %>%\n  rename(id = target)\ncatfish_nodes <- rbind(id1_catfish, id2_catfish) %>%\n  distinct()\n\ncatfish_edges_cleaned <- catfish_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\ncatfish_graph_weight <- tbl_graph(nodes = catfish_nodes, edges = catfish_edges_cleaned, directed = TRUE)\nggraph(catfish_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.8 Chub Mackerel\n\nid1_chub_mackerel <- chub_mackerel_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_chub_mackerel <- chub_mackerel_edges %>%\n  select(target) %>%\n  rename(id = target)\nchub_mackerel_nodes <- rbind(id1_chub_mackerel, id2_chub_mackerel) %>%\n  distinct()\n\nchub_mackerel_edges_cleaned <- chub_mackerel_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\nchub_mackerel_graph_weight <- tbl_graph(nodes = chub_mackerel_nodes, edges = chub_mackerel_edges_cleaned, directed = TRUE)\nggraph(chub_mackerel_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.9 Herring\n\nid1_herring <- herring_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_herring <- herring_edges %>%\n  select(target) %>%\n  rename(id = target)\nherring_nodes <- rbind(id1_herring, id2_herring) %>%\n  distinct()\n\nherring_edges_cleaned <- herring_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\nherring_graph_weight <- tbl_graph(nodes = herring_nodes, edges = herring_edges_cleaned, directed = TRUE)\nggraph(catfish_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.10 Lichen\n\nid1_lichen <- lichen_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_lichen <- lichen_edges %>%\n  select(target) %>%\n  rename(id = target)\nlichen_nodes <- rbind(id1_lichen, id2_lichen) %>%\n  distinct()\n\nlichen_edges_cleaned <- lichen_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\nlichen_graph_weight <- tbl_graph(nodes = lichen_nodes, edges = lichen_edges_cleaned, directed = TRUE)\nggraph(lichen_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.11 Pollock\n\nid1_pollock <- pollock_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_pollock <- pollock_edges %>%\n  select(target) %>%\n  rename(id = target)\npollock_nodes <- rbind(id1_pollock, id2_pollock) %>%\n  distinct()\n\npollock_edges_cleaned <- pollock_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\npollock_graph_weight <- tbl_graph(nodes = pollock_nodes, edges = pollock_edges_cleaned, directed = TRUE)\nggraph(pollock_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n\n6.2.12 Salmon_wgl\n\nid1_salmon_wgl <- salmon_wgl_edges %>%\n  select(source) %>%\n  rename(id = source)\nid2_salmon_wgl <- salmon_wgl_edges %>%\n  select(target) %>%\n  rename(id = target)\nsalmon_wgl_nodes <- rbind(id1_salmon_wgl, id2_salmon_wgl) %>%\n  distinct()\n\nsalmon_wgl_edges_cleaned <- salmon_wgl_edges %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(\n    Weight = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(source != target)\n\nset.seed(123)\nsalmon_wgl_graph_weight <- tbl_graph(nodes = salmon_wgl_nodes, edges = salmon_wgl_edges_cleaned, directed = TRUE)\nggraph(salmon_wgl_graph_weight, layout = \"fr\") +\n  geom_edge_link() + #(aes(colour = factor(year)))\n  geom_node_point()\n\n\n\n\n\n##notes\n#classification to fish type might have errors\n#but we can gleam roughly what is salmon\n\n#big graph has many years\n#tidygraph graph_join or bingraph\n\n#purpose is to zoom into specific relationship for e.g. shark tuna carp\n#pick from bigger graph the fisherman that are involved in shark harvesting"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#joining-with-main-dataset",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#joining-with-main-dataset",
    "title": "Take-home Exercise 2",
    "section": "6.3 Joining with main dataset",
    "text": "6.3 Joining with main dataset\n\ncarp_edges$hscode <- as.character(carp_edges$hscode)\n\nMC2_edges_master <- left_join(carp_edges, MC2_edges, by = c(\"source\", \"target\", \"arrivaldate\", \"hscode\"))\nMC2_edges_master\n\n# A tibble: 177 × 18\n   arrivaldate hscode valueofgoods_omu.x volumeteu.x weightkg.x generated_by\n   <date>      <chr>               <dbl>       <dbl>      <int> <chr>       \n 1 2034-03-20  80440               15915           0      15720 carp        \n 2 2034-08-01  940179                 NA           0      10795 carp        \n 3 2034-11-27  940161                 NA           0       6555 carp        \n 4 2034-10-10  940161                 NA           0       6675 carp        \n 5 2034-04-09  40729                  NA          15      54775 carp        \n 6 2034-03-30  700711                 NA           0      22430 carp        \n 7 2034-03-12  701090                 NA           5      32890 carp        \n 8 2034-04-02  160419              63125           0      20775 carp        \n 9 2034-01-26  841950                 NA           0        720 carp        \n10 2034-12-06  200600              57020           0      17855 carp        \n# ℹ 167 more rows\n# ℹ 12 more variables: dataset <chr>, source <chr>, target <chr>, year.x <dbl>,\n#   year.y <dbl>, hscode_des <chr>, valueofgoods_omu.y <dbl>,\n#   volumeteu.y <dbl>, weightkg.y <int>, valueofgoodsusd <dbl>,\n#   hscode_parent <chr>, hscode_parent_des <chr>"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#creating-ternary-plot-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#creating-ternary-plot-with-r",
    "title": "Hands-on Exercise 6",
    "section": "6. Creating Ternary Plot with R",
    "text": "6. Creating Ternary Plot with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#overview",
    "title": "Hands-on Exercise 6a",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nTernary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, we will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps: - Install and launch tidyverse and ggtern packages. - Derive three new measures using mutate() function of dplyr package. - Build a static ternary plot using ggtern() function of ggtern package. - Build an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 6a",
    "section": "6.2 Installing and launching R packages",
    "text": "6.2 Installing and launching R packages\nFor this exercise, two main R packages will be used in this hands-on exercise, they are: - ggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots. - Plotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load('plotly', 'tidyverse')\n\n\n\n\n\n\n\nNote\n\n\n\nDue to some technical issue, ggtern is currently not available for downloading via cran. We need to download ggtern from the archive by using the code chunk below. The latest archive version is 3.4.1.\n\n\n\n#require(devtools)\n#install_version(\"ggtern\", version = \"3.4.1\", repos = \"http://cran.us.r-project.org\")\n\nNext, load ggtern package into R environment by using the code chunk below.\n\nlibrary(ggtern)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#data-preparation",
    "title": "Hands-on Exercise 6a",
    "section": "6.3 Data Preparation",
    "text": "6.3 Data Preparation\n\n6.3.1 The data\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n6.3.2 Importing Data\nTo import respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data <- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n6.3.3 Preparing the Data\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated <- pop_data %>%\n  mutate(`Year` = as.character(Year))%>%\n  spread(AG, Population) %>%\n  mutate(YOUNG = rowSums(.[4:8]))%>%\n  mutate(ACTIVE = rowSums(.[9:16]))  %>%\n  mutate(OLD = rowSums(.[17:21])) %>%\n  mutate(TOTAL = rowSums(.[22:24])) %>%\n  filter(Year == 2018)%>%\n  filter(TOTAL > 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06a.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 6a",
    "section": "6.4 Plotting Ternary Diagram with R",
    "text": "6.4 Plotting Ternary Diagram with R\n\n6.4.1 Plotting a static ternary diagram\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n6.4.2 Plotting an interactive ternary diagram\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel <- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis <- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes <- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %>%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#visual-correlation-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#visual-correlation-analysis",
    "title": "Hands-on Exercise 6",
    "section": "6. Visual Correlation Analysis",
    "text": "6. Visual Correlation Analysis"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#overview",
    "title": "Hands-on Exercise 6b",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nCorrelation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix. - To reveal the relationship between high-dimensional variables pair-wisely. - To input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise. - As a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, we will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, we will learn how to create correlation matrix using pairs() of R Graphics. Next, we will learn how to plot corrgram using corrplot package of R. Lastly, we will learn how to create an interactive correlation matrix using plotly R."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 6b",
    "section": "6.2 Installing and Launching R Packages",
    "text": "6.2 Installing and Launching R Packages\nBefore we get started, we are required: - to start a new R project, and - to create a new R Markdown document.\nNext, we will use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 6b",
    "section": "6.3 Importing and Preparing The Data Set",
    "text": "6.3 Importing and Preparing The Data Set\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n6.3.1 Importing Data\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine <- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type.\n\n\n6.4 Building Correlation Matrix: pairs() method\nThere are more than one way to build scatterplot matrix with R. In this section, we will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\n\n\n6.4.1 Building a basic correlation matrix\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n6.4.2 Drawing the lower corner\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n6.4.3 Including with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\n\npanel.cor <- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr <- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr <- abs(cor(x, y, use=\"complete.obs\"))\ntxt <- format(c(r, 0.123456789), digits=digits)[1]\ntxt <- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 6b",
    "section": "6.5 Visualising Correlation Matrix: ggcormat()",
    "text": "6.5 Visualising Correlation Matrix: ggcormat()\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, we will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n6.5.1 The basic plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p < 0.05\"\n)\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#building-multiple-plots",
    "title": "Hands-on Exercise 6b",
    "section": "6.6 Building multiple plots",
    "text": "6.6 Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06b.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 6b",
    "section": "6.7 Visualising Correlation Matrix using corrplot Package",
    "text": "6.7 Visualising Correlation Matrix using corrplot Package\nIn this hands-on exercise, we will focus on corrplot.\n\n6.7.1 Getting started with corrplot\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor <- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n6.7.2 Working with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n6.7.3 Working with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n6.7.4 Working with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n6.7.5 Combining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n6.7.6 Reorder a corrgram\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n6.7.7 Reordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#heatmap-for-visualising-and-analysing-multivariate-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#heatmap-for-visualising-and-analysing-multivariate-data",
    "title": "Hands-on Exercise 6",
    "section": "6. Heatmap for VIsualising and Analysing Multivariate Data",
    "text": "6. Heatmap for VIsualising and Analysing Multivariate Data"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#overview",
    "title": "Hands-on Exercise 6c",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nHeatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, we will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 6c",
    "section": "6.2 Installing and launching R Packages",
    "text": "6.2 Installing and launching R Packages\nBefore we get started, we are required:\n\nto start a new R project, and\nto create a new R Markdown document.\n\nNext, we will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse, corrplot, ggstatsplot)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 6c",
    "section": "6.3 Importing and Preparing The Data Set",
    "text": "6.3 Importing and Preparing The Data Set\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n6.3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh <- read_csv(\"data/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\n6.3.2 Preparing the data\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) <- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n6.3.3 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make our heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 <- dplyr::select(wh, c(3, 7:12))\nwh_matrix <- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#static-heatmap",
    "title": "Hands-on Exercise 6c",
    "section": "6.4 Static Heatmap",
    "text": "6.4 Static Heatmap\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis).\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types.\n\nIn this section, we will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n6.4.1 heatmap() of R Stats\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap <- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms. To plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap <- heatmap(wh_matrix)\n\n\n\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap <- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06c.html#creating-interactive-heatmap",
    "title": "Hands-on Exercise 6c",
    "section": "6.5 Creating Interactive Heatmap",
    "text": "6.5 Creating Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nIn this section, we will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n6.5.1 Working with heatmaply\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n6.5.2 Data transformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n6.5.2.1 Scaling method\n\nWhen all variables came from or are assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values column wise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n6.5.2.2 Normalising method\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n6.5.2.3 Percentising method\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n6.5.3 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n6.5.4 Manual approach\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n6.5.5 Statistical approach\nIn order to determine the best clustering method and number of cluster, the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d <- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust <- hclust(wh_d, method = \"average\")\nnum_k <- find_k(wh_clust)\nplot(num_k)\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n6.5.6 Seriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n6.5.7 Working with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n6.5.8 The finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#visual-multivariate-analysis-with-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#visual-multivariate-analysis-with-parallel-coordinates-plot",
    "title": "Hands-on Exercise 6",
    "section": "6. Visual Multivariate Analysis with Parallel Coordinates Plot",
    "text": "6. Visual Multivariate Analysis with Parallel Coordinates Plot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#overview",
    "title": "Hands-on Exercise 6d",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nParallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, we will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 6d",
    "section": "6.2 Installing and Launching R Packages",
    "text": "6.2 Installing and Launching R Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse, corrplot, ggstatsplot)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#data-preparation",
    "title": "Hands-on Exercise 6d",
    "section": "6.3 Data preparation",
    "text": "6.3 Data preparation\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh <- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 6d",
    "section": "6.4 Plotting Static Parallel Coordinates Plot",
    "text": "6.4 Plotting Static Parallel Coordinates Plot\nIn this section, we will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package.\n\n6.4.1 Plotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n6.4.2 Plotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, we will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n6.4.3 Parallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combine usage of some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n6.4.4 Rotating x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n6.4.5 Adjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 6d",
    "section": "6.5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "6.5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, we will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n6.5.1 The basic plot\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh <- wh %>%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. We will learn how to overcome this problem in the next step.\n\n\n6.5.2 Rotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n6.5.3 Changing the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunk below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n6.5.4 Parallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility <- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#treemap-visualisation-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#treemap-visualisation-with-r",
    "title": "Hands-on Exercise 6",
    "section": "6. Treemap Visualisation with R",
    "text": "6. Treemap Visualisation with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#overview",
    "title": "Hands-on Exercise 6e",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nIn this hands-on exercise, we will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, we will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, we will learn how to plot static treemap by using treemap package. In the third section, we will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 6e",
    "section": "6.2 Installing and Launching R Packages",
    "text": "6.2 Installing and Launching R Packages\nBefore we get started, you are required to check if treemap and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#data-wrangling",
    "title": "Hands-on Exercise 6e",
    "section": "6.3 Data Wrangling",
    "text": "6.3 Data Wrangling\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n6.3.1 Importing the dataset\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 <- read_csv(\"data/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n6.3.2 Data Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively. Two key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\n\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n6.3.3 Grouped summaries without the Pipe\nThe code chunk below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped <- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised <- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n6.3.4 Grouped summarise with the pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %>%:\n\nrealis2018_summarised <- realis2018 %>% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %>%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 6e",
    "section": "6.4 Designing Treemap with treemap Package",
    "text": "6.4 Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n6.4.1 Designing a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected <- realis2018_summarised %>%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n6.4.2 Using the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThings to learn from the three arguments used:\n\nindex: The index vector must consist of at least two column names or else no hierarchy treemap will be plotted.If multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\nvSize: The column must not contain negative values. This is because it’s values will be used to map the sizes of the rectangles of the treemaps.\n\nWarning: The treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n6.4.3 Working with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThinking to learn from the code chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n6.4.4 Colours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n6.4.5 The “value” type treemap\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n6.4.6 The “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n6.4.7 Treemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n6.4.8 Working with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n6.4.9 Using sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 6e",
    "section": "6.5 Designing Treemap using treemapify Package",
    "text": "6.5 Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify.\n\n6.5.1 Designing a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n6.5.2 Defining hierarchy\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 6e",
    "section": "6.6 Designing Interactive Treemap using d3treeR",
    "text": "6.6 Designing Interactive Treemap using d3treeR"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#installing-d3treer-package",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06/Hands-on_Ex06e.html#installing-d3treer-package",
    "title": "Hands-on Exercise 6e",
    "section": "6.6.1 Installing d3treeR package",
    "text": "6.6.1 Installing d3treeR package\nIf this is the first time installing a package from github, we should install devtools package by using the code below or else we can skip this step.\n\n#install.packages(\"devtools\")\n\nNext, we will load the devtools library and install the package found in github by using the codes below.\n\n#library(devtools)\n#install_github(\"timelyportfolio/d3treeR\")\n\nNow you are ready to launch d3treeR package\n\nlibrary(d3treeR)\n\n\n6.6.2 Designing an Interactive Treemap\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm <- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#flag-pairing",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#flag-pairing",
    "title": "Take-home Exercise 2",
    "section": "5.4 Flag Pairing",
    "text": "5.4 Flag Pairing\n\nlibrary(igraph)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create the graph with filtered edges\nMC2_graph_undirected <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028, edges = MC2_edges_weight_2028, directed = FALSE)\n\n# Calculate edge weights or counts\nedge_weights <- as.matrix(MC2_graph_undirected, sparse = FALSE, attr = \"weights\")\n\n# Define threshold for significant connections\nthreshold <- 150\n\n# Initialize an empty list to store flag pairings\nflag_pairings <- list()\n\n# Iterate over the nodes and their connections\nfor (i in 1:vcount(MC2_graph_undirected)) {\n  neighbors <- neighbors(MC2_graph_undirected, i)\n  neighbor_weights <- edge_weights[i, neighbors]\n  significant_neighbors <- neighbors[neighbor_weights >= threshold]\n  \n  # Store the flag pairings in the list\n  if (length(significant_neighbors) > 0) {\n    flag_pairings <- c(flag_pairings, list(c(i, significant_neighbors)))\n  }\n}\n\n# Print the flag pairings\nfor (pairing in flag_pairings) {\n  node <- pairing[[1]]\n  neighbors <- pairing[[2]]\n  print(paste(\"Flag Pairing:\", node, \"-\", neighbors))\n}\n\n[1] \"Flag Pairing: 6 - 287\"\n[1] \"Flag Pairing: 15 - 298\"\n[1] \"Flag Pairing: 21 - 198\"\n[1] \"Flag Pairing: 24 - 301\"\n[1] \"Flag Pairing: 27 - 279\"\n[1] \"Flag Pairing: 47 - 51\"\n[1] \"Flag Pairing: 48 - 279\"\n[1] \"Flag Pairing: 51 - 47\"\n[1] \"Flag Pairing: 64 - 278\"\n[1] \"Flag Pairing: 71 - 276\"\n[1] \"Flag Pairing: 77 - 315\"\n[1] \"Flag Pairing: 90 - 282\"\n[1] \"Flag Pairing: 91 - 279\"\n[1] \"Flag Pairing: 92 - 315\"\n[1] \"Flag Pairing: 117 - 279\"\n[1] \"Flag Pairing: 139 - 286\"\n[1] \"Flag Pairing: 147 - 391\"\n[1] \"Flag Pairing: 148 - 395\"\n[1] \"Flag Pairing: 157 - 346\"\n[1] \"Flag Pairing: 169 - 329\"\n[1] \"Flag Pairing: 195 - 422\"\n[1] \"Flag Pairing: 198 - 21\"\n[1] \"Flag Pairing: 210 - 279\"\n[1] \"Flag Pairing: 217 - 279\"\n[1] \"Flag Pairing: 218 - 429\"\n[1] \"Flag Pairing: 240 - 283\"\n[1] \"Flag Pairing: 244 - 395\"\n[1] \"Flag Pairing: 249 - 283\"\n[1] \"Flag Pairing: 250 - 302\"\n[1] \"Flag Pairing: 257 - 409\"\n[1] \"Flag Pairing: 262 - 281\"\n[1] \"Flag Pairing: 263 - 444\"\n[1] \"Flag Pairing: 276 - 71\"\n[1] \"Flag Pairing: 278 - 64\"\n[1] \"Flag Pairing: 279 - 27\"\n[1] \"Flag Pairing: 281 - 262\"\n[1] \"Flag Pairing: 282 - 90\"\n[1] \"Flag Pairing: 283 - 240\"\n[1] \"Flag Pairing: 285 - 91\"\n[1] \"Flag Pairing: 286 - 90\"\n[1] \"Flag Pairing: 287 - 6\"\n[1] \"Flag Pairing: 294 - 21\"\n[1] \"Flag Pairing: 298 - 15\"\n[1] \"Flag Pairing: 301 - 24\"\n[1] \"Flag Pairing: 302 - 250\"\n[1] \"Flag Pairing: 315 - 77\"\n[1] \"Flag Pairing: 324 - 91\"\n[1] \"Flag Pairing: 326 - 48\"\n[1] \"Flag Pairing: 329 - 169\"\n[1] \"Flag Pairing: 346 - 157\"\n[1] \"Flag Pairing: 372 - 169\"\n[1] \"Flag Pairing: 391 - 147\"\n[1] \"Flag Pairing: 395 - 148\"\n[1] \"Flag Pairing: 409 - 257\"\n[1] \"Flag Pairing: 422 - 195\"\n[1] \"Flag Pairing: 429 - 218\"\n[1] \"Flag Pairing: 442 - 262\"\n[1] \"Flag Pairing: 443 - 262\"\n[1] \"Flag Pairing: 444 - 263\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#communitycluster-detection",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#communitycluster-detection",
    "title": "Take-home Exercise 2",
    "section": "5. Community/Cluster Detection",
    "text": "5. Community/Cluster Detection\nIn the section below, we plot a basic static graph to gain an overall view of the node edge connection to better understand and detect communities/clusters. “fr” (Fruchterman-Reingold) layout was utilised as it is a special force directed layout where nodes are drawn closer to nodes they are connected to. A total of three methods were explored, namely: infomap clustering, leading eigen clustering and group components.\nThe below shows the code for plotting a infomap cluster which is suitable for both directed and undirected graph.\nWe can see that there are several small sub-clusters at the rim of the plot, with a few larger clusters dominating the center.\n\n5.1 Infomap ClusteringCode Chunk\n\n\n\n\n\n\n\n\n\n\n# Create the directed graph\nMC2_graph_directed_informap <- tbl_graph(nodes = MC2_nodes_extracted_weight, \n                                         edges = MC2_edges_weight, \n                                         directed = TRUE)\n\n# Convert the graph to an igraph object\nigraph_object <- as.igraph(MC2_graph_directed_informap)\n\n# Apply the Infomap algorithm for community detection\ninfomap_result <- infomap.community(igraph_object)\n\n# Get the community membership of nodes\nmembership <- membership(infomap_result)\n\n# Visualize the graph with community colors\nggraph(MC2_graph_directed_informap, layout = \"fr\") +\n  geom_edge_link(color = \"grey\", alpha = 0.7, arrow = arrow(length = unit(0.015, \"npc\"))) +\n  geom_node_point(aes(color = as.factor(membership))) +\n  scale_color_discrete(name = \"Community\") +\n  theme(legend.position = \"right\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"Community Detection using Infomap\")\n\n\n\n\nThe below shows the code for plotting a group components plot which primarily groups by connected components. Likewise, there are several small sub clusters at the rim of the plot. Despite further refining the group components to only show cluster 1 to 4, the results were still not optimal for visualisation. Hence, we proceed to try leading eigen clustering.\n\n5.2 Group ComponentsCode Chunk\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\nMC2_graph_groupcomponents <- tbl_graph(nodes = MC2_nodes_extracted_weight, \n                                       edges = MC2_edges_weight, \n                                       directed = TRUE)\n\nMC2_graph_groupcomponents <- MC2_graph_groupcomponents %>%\n  activate(nodes) %>%\n  mutate(group_component = group_components())\n\n# Visualize the filtered graph using ggraph\nggraph(MC2_graph_groupcomponents, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(0.02, \"npc\"))) +\n  geom_node_point(aes(color = as.factor(group_component))) +\n  scale_color_discrete(name = \"Group Component\") +\n  theme(legend.position = \"right\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"Clustering based on Group Components\")\n\n\n\n\nFinally, the below shows the plot for leading eigen cluster which groups nodes based on the leading eigenvector of the modularity matrix. There appaers to be a more visible clusters as seen from the plot below. Within the centre of the plot, there are at least three bigger clusters. Please refer to Section 7. where we briefly go into details on the top 3 cluster percentages.\n\n5.3 Leading Eigen ClusteringCode Chunk\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\n# Create the graph with filtered edges\nMC2_graph_undirected_eigen <- tbl_graph(nodes = MC2_nodes_extracted_weight, \n                                        edges = MC2_edges_weight, \n                                        directed = TRUE)\n\n# Convert the tbl_graph to an igraph object\nMC2_igraph_undirected_eigen <- as.igraph(MC2_graph_undirected_eigen)\n\n# Extract & weaves the edge weights from the MC2_edges_weight dataset\nedge_weights <- MC2_edges_weight$weights\nE(MC2_igraph_undirected_eigen)$weight <- edge_weights\n\n# Perform leading eigenvector community detection\neigen_groups <- cluster_leading_eigen(MC2_igraph_undirected_eigen, \n                                      weights = E(MC2_igraph_undirected_eigen)$weight)\n\n# Assign categories based on the groups\neigen_cluster <- as.character(eigen_groups$membership)\n\n# Visualize the graph using ggraph\nggraph(MC2_graph_undirected_eigen, layout = \"fr\") +\n  geom_edge_link(color = \"grey\", alpha = 0.7, arrow = arrow(length = unit(0.015, \"npc\"))) +\n  geom_node_point(aes(color = eigen_cluster)) +\n  scale_color_manual(values = rainbow(length(unique(eigen_groups$membership)))) +\n  theme(legend.position = \"right\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"Clustering based on Eigenvector Centrality\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#in-degree-centrality-by-years",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#in-degree-centrality-by-years",
    "title": "Take-home Exercise 2",
    "section": "6.1 In-Degree Centrality by Years",
    "text": "6.1 In-Degree Centrality by Years\nWithin this section, we will leverage on degree centrality and ggraph to plot a static network graph. Indegree centrality is defined in the below code via mode = “in”. Note that the dataset is furthered focused into a smaller subset in the below section as we are filtering for weights which are >= quantile 0.8.\nWithin the code outlined in Code Chunk below, we have also defined a node to be a “Central Carrier” if it’s above a threshold currently defined at 2. If otherwise, then the node will be defined as “Fishing Vessel”. This value was arrived after a few trial and errors before deciding that this was an optimal value. This can potentially help us to detect transshipments which are frequent perpetrators that promotes IUU fishing occuring out at sea. A high indegree centrality for transhipment (otherwise labeled as a Central Carrier) can be viewed as a critical hub for receiving goods from various nodes.\nThere appears to be several “Central Carriers” spread out across several fishing vessels across the various years per the plots below.\n\n2028202920302031203220332034Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe below shows a sample code chunk of how we can plot for the year 2028. Users need to take note to configure it to the respective years they are looking to analyse.\n\nset.seed(123)\n\nMC2_edges_weight_2028_indegree <- MC2_edges %>%\n  filter(year == 2028) %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.80)) #or filter for unique target\n\nid1_weight_2028_indegree <- MC2_edges_weight_2028_indegree %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight_2028_indegree <- MC2_edges_weight_2028_indegree %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight_2028_indegree <- rbind(id1_weight_2028_indegree, id2_weight_2028_indegree) %>% \n  distinct()\n\nMC2_nodes_extracted_weight_2028_indegree <- left_join(MC2_nodes_extracted_weight_2028_indegree, MC2_nodes_cleaned, by = \"id\")\n\n# Create the graph\nMC2_graph_weights_2028_indegree <- tbl_graph(nodes=MC2_nodes_extracted_weight_2028_indegree,\n                                             edges = MC2_edges_weight_2028_indegree, \n                                             directed = TRUE)\n\n# Calculate degree centrality & define threshold to assign nodes to either Central Carrier or Fishing Vessel\nindegree_centrality_2028 <- degree(MC2_graph_weights_2028_indegree, mode = \"in\")\nindegree_threshold_2028 <- 2\n\n# Set size to degree centrality\nV(MC2_graph_weights_2028_indegree)$size <- indegree_centrality_2028\n\n# Assign categories based on centrality measures\nindegree_categories_2028 <- ifelse(indegree_centrality_2028 > indegree_threshold_2028, \"Central Carrier\", \"Fishing Vessel\")\n\n# Store category information in MC2_nodes_extracted_weight_2028\nMC2_nodes_extracted_weight_2028_indegree$category <- indegree_categories_2028\n\n# Get top nodes based on degree centrality\ntop_nodes_2028 <- MC2_nodes_extracted_weight_2028_indegree %>%\n  top_n(50, indegree_centrality_2028)\n\n# Visualize the filtered graph using ggraph\nggraph(MC2_graph_weights_2028_indegree, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(0.02, \"npc\"))) +\n  geom_node_point(aes(color = indegree_categories_2028, size = size)) +\n  scale_color_manual(values = c(\"Central Carrier\" = \"red\", \"Fishing Vessel\" = \"grey\")) +\n  scale_size_continuous(range = c(1, 2)) +\n  theme(legend.position = \"right\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"2028 Graph based on In-Degree Centrality\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#out-degree-centrality-by-years",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#out-degree-centrality-by-years",
    "title": "Take-home Exercise 2",
    "section": "6.2 Out-Degree Centrality by Years",
    "text": "6.2 Out-Degree Centrality by Years\nWithin this section, we will leverage on degree centrality and ggraph to plot a static network graph. Out degree centrality is defined in the below code via mode = “out”. Note that the dataset is furthered focused into a smaller subset in the below section as we are filtering for weights which are >= quantile 0.8.\nWithin the code outlined in Code Chunk below, we have also defined a node to be a “Central Carrier” if it’s above a threshold currently defined at 2. If otherwise, then the node will be defined as “Fishing Vessel”. This value was arrived after a few trial and errors before deciding that this was an optimal value. This helps us to detect transshipments which are frequent perpetrators of IUU fishing occuring out at sea.\n\n2028202920302031203220332034Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe below shows a sample code chunk of how we can plot for the year 2028. Users need to take note to configure it to the respective years they are looking to analyse.\n\nset.seed(123)\n\nMC2_edges_weight_2028_outdegree <- MC2_edges %>%\n  filter(year == 2028) %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.80)) #or filter for unique target\n\nid1_weight_2028_outdegree <- MC2_edges_weight_2028_outdegree %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight_2028_outdegree <- MC2_edges_weight_2028_outdegree %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight_2028_outdegree <- rbind(id1_weight_2028_outdegree, id2_weight_2028_outdegree) %>% \n  distinct()\n\nMC2_nodes_extracted_weight_2028_outdegree <- left_join(MC2_nodes_extracted_weight_2028_outdegree, MC2_nodes_cleaned, by = \"id\")\n\n# Create the graph\nMC2_graph_weights_2028_outdegree <- tbl_graph(nodes=MC2_nodes_extracted_weight_2028_outdegree,\n                                             edges = MC2_edges_weight_2028_outdegree, \n                                             directed = TRUE)\n\n# Calculate degree centrality & define threshold to assign nodes to either Central Carrier or Fishing Vessel\noutdegree_centrality_2028 <- degree(MC2_graph_weights_2028_outdegree, mode = \"out\")\noutdegree_threshold_2028 <- 2\n\n# Set size to degree centrality\nV(MC2_graph_weights_2028_outdegree)$size <- outdegree_centrality_2028\n\n# Assign categories based on centrality measures\noutdegree_categories_2028 <- ifelse(outdegree_centrality_2028 > outdegree_threshold_2028, \"Central Carrier\", \"Fishing Vessel\")\n\n# Store category information in MC2_nodes_extracted_weight_2028\nMC2_nodes_extracted_weight_2028_outdegree$category <- outdegree_categories_2028\n\n# Visualize the filtered graph using ggraph\nggraph(MC2_graph_weights_2028_outdegree, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(0.02, \"npc\"))) +\n  geom_node_point(aes(color = outdegree_categories_2028, size = size)) +\n  scale_color_manual(values = c(\"Central Carrier\" = \"red\", \"Fishing Vessel\" = \"grey\")) +\n  scale_size_continuous(range = c(1, 2)) +\n  theme(legend.position = \"right\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"2028 Graph based on Out-Degree Centrality\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mc2_edges_weight-by-years-eigenvector-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mc2_edges_weight-by-years-eigenvector-centrality",
    "title": "Take-home Exercise 2",
    "section": "6.3 MC2_edges_weight by years (Eigenvector Centrality)",
    "text": "6.3 MC2_edges_weight by years (Eigenvector Centrality)\n\n[2028][2029][2030][2031][2032][2033][2034]The Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe below shows a sample code chunk of how we can plot for the year 2028. Users need to take note to configure it to the respective years they are looking to analyse.\n\nset.seed(123)\n\nMC2_edges_weight_2028_eigenvector <- MC2_edges %>%\n  filter(year == 2028) %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.8)) #or filter for unique target\n\nid1_weight_2028_eigenvector <- MC2_edges_weight_2028_eigenvector %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight_2028_eigenvector <- MC2_edges_weight_2028_eigenvector %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight_2028_eigenvector <- rbind(id1_weight_2028_eigenvector, id2_weight_2028_eigenvector) %>%\n  distinct()\n\nMC2_nodes_extracted_weight_2028_eigenvector <- left_join(MC2_nodes_extracted_weight_2028_eigenvector, MC2_nodes_cleaned, by = \"id\")\n\n# Create the graph with filtered edges\nMC2_graph_weights_2028_eigenvector <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028_eigenvector, \n                                    edges = MC2_edges_weight_2028_eigenvector, \n                                    directed = TRUE)\n\n# Calculate eigenvector centrality & define threshold to assign nodes to either Central Carrier or Fishing Vessel\neigenvector_centrality_2028 <- eigen_centrality(MC2_graph_weights_2028_eigenvector)$vector\neigenvector_threshold_2028 <- 0.3\n\n# Set size to eigenvector centrality\nV(MC2_graph_weights_2028_eigenvector)$size <- eigenvector_centrality_2028\n\n# Assign categories based on eigenvector centrality measures\neigen_categories_2028 <- ifelse(eigenvector_centrality_2028 > eigenvector_threshold_2028, \"Central Carrier\", \"Fishing Vessel\")\n\n# Store category information in MC2_nodes_extracted_weight_2034\nMC2_nodes_extracted_weight_2028_eigenvector$category <- eigen_categories_2028\n\n# Get top nodes based on eigenvector centrality\ntop_nodes_2028_eigenvector <- MC2_nodes_extracted_weight_2028_eigenvector %>%\n  top_n(50, eigenvector_centrality_2028)\n\n# Visualize the filtered graph using ggraph\nggraph(MC2_graph_weights_2028_eigenvector, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(0.02, \"npc\"))) +\n  geom_node_point(aes(color = eigen_categories_2028, size = size)) +\n  scale_color_manual(values = c(\"Central Carrier\" = \"red\", \"Fishing Vessel\" = \"grey\")) +\n  scale_size_continuous(range = c(1, 2)) +\n  theme(legend.position = \"right\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"2028 Graph based on Eigenvector Centrality\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-network-graph---in-degree-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-network-graph---in-degree-centrality",
    "title": "Take-home Exercise 2",
    "section": "7.2 Interactive Network Graph - In-Degree Centrality",
    "text": "7.2 Interactive Network Graph - In-Degree Centrality\n\nThe Code Chunk[2028] In-Degree Centrality Interactive Network Graph[2029] In-Degree Centrality Interactive Network Graph[2030] In-Degree Centrality Interactive Network Graph[2031] In-Degree Centrality Interactive Network Graph[2032] In-Degree Centrality Interactive Network Graph[2033] In-Degree Centrality Interactive Network Graph[2034] In-Degree Centrality Interactive Network Graph\n\n\nThe below code chunk shows a sample of how we can plot an interactive network graph leveraging on the computed in-degree centrality over the years. We first begin by converting the graph into a tibble the interactive network graph requires us to work with numerical identifiers.\n\nMC2_graph_weights_2028_indegree <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028_indegree, edges = MC2_edges_weight_2028_indegree, directed = TRUE)\nnodes_df_weights_2028_indegree <- MC2_graph_weights_2028_indegree %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2028_indegree <- MC2_graph_weights_2028_indegree %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2028_indegree <- MC2_nodes_extracted_weight_2028_indegree %>%\n  rename(label = id)\n\nnodes_df_weights_2028_indegree <- inner_join(nodes_df_weights_2028_indegree, MC2_nodes_extracted_weight_2028_indegree, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2028_indegree <- nodes_df_weights_2028_indegree %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2028_indegree <- sort(nodes_df_weights_2028_indegree$id) # for the ID nodes dropdown box\n\nvis_plot_2028_indegree <- visNetwork(nodes = nodes_df_weights_2028_indegree, edges = edges_df_weights_2028_indegree) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,         \n                  physics = TRUE         \n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2028_indegree)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), \n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2028_indegree\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC2_graph_weights_2031_indegree <- tbl_graph(nodes = MC2_nodes_extracted_weight_2031_indegree, edges = MC2_edges_weight_2031_indegree, directed = TRUE)\nnodes_df_weights_2031_indegree <- MC2_graph_weights_2031_indegree %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2031_indegree <- MC2_graph_weights_2031_indegree %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2031_indegree <- MC2_nodes_extracted_weight_2031_indegree %>%\n  rename(label = id)\n\nnodes_df_weights_2031_indegree <- inner_join(nodes_df_weights_2031_indegree, MC2_nodes_extracted_weight_2031_indegree, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2031_indegree <- nodes_df_weights_2031_indegree %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2031_indegree <- sort(nodes_df_weights_2031_indegree$id) # for the ID nodes dropdown box\n\nvis_plot_2031_indegree <- visNetwork(nodes = nodes_df_weights_2031_indegree, edges = edges_df_weights_2031_indegree) %>%\n  visIgraphLayout(layout = \"layout_with_fr\",\n                  smooth = FALSE,\n                  physics = TRUE\n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2031_indegree)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50),\n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2031_indegree"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-network-graph---eigenvector-centrality",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interactive-network-graph---eigenvector-centrality",
    "title": "Take-home Exercise 2",
    "section": "7.3 Interactive Network Graph - Eigenvector Centrality",
    "text": "7.3 Interactive Network Graph - Eigenvector Centrality\n\nThe Code Chunk[2028] Eigenvector Centrality Interactive Network Graph[2029] Eigenvector Centrality Interactive Network Graph[2030] Eigenvector Centrality Interactive Network Graph[2031] Eigenvector Centrality Interactive Network Graph[2032] Eigenvector Centrality Interactive Network Graph[2033] Eigenvector Centrality Interactive Network Graph[2034] Eigenvector Centrality Interactive Network Graph\n\n\nThe below code chunk shows a sample of how we can plot an interactive network graph leveraging on the computed eigenvector centrality over the years. We first begin by converting the graph into a tibble the interactive network graph requires us to work with numerical identifiers.\n\nMC2_graph_weights_2028_eigenvector <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028_eigenvector, edges = MC2_edges_weight_2028_eigenvector, directed = TRUE)\nnodes_df_weights_2028_eigenvector <- MC2_graph_weights_2028_eigenvector %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2028_eigenvector <- MC2_graph_weights_2028_eigenvector %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2028_eigenvector <- MC2_nodes_extracted_weight_2028_eigenvector %>%\n  rename(label = id)\n\nnodes_df_weights_2028_eigenvector <- inner_join(nodes_df_weights_2028_eigenvector, MC2_nodes_extracted_weight_2028_eigenvector, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2028_eigenvector <- nodes_df_weights_2028_eigenvector %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2028_eigenvector <- sort(nodes_df_weights_2028_eigenvector$id) # for the ID nodes dropdown box\n\nvis_plot_2028_eigenvector <- visNetwork(nodes = nodes_df_weights_2028_eigenvector, edges = edges_df_weights_2028_eigenvector) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,         \n                  physics = TRUE         \n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2028_eigenvector)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), \n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2028_eigenvector\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC2_graph_weights_2031_eigenvector <- tbl_graph(nodes = MC2_nodes_extracted_weight_2031_eigenvector, edges = MC2_edges_weight_2031_eigenvector, directed = TRUE)\nnodes_df_weights_2031_eigenvector <- MC2_graph_weights_2031_eigenvector %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2031_eigenvector <- MC2_graph_weights_2031_eigenvector %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2031_eigenvector <- MC2_nodes_extracted_weight_2031_eigenvector %>%\n  rename(label = id)\n\nnodes_df_weights_2031_eigenvector <- inner_join(nodes_df_weights_2031_eigenvector, MC2_nodes_extracted_weight_2031_eigenvector, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2031_eigenvector <- nodes_df_weights_2031_eigenvector %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2031_eigenvector <- sort(nodes_df_weights_2031_eigenvector$id) # for the ID nodes dropdown box\n\nvis_plot_2031_eigenvector <- visNetwork(nodes = nodes_df_weights_2031_eigenvector, edges = edges_df_weights_2031_eigenvector) %>%\n  visIgraphLayout(layout = \"layout_with_fr\",\n                  smooth = FALSE,\n                  physics = TRUE\n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2031_eigenvector)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50),\n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2031_eigenvector"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#eigenvector-centrality-by-years",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#eigenvector-centrality-by-years",
    "title": "Take-home Exercise 2",
    "section": "6.2 Eigenvector Centrality by Years",
    "text": "6.2 Eigenvector Centrality by Years\nWithin this section, we will leverage on eigenvector centrality and ggraph to plot a static network graph. Eigenvector Centrality helps to detect notes that has a more transitive influence. A node who has a higher eigenvector score implies that it is connected to many nodes who also have high scores. Note that the dataset is furthered focused into a smaller subset in the below section as we are filtering for weights which are >= quantile 0.8.\nWithin the code outlined in Code Chunk below, we have also defined a node to be a “Central Carrier” if it’s above a threshold currently defined at 0.3. If otherwise, then the node will be defined as “Fishing Vessel”. This value was arrived after a few trial and errors before deciding that this was an optimal value.\nThere appears to be a growing number of “Central Carriers” over the years, where 2028 started off with 5 key carriers identified via Eigenvector Centrality, to 2024 ending off with more than 10 identified key carriers.\n\n2028202920302031203220332034Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe below shows a sample code chunk of how we can plot for the year 2028. Users need to take note to configure it to the respective years they are looking to analyse.\n\nset.seed(123)\n\nMC2_edges_weight_2028_eigenvector <- MC2_edges %>%\n  filter(year == 2028) %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.8)) #or filter for unique target\n\nid1_weight_2028_eigenvector <- MC2_edges_weight_2028_eigenvector %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight_2028_eigenvector <- MC2_edges_weight_2028_eigenvector %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight_2028_eigenvector <- rbind(id1_weight_2028_eigenvector, id2_weight_2028_eigenvector) %>%\n  distinct()\n\nMC2_nodes_extracted_weight_2028_eigenvector <- left_join(MC2_nodes_extracted_weight_2028_eigenvector, MC2_nodes_cleaned, by = \"id\")\n\n# Create the graph with filtered edges\nMC2_graph_weights_2028_eigenvector <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028_eigenvector, \n                                    edges = MC2_edges_weight_2028_eigenvector, \n                                    directed = TRUE)\n\n# Calculate eigenvector centrality & define threshold to assign nodes to either Central Carrier or Fishing Vessel\neigenvector_centrality_2028 <- eigen_centrality(MC2_graph_weights_2028_eigenvector)$vector\neigenvector_threshold_2028 <- 0.3\n\n# Set size to eigenvector centrality\nV(MC2_graph_weights_2028_eigenvector)$size <- eigenvector_centrality_2028\n\n# Assign categories based on eigenvector centrality measures\neigen_categories_2028 <- ifelse(eigenvector_centrality_2028 > eigenvector_threshold_2028, \"Central Carrier\", \"Fishing Vessel\")\n\n# Store category information in MC2_nodes_extracted_weight_2034\nMC2_nodes_extracted_weight_2028_eigenvector$category <- eigen_categories_2028\n\n# Get top nodes based on eigenvector centrality\ntop_nodes_2028_eigenvector <- MC2_nodes_extracted_weight_2028_eigenvector %>%\n  top_n(50, eigenvector_centrality_2028)\n\n# Visualize the filtered graph using ggraph\nggraph(MC2_graph_weights_2028_eigenvector, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(0.02, \"npc\"))) +\n  geom_node_point(aes(color = eigen_categories_2028, size = size)) +\n  scale_color_manual(values = c(\"Central Carrier\" = \"red\", \"Fishing Vessel\" = \"grey\")) +\n  scale_size_continuous(range = c(1, 2)) +\n  theme(legend.position = \"right\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"2028 Graph based on Eigenvector Centrality\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mini-case-2-challenge-writeup-4-images",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mini-case-2-challenge-writeup-4-images",
    "title": "Take-home Exercise 2",
    "section": "Mini-Case 2 Challenge Writeup & 4 Images",
    "text": "Mini-Case 2 Challenge Writeup & 4 Images\nGiven that transshipments are a huge potential for IUU fishing trades to occur as fishing vessels can obscure the origins of their catches that might have been caught illegally, the focus of this visualisation targets the transshipments between central carriers and fishing vessels. The visualisation outlined in Section 7 serves to put forth the basis to enable the identification of companies that fit patterns of illegal fishing.\nFor the purpose of mini-case 2 challenge writeup question 4, a summary is provided per below bullet points. Further supporting images and dynamic visualisation can be found in section 7.1 - 7.4.\n\nWe begin with an overall picture on the key clusters as outlined in Section 7.1. From the results, it appears that the top 3 clusters (in descending order, Cluster 20, 1, 23) consists of 14.8%, 14.3% and 9.2% of the framed dataset. This seems to suggest a presence of three bigger communities/clusters.\nLeveraging on Section 7.2 Interactive Visual, we can use this tool to identify companies who potentially fit a pattern of illegal fishing. “Mar del Placer CJSC Transport” is one such company who potentially fit a pattern of illegal fishing.\nThis was detected via “Turkish Mussels LLC and” which was identified as a Central Carrier with in-degree associations from three main nodes: 1) Arena del Sol Pic, 2) Mar del Placer CJSC Transport and 3) Saltwater Sirens Limited Liability Company Marine conservation.\nWhen we did a deep dive on “Mar del Placer CJSC Transport” via Section 7.2 Interactive Visual, it appears to be last seen in 2030. Zooming further into the profile of the said company, it is providing fishing products to three nodes, all of which are “Central Carriers” per image below.\n\nArena del Sol Pic has a high outdegree centrality as it goes out to several nodes during 2029 and it is still in the business even in 2034, reaching out to nodes such as “Mar del Este CJSC” which has a high indegree centrality. Coincidentally, “Mar del Este CJSC” appears to have a similar name to “Mar del Placer CJSC Transport”. Per image below, “Mar del Este CJSC” conversely has a high in-degree centrality unlike “Mar del Placer CJSC Transport”.\n\nThis makes “Mar del Placer CJSC Transport” very suspicious as an entity that is involved in illegal fishing given that according to Fisheye, companies who are caught fishing illegally will shut down and start up under a different name again.\n\n\n7.1 Cluster Analysis (Leading Eigen Visual)\nLet us take a closer look at the clusters using the leading eigen clustering method, both in a tabular and visual manner.\n\n\nShow code\nset.seed(123)\n\n# Create the graph with filtered edges\nMC2_graph_undirected_eigen <- tbl_graph(nodes = MC2_nodes_extracted_weight, \n                                        edges = MC2_edges_weight, \n                                        directed = TRUE)\n\n# Convert the tbl_graph to an igraph object\nMC2_igraph_undirected_eigen <- as.igraph(MC2_graph_undirected_eigen)\n\n# Extract & weaves the edge weights from the MC2_edges_weight dataset\nedge_weights <- MC2_edges_weight$weights\nE(MC2_igraph_undirected_eigen)$weight <- edge_weights\n\n# Perform leading eigenvector community detection\neigen_groups <- cluster_leading_eigen(MC2_igraph_undirected_eigen, \n                                      weights = E(MC2_igraph_undirected_eigen)$weight)\n\n# Assign categories based on the groups\neigen_cluster <- as.character(eigen_groups$membership)\n\n# Visualize the graph using ggraph\nggraph(MC2_graph_undirected_eigen, layout = \"fr\") +\n  geom_edge_link(color = \"grey\", alpha = 0.7, arrow = arrow(length = unit(0.015, \"npc\"))) +\n  geom_node_point(aes(color = eigen_cluster)) +\n  scale_color_manual(values = rainbow(length(unique(eigen_groups$membership)))) +\n  theme(legend.position = \"right\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"Clustering based on Eigenvector Centrality\")\n\n\n\n\n\nIt appears that the top 3 clusters (in descending order, Cluster 20, 1, 23) consists of 14.8%, 14.3% and 9.2% of the framed dataset. This seems to suggest a presence of three bigger communities/clusters.\n\n\nShow code\n# Calculate the percentage for each cluster\ncluster_counts <- table(eigen_cluster)\npercentage_clusters <- cluster_counts %>%\n  prop.table() * 100\n\n# Top 3 clusters with the highest percentages\ntop_3_clusters <- percentage_clusters %>%\n  sort(decreasing = TRUE) %>%\n  head(3) %>%\n  enframe(name = \"Cluster\", value = \"Percentage\") %>%\n  mutate(Cluster = as.character(Cluster)) \n\ntop_3_clusters\n\n\n# A tibble: 3 × 2\n  Cluster Percentage \n  <chr>   <table[1d]>\n1 20      14.833760  \n2 1       14.322251  \n3 23       9.207161  \n\n\n\n\n7.2 Interactive Network Graph - In-Degree Centrality\nHere, we explore the In-Degree Centrality network graph in an interactive manner across the various years. The below code chunk shows a sample of how we can plot an interactive network graph leveraging on the computed in-degree centrality over the years. We first begin by converting the graph into a tibble the interactive network graph requires us to work with numerical identifiers before defining the dropdown option via VisOptions.\n\n2028202920302031203220332034Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC2_graph_weights_2028_indegree <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028_indegree, edges = MC2_edges_weight_2028_indegree, directed = TRUE)\nnodes_df_weights_2028_indegree <- MC2_graph_weights_2028_indegree %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2028_indegree <- MC2_graph_weights_2028_indegree %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2028_indegree <- MC2_nodes_extracted_weight_2028_indegree %>%\n  rename(label = id)\n\nnodes_df_weights_2028_indegree <- inner_join(nodes_df_weights_2028_indegree, MC2_nodes_extracted_weight_2028_indegree, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2028_indegree <- nodes_df_weights_2028_indegree %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2028_indegree <- sort(nodes_df_weights_2028_indegree$id) # for the ID nodes dropdown box\n\nvis_plot_2028_indegree <- visNetwork(nodes = nodes_df_weights_2028_indegree, edges = edges_df_weights_2028_indegree) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,         \n                  physics = TRUE         \n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2028_indegree)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), \n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2028_indegree\n\n\n\n\n\n\n7.3 Interactive Network Graph - Eigenvector Centrality\nHere, we explore the Eigenvector Centrality network graph in an interactive manner across the various years. The below code chunk shows a sample of how we can plot an interactive network graph leveraging on the computed eigenvector centrality over the years. We first begin by converting the graph into a tibble the interactive network graph requires us to work with numerical identifiers before defining the dropdown option via VisOptions.\n\n2028202920302031203220332034Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC2_graph_weights_2028_eigenvector <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028_eigenvector, edges = MC2_edges_weight_2028_eigenvector, directed = TRUE)\nnodes_df_weights_2028_eigenvector <- MC2_graph_weights_2028_eigenvector %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2028_eigenvector <- MC2_graph_weights_2028_eigenvector %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2028_eigenvector <- MC2_nodes_extracted_weight_2028_eigenvector %>%\n  rename(label = id)\n\nnodes_df_weights_2028_eigenvector <- inner_join(nodes_df_weights_2028_eigenvector, MC2_nodes_extracted_weight_2028_eigenvector, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2028_eigenvector <- nodes_df_weights_2028_eigenvector %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2028_eigenvector <- sort(nodes_df_weights_2028_eigenvector$id) # for the ID nodes dropdown box\n\nvis_plot_2028_eigenvector <- visNetwork(nodes = nodes_df_weights_2028_eigenvector, edges = edges_df_weights_2028_eigenvector) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,         \n                  physics = TRUE         \n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2028_eigenvector)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), \n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2028_eigenvector\n\n\n\n\n\n\n7.4 Ego Network\nHere, we explore the Ego network graph for a specific node (Turkish Mussels LLC and) identified as potentially engaged in illegal fishing.\n\n\nShow code\nset.seed(123)\n\nMC2_edges_weight_ego <- MC2_edges %>%\n  filter(source == \"Mar del Placer CJSC Transport\" | target == \"Mar del Placer CJSC Transport\") %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.80)) #or filter for unique target\n  \nid1_weight_ego <- MC2_edges_weight_ego %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight_ego <- MC2_edges_weight_ego %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight_ego <- rbind(id1_weight_ego, id2_weight_ego) %>% \n  distinct()\n\nMC2_nodes_extracted_weight_ego <- left_join(MC2_nodes_extracted_weight_ego, MC2_nodes_cleaned, by = \"id\")\n\n# Create the graph\nMC2_graph_weights_ego <- tbl_graph(nodes=MC2_nodes_extracted_weight_ego,\n                                             edges = MC2_edges_weight_ego, \n                                             directed = TRUE)\n\n\nMC2_graph_weights_ego <- tbl_graph(nodes = MC2_nodes_extracted_weight_ego, edges = MC2_edges_weight_ego, directed = TRUE)\nnodes_df_weights_ego <- MC2_graph_weights_ego %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_ego <- MC2_graph_weights_ego %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_ego <- MC2_nodes_extracted_weight_ego %>%\n  rename(label = id)\n\nnodes_df_weights_ego <- inner_join(nodes_df_weights_ego,\n                                   MC2_nodes_extracted_weight_ego, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x)\n\nnames_ego <- sort(nodes_df_weights_ego$id) # for the ID nodes dropdown box\nvis_plot_ego <- visNetwork(nodes = nodes_df_weights_ego, edges = edges_df_weights_ego) %>%\n  visIgraphLayout(layout = \"layout_with_fr\",\n                  smooth = FALSE,\n                  physics = TRUE\n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_ego)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50),\n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_ego"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#reflections",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#reflections",
    "title": "Take-home Exercise 2",
    "section": "8. Reflections",
    "text": "8. Reflections\nThe most difficult aspect of working with this graph is in relation to the steep learning curve and computing resources. Personally, I do not have much exposure into coding with R, and hence this was a huge challenge for me as I faced repeated roadblocks when coding or data wrangling despite several research online. Additionally, my computer’s computing resources was rather limited, and I also had to spend a lot of wait time to enable it to run the codes without my computer hanging.\nThankfully, Professor Kam extended the submission due date and he was always readily available and willing to share his time to help get me up to speed. Professor Kam also helped me and the class to get up to speed by explaining data wrangling concepts very clearly, and I truly benefitted from it.\nI learnt a lot from this take-home exercise but I’m sure there is more to learn as knowledge graph always reveals new insights and patterns for individuals to perform analysis on."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#future-work",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#future-work",
    "title": "Take-home Exercise 2",
    "section": "9. Future Work",
    "text": "9. Future Work\nKeeping in mind the computing resources constraint, the analysis was done with a few key assumptions, some of which includes setting up filters with reasonable assumptions (for example filtering to anchor towards top 3 HSCodes, and filtering to sieve out weights within a certain percentile). As part of future work, we propose to leverage on Shiny app to offer a dynamic dropdown list to enable users who have different interests and focus to easily toggle to their desired HSCodes and years for analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#references",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#references",
    "title": "Take-home Exercise 2",
    "section": "10. References",
    "text": "10. References\n\nhttps://ona-book.org/vertex-importance.html\nhttps://www.data-imaginist.com/2017/ggraph-introduction-layouts/\nhttps://kateto.net/netscix2016.html\nhttps://www.data-imaginist.com/2017/ggraph-introduction-layouts/\nhttps://tidygraph.data-imaginist.com/reference/group_graph.html\nhttps://rdrr.io/cran/igraph/\nhttps://rpubs.com/neloe/ggraph_intro\nhttps://medium.com/analytics-vidhya/social-network-analysis-in-r-part-1-ego-network-ab6b0d23ebc8\nhttps://www.theguardian.com/environment/2022/oct/26/illegal-fishing-billions-losses-developing-countries"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mini-case-2-challenge-writeup",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#mini-case-2-challenge-writeup",
    "title": "Take-home Exercise 2",
    "section": "Mini-Case 2 Challenge Writeup",
    "text": "Mini-Case 2 Challenge Writeup\nGiven that transshipments are a huge potential for IUU fishing trades to occur as fishing vessels can obscure the origins of their catches that might have been caught illegally, the focus of this visualisation targets the transshipments between central carriers and fishing vessels. The visualisation outlined in Section 7 serves to put forth the basis to enable the identification of companies that fit patterns of illegal fishing.\nFor the purpose of mini-case 2 challenge writeup question 4, a summary is provided per below bullet points. Further supporting images and dynamic visualisation can be found in section 7.1 - 7.4.\n\nWe use Section 7.2 Interactive Visual to identify companies who potentially fit a pattern of illegal fishing. “Mar del Placer CJSC Transport” is one such company.\nThis was detected via “Turkish Mussels LLC and” which was identified as a Central Carrier with in-degree associations from three main nodes: 1) Arena del Sol Pic, 2) Mar del Placer CJSC Transport and 3) Saltwater Sirens Limited Liability Company Marine conservation.\nWhen we did a deep dive on “Mar del Placer CJSC Transport” via Section 7.2 Interactive Visual, it appears to be last seen in 2030. Zooming further into the profile of the said company, it is providing fishing products to three nodes, all of which are “Central Carriers” per image below.\n\nArena del Sol Pic continues to still be in the business even in 2034, reaching out to nodes such as “Mar del Este CJSC” with a high indegree centrality. Coincidentally, “Mar del Este CJSC” appears to have a similar name to “Mar del Placer CJSC Transport”. Per image below, “Mar del Este CJSC” conversely has a high in-degree centrality unlike “Mar del Placer CJSC Transport”. This makes “Mar del Placer CJSC Transport” very suspicious as an entity involved in illegal fishing given that according to Fisheye, companies caught fishing illegally will shut down and start up under a different name again.\n\nLooking at Section 7.3 (screencap below), it is concerning as we can see “Mar del Placer CJSC Transport” shipping its products to nodes with high eigenvector centrality (ie nodes with high transitive influence) in 2028. Additionally, we can see that it is also shipping to “Mar del Este CJSC” which implies that “Mar del Este CJSC” is getting its fishing products legally from several sources, but is also potentially getting its illegal source from “Mar del Placer CJSC Transport”.\n\n\n\n7.1 Cluster Analysis (Leading Eigen Visual)\nLet us take a closer look at the clusters using the leading eigen clustering method, both in a tabular and visual manner.\n\n\nShow code\nset.seed(123)\n\n# Create the graph with filtered edges\nMC2_graph_undirected_eigen <- tbl_graph(nodes = MC2_nodes_extracted_weight, \n                                        edges = MC2_edges_weight, \n                                        directed = TRUE)\n\n# Convert the tbl_graph to an igraph object\nMC2_igraph_undirected_eigen <- as.igraph(MC2_graph_undirected_eigen)\n\n# Extract & weaves the edge weights from the MC2_edges_weight dataset\nedge_weights <- MC2_edges_weight$weights\nE(MC2_igraph_undirected_eigen)$weight <- edge_weights\n\n# Perform leading eigenvector community detection\neigen_groups <- cluster_leading_eigen(MC2_igraph_undirected_eigen, \n                                      weights = E(MC2_igraph_undirected_eigen)$weight)\n\n# Assign categories based on the groups\neigen_cluster <- as.character(eigen_groups$membership)\n\n# Visualize the graph using ggraph\nggraph(MC2_graph_undirected_eigen, layout = \"fr\") +\n  geom_edge_link(color = \"grey\", alpha = 0.7, arrow = arrow(length = unit(0.015, \"npc\"))) +\n  geom_node_point(aes(color = eigen_cluster)) +\n  scale_color_manual(values = rainbow(length(unique(eigen_groups$membership)))) +\n  theme(legend.position = \"right\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"Clustering based on Eigenvector Centrality\")\n\n\n\n\n\nIt appears that the top 3 clusters (in descending order, Cluster 20, 1, 23) consists of 14.8%, 14.3% and 9.2% of the framed dataset. This seems to suggest a presence of three bigger communities/clusters.\n\n\nShow code\n# Calculate the percentage for each cluster\ncluster_counts <- table(eigen_cluster)\npercentage_clusters <- cluster_counts %>%\n  prop.table() * 100\n\n# Top 3 clusters with the highest percentages\ntop_3_clusters <- percentage_clusters %>%\n  sort(decreasing = TRUE) %>%\n  head(3) %>%\n  enframe(name = \"Cluster\", value = \"Percentage\") %>%\n  mutate(Cluster = as.character(Cluster)) \n\ntop_3_clusters\n\n\n# A tibble: 3 × 2\n  Cluster Percentage \n  <chr>   <table[1d]>\n1 20      14.833760  \n2 1       14.322251  \n3 23       9.207161  \n\n\n\n\n7.2 Interactive Network Graph - In-Degree Centrality\nHere, we explore the In-Degree Centrality network graph in an interactive manner across the various years. The below code chunk shows a sample of how we can plot an interactive network graph leveraging on the computed in-degree centrality over the years. We first begin by converting the graph into a tibble the interactive network graph requires us to work with numerical identifiers before defining the dropdown option via VisOptions.\n\n2028202920302031203220332034Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC2_graph_weights_2028_indegree <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028_indegree, edges = MC2_edges_weight_2028_indegree, directed = TRUE)\nnodes_df_weights_2028_indegree <- MC2_graph_weights_2028_indegree %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2028_indegree <- MC2_graph_weights_2028_indegree %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2028_indegree <- MC2_nodes_extracted_weight_2028_indegree %>%\n  rename(label = id)\n\nnodes_df_weights_2028_indegree <- inner_join(nodes_df_weights_2028_indegree, MC2_nodes_extracted_weight_2028_indegree, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2028_indegree <- nodes_df_weights_2028_indegree %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2028_indegree <- sort(nodes_df_weights_2028_indegree$id) # for the ID nodes dropdown box\n\nvis_plot_2028_indegree <- visNetwork(nodes = nodes_df_weights_2028_indegree, edges = edges_df_weights_2028_indegree) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,         \n                  physics = TRUE         \n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2028_indegree)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), \n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2028_indegree\n\n\n\n\n\n\n7.3 Interactive Network Graph - Eigenvector Centrality\nHere, we explore the Eigenvector Centrality network graph in an interactive manner across the various years. The below code chunk shows a sample of how we can plot an interactive network graph leveraging on the computed eigenvector centrality over the years. We first begin by converting the graph into a tibble the interactive network graph requires us to work with numerical identifiers before defining the dropdown option via VisOptions.\n\n2028202920302031203220332034Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC2_graph_weights_2028_eigenvector <- tbl_graph(nodes = MC2_nodes_extracted_weight_2028_eigenvector, edges = MC2_edges_weight_2028_eigenvector, directed = TRUE)\nnodes_df_weights_2028_eigenvector <- MC2_graph_weights_2028_eigenvector %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_2028_eigenvector <- MC2_graph_weights_2028_eigenvector %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_2028_eigenvector <- MC2_nodes_extracted_weight_2028_eigenvector %>%\n  rename(label = id)\n\nnodes_df_weights_2028_eigenvector <- inner_join(nodes_df_weights_2028_eigenvector, MC2_nodes_extracted_weight_2028_eigenvector, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x, category)\n\nnodes_df_weights_2028_eigenvector <- nodes_df_weights_2028_eigenvector %>%\n  mutate(color = ifelse(category == \"Central Carrier\", \"red\", \"grey\"))\n\nnames_2028_eigenvector <- sort(nodes_df_weights_2028_eigenvector$id) # for the ID nodes dropdown box\n\nvis_plot_2028_eigenvector <- visNetwork(nodes = nodes_df_weights_2028_eigenvector, edges = edges_df_weights_2028_eigenvector) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,         \n                  physics = TRUE         \n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"category\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_2028_eigenvector)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), \n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_2028_eigenvector\n\n\n\n\n\n\n7.4 Ego Network\nHere, we explore the Ego network graph for a specific node (Mar del Placer CJSC Transport) identified as potentially engaged in illegal fishing.\n\n\nShow code\nset.seed(123)\n\nMC2_edges_weight_ego <- MC2_edges %>%\n  filter(source == \"Mar del Placer CJSC Transport\" | target == \"Mar del Placer CJSC Transport\") %>%\n  filter(hscode == \"306170\" | hscode == \"304620\" | hscode == \"160414\") %>%\n  group_by(source, target, hscode, year) %>%\n  summarise(weights = n(), .groups = \"drop\") %>%\n  filter(source != target) %>%\n  ungroup() %>%\n  group_by(source) %>%\n  mutate(noofunique_targets = n_distinct(target)) %>%\n  ungroup() %>%\n  filter(weights >= quantile(weights, 0.80)) #or filter for unique target\n  \nid1_weight_ego <- MC2_edges_weight_ego %>%\n  select(source) %>%\n  rename(id = source)\nid2_weight_ego <- MC2_edges_weight_ego %>%\n  select(target) %>%\n  rename(id = target)\n\nMC2_nodes_extracted_weight_ego <- rbind(id1_weight_ego, id2_weight_ego) %>% \n  distinct()\n\nMC2_nodes_extracted_weight_ego <- left_join(MC2_nodes_extracted_weight_ego, MC2_nodes_cleaned, by = \"id\")\n\n# Create the graph\nMC2_graph_weights_ego <- tbl_graph(nodes=MC2_nodes_extracted_weight_ego,\n                                             edges = MC2_edges_weight_ego, \n                                             directed = TRUE)\n\n\nMC2_graph_weights_ego <- tbl_graph(nodes = MC2_nodes_extracted_weight_ego, edges = MC2_edges_weight_ego, directed = TRUE)\nnodes_df_weights_ego <- MC2_graph_weights_ego %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, shpcountry, rcvcountry)\n  \nedges_df_weights_ego <- MC2_graph_weights_ego %>%\n  activate(edges) %>%\n  as.tibble()\n\nMC2_nodes_extracted_weight_ego <- MC2_nodes_extracted_weight_ego %>%\n  rename(label = id)\n\nnodes_df_weights_ego <- inner_join(nodes_df_weights_ego,\n                                   MC2_nodes_extracted_weight_ego, by = \"label\") %>%\n  select(id, label, shpcountry.x, rcvcountry.x)\n\nnames_ego <- sort(nodes_df_weights_ego$id) # for the ID nodes dropdown box\nvis_plot_ego <- visNetwork(nodes = nodes_df_weights_ego, edges = edges_df_weights_ego) %>%\n  visIgraphLayout(layout = \"layout_with_fr\",\n                  smooth = FALSE,\n                  physics = TRUE\n                ) %>%\n  visNodes(size = 50) %>%\n  visEdges(color = list(highlight = \"lightgray\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = names_ego)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50),\n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 0.25)\n\nvis_plot_ego"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#learning-outcome",
    "title": "Hands-on Exercise 7a",
    "section": "7.1 Learning Outcome",
    "text": "7.1 Learning Outcome\nBy the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#getting-started",
    "title": "Hands-on Exercise 7a",
    "section": "7.2 Getting started",
    "text": "7.2 Getting started\nBefore getting start, we need to make sure that respective packages below are installed and launched.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 7a",
    "section": "7.3 Plotting Calendar Heatmap",
    "text": "7.3 Plotting Calendar Heatmap\nIn this section, we will learn how to plot a calender heatmap programmetically by using ggplot2 package.\nBy the end of this section, we will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n7.3.1 The Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n7.3.2 Importing the data\nFirst, we will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks <- read_csv(\"data/eventlog.csv\")\n\n\n\n7.3.3 Examining the data structure\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 7a",
    "section": "7.4 Plotting Cycle Plot",
    "text": "7.4 Plotting Cycle Plot\nIn this section, we will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n7.4.1 Step 1: Data Import\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair <- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n7.4.2 Step 2: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month <- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year <- year(ymd(air$`Month-Year`))\n\n\n\n7.4.3 Step 3: Extracting the target country\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam).\n\nVietnam <- air %>% \n  select(`Vietnam`, \n         month, \n         year) %>%\n  filter(year >= 2010)\n\n\n\n7.4.4 Step 4: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data <- Vietnam %>% \n  group_by(month) %>%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n7.4.5 Step 5: Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-slopegraph",
    "title": "Hands-on Exercise 7a",
    "section": "7.5 Plotting Slopegraph",
    "text": "7.5 Plotting Slopegraph\nIn this section we will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment.\n\n7.5.1 Step 1: Data Import\nImport the rice data set into R environment by using the code chunk below.\n\nrice <- read_csv(\"data/rice.csv\")\n\n\n\n7.5.2 Step 2: Plotting the slopegraph\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor.\n\nrice %>% \n  mutate(Year = factor(Year)) %>%\n  filter(Year %in% c(1961, 1980)) %>%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared using notes provided by: Dr. Kam Tin Seong\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07b.html#overview",
    "title": "Hands-on Exercise 7b",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nA horizon graph is an analytical graphical method specially designed for visualising large numbers of time-series. It aims to overcome the issue of visualising highly overlapping time-series as shown in the figure below.\n\nA horizon graph essentially an area chart that has been split into slices and the slices then layered on top of one another with the areas representing the highest (absolute) values on top. Each slice has a greater intensity of colour based on the absolute value it represents."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07b.html#getting-started",
    "title": "Hands-on Exercise 7b",
    "section": "7.2 Getting started",
    "text": "7.2 Getting started\nBefore getting start, we need to make sure that ggHoriPlot is loaded.\n\npacman::p_load('plotly', 'tidyverse', 'ggHoriPlot', 'ggthemes')\n#tidyverse covers lubridate, dplyr, tidyr already"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07b.html#data-import",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07b.html#data-import",
    "title": "Hands-on Exercise 7b",
    "section": "7.2 Data Import",
    "text": "7.2 Data Import\nFor the purpose of this hands-on exercise, Average Retail Prices Of Selected Consumer Items will be used. We use the code chunk below to import the AVERP.csv file into R environment.\n\naverp <- read_csv(\"data/AVERP.csv\") \n\n\naverp <- read_csv(\"data/AVERP.csv\") %>%\n  mutate(`Date` = dmy(`Date`))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07b.html#plotting-the-horizon-graph",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07/Hands-on_Ex07b.html#plotting-the-horizon-graph",
    "title": "Hands-on Exercise 7b",
    "section": "7.3 Plotting the horizon graph",
    "text": "7.3 Plotting the horizon graph\nNext, the code chunk below will be used to plot the horizon graph.\n\naverp %>% \n  filter(Date >= \"2018-01-01\") %>%\n  ggplot() + # still need ggplot as geom_horizon is an extension of ggplot()\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) + # need to have '' due to the space, ~ refers to function\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") + #breakline at 3 months\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08a.html#overview",
    "title": "Hands-on Exercise 8a",
    "section": "8.1 Overview",
    "text": "8.1 Overview\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, we will learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08a.html#getting-started",
    "title": "Hands-on Exercise 8a",
    "section": "8.2 Getting Started",
    "text": "8.2 Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that, we only need to install tidyverse instead of readr, tidyr and dplyr individually."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08a.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08a.html#importing-data-into-r",
    "title": "Hands-on Exercise 8a",
    "section": "8.3 Importing Data into R",
    "text": "8.3 Importing Data into R\n\n8.3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n8.3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Users\\p-RIP\\Documents\\leepeckkhee\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex08\\Hands-on_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nWe can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n8.3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n8.3.4 Data Preparation\nBefore a thematic map can be prepared, we are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n8.3.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n8.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above: - left_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame. write_rds(mpsz_pop2020, “data/rds/mpszpop2020.rds”)\n\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08a.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08a.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 8a",
    "section": "8.4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "8.4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n8.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above: - tmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used. - fill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n8.4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will share the tmap functions that used to plot these elements.\n\n8.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n8.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons(): - The default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3. - The default colour scheme used is YlOrRd of ColorBrewer. We will learn more about the color scheme in sub-section 4.4. - By default, Missing value will be shaded in grey.\n\n\n8.4.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n8.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n8.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n8.4.3.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n8.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n8.4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n8.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n8.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n8.4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n8.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n8.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n8.4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n8.4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, we can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#overview",
    "title": "Hands-on Exercise 8b",
    "section": "Overview",
    "text": "Overview\nProportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, we can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, we will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#learning-outcome",
    "title": "Hands-on Exercise 8b",
    "section": "8.1 Learning Outcome",
    "text": "8.1 Learning Outcome\nBy the end of this hands-on exercise, we will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#getting-started",
    "title": "Hands-on Exercise 8b",
    "section": "8.2 Getting Started",
    "text": "8.2 Getting Started\nBefore we get started, we need to ensure that tmap package of R and other related R packages have been installed and loaded into R.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#the-data",
    "title": "Hands-on Exercise 8b",
    "section": "8.3 The Data",
    "text": "8.3 The Data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#data-import-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#data-import-and-preparation",
    "title": "Hands-on Exercise 8b",
    "section": "8.4 Data Import and Preparation",
    "text": "8.4 Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools <- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   <chr>          <chr>      <dbl>  <dbl>  <dbl> <chr>                     <dbl>\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nNotice that the sgpools data is in tibble data frame and not the common R data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 8b",
    "section": "8.5 Creating a sf data frame from an aspatial data frame",
    "text": "8.5 Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf <- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above: - The coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates. - The crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\n\nWe can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * <chr>                        <chr>      <dbl> <chr>                     <dbl>\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry <POINT [m]>\n\n\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#drawing-proportional-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08b.html#drawing-proportional-symbol-map",
    "title": "Hands-on Exercise 8b",
    "section": "8.6 Drawing Proportional Symbol Map",
    "text": "8.6 Drawing Proportional Symbol Map\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\n\n8.6.1 It all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n8.6.2 Lets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n8.6.3 Lets give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n8.6.4 I have a twin brothers :)\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#overview",
    "title": "Hands-on Exercise 8c",
    "section": "8.1 Overview",
    "text": "8.1 Overview"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#objectives",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#objectives",
    "title": "Hands-on Exercise 8c",
    "section": "8.1.1 Objectives",
    "text": "8.1.1 Objectives\nIn this exercise, we will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n8.1.2 Learning outcome\nBy the end of this exercise, we will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map = Creating boxmap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#getting-sstarted",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#getting-sstarted",
    "title": "Hands-on Exercise 8c",
    "section": "8.2 Getting sStarted",
    "text": "8.2 Getting sStarted\n\n8.2.1 Installing and loading packages\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n8.2.2 Importing data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. We can find the data set in the rds sub-direct of the hands-on data folder.\n\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 8c",
    "section": "8.3 Basic Choropleth Mapping",
    "text": "8.3 Basic Choropleth Mapping\n\n8.3.1 Visualising distribution of non-functional water point\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 8c",
    "section": "8.4 Choropleth Map for Rates",
    "text": "8.4 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n8.4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n8.4.2 Plotting map of rate\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08/Hands-on_Ex08c.html#extreme-value-maps",
    "title": "Hands-on Exercise 8c",
    "section": "8.5 Extreme Value Maps",
    "text": "8.5 Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n8.5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n8.5.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\n8.5.1.2\nWhy writing functions? Writing a function has three big advantages over using copy-and-paste: - You can give a function an evocative name that makes your code easier to understand. - As requirements change, you only need to update code in one place, instead of many. - You eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\n\n8.5.1.3 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\narguments: - vname: variable name (as character, in quotes) - df: name of sf data frame\nreturns: - v: vector with values (without a column name)\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% \n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n8.5.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n8.5.1.5 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n8.5.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n8.5.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\narguments: - v: vector with observations - mult: multiplier for IQR (default 1.5)\nreturns: - bb: vector with 7 break points compute quartile and fences\n\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\n\n8.5.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\narguments: - vname: variable name (as character, in quotes) - df: name of sf data frame\nreturns: - v: vector with values (without a column name)\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n8.5.2.3 Test drive the newly created function\nLet’s test the newly created function\n\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n8.5.2.4 Boxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#overview",
    "title": "Hands-on Exercise 8b",
    "section": "Overview",
    "text": "Overview\nProportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, we can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, we will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#learning-outcome",
    "title": "Hands-on Exercise 8b",
    "section": "8.1 Learning Outcome",
    "text": "8.1 Learning Outcome\nBy the end of this hands-on exercise, we will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#getting-started",
    "title": "Hands-on Exercise 8b",
    "section": "8.2 Getting Started",
    "text": "8.2 Getting Started\nBefore we get started, we need to ensure that tmap package of R and other related R packages have been installed and loaded into R.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#the-data",
    "title": "Hands-on Exercise 8b",
    "section": "8.3 The Data",
    "text": "8.3 The Data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#data-import-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#data-import-and-preparation",
    "title": "Hands-on Exercise 8b",
    "section": "8.4 Data Import and Preparation",
    "text": "8.4 Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools <- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   <chr>          <chr>      <dbl>  <dbl>  <dbl> <chr>                     <dbl>\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nNotice that the sgpools data is in tibble data frame and not the common R data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 8b",
    "section": "8.5 Creating a sf data frame from an aspatial data frame",
    "text": "8.5 Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf <- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above: - The coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates. - The crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\n\nWe can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * <chr>                        <chr>      <dbl> <chr>                     <dbl>\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry <POINT [m]>\n\n\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#drawing-proportional-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08b.html#drawing-proportional-symbol-map",
    "title": "Hands-on Exercise 8b",
    "section": "8.6 Drawing Proportional Symbol Map",
    "text": "8.6 Drawing Proportional Symbol Map\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\n\n8.6.1 It all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n8.6.2 Lets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n8.6.3 Lets give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n8.6.4 I have a twin brothers :)\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#overview",
    "title": "Hands-on Exercise 7b",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nA horizon graph is an analytical graphical method specially designed for visualising large numbers of time-series. It aims to overcome the issue of visualising highly overlapping time-series as shown in the figure below.\n\nA horizon graph essentially an area chart that has been split into slices and the slices then layered on top of one another with the areas representing the highest (absolute) values on top. Each slice has a greater intensity of colour based on the absolute value it represents."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#getting-started",
    "title": "Hands-on Exercise 7b",
    "section": "7.2 Getting started",
    "text": "7.2 Getting started\nBefore getting start, we need to make sure that ggHoriPlot is loaded.\n\npacman::p_load('plotly', 'tidyverse', 'ggHoriPlot', 'ggthemes')\n#tidyverse covers lubridate, dplyr, tidyr already"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#data-import",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#data-import",
    "title": "Hands-on Exercise 7b",
    "section": "7.2 Data Import",
    "text": "7.2 Data Import\nFor the purpose of this hands-on exercise, Average Retail Prices Of Selected Consumer Items will be used. We use the code chunk below to import the AVERP.csv file into R environment.\n\naverp <- read_csv(\"data/AVERP.csv\") \n\n\naverp <- read_csv(\"data/AVERP.csv\") %>%\n  mutate(`Date` = dmy(`Date`))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#plotting-the-horizon-graph",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#plotting-the-horizon-graph",
    "title": "Hands-on Exercise 7b",
    "section": "7.3 Plotting the horizon graph",
    "text": "7.3 Plotting the horizon graph\nNext, the code chunk below will be used to plot the horizon graph.\n\naverp %>% \n  filter(Date >= \"2018-01-01\") %>%\n  ggplot() + # still need ggplot as geom_horizon is an extension of ggplot()\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) + # need to have '' due to the space, ~ refers to function\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") + #breakline at 3 months\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#learning-outcome",
    "title": "Hands-on Exercise 7a",
    "section": "7.1 Learning Outcome",
    "text": "7.1 Learning Outcome\nBy the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#getting-started",
    "title": "Hands-on Exercise 7a",
    "section": "7.2 Getting started",
    "text": "7.2 Getting started\nBefore getting start, we need to make sure that respective packages below are installed and launched.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 7a",
    "section": "7.3 Plotting Calendar Heatmap",
    "text": "7.3 Plotting Calendar Heatmap\nIn this section, we will learn how to plot a calender heatmap programmetically by using ggplot2 package.\nBy the end of this section, we will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n7.3.1 The Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n7.3.2 Importing the data\nFirst, we will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks <- read_csv(\"data/eventlog.csv\")\n\n\n\n7.3.3 Examining the data structure\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 7a",
    "section": "7.4 Plotting Cycle Plot",
    "text": "7.4 Plotting Cycle Plot\nIn this section, we will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n7.4.1 Step 1: Data Import\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair <- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n7.4.2 Step 2: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month <- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year <- year(ymd(air$`Month-Year`))\n\n\n\n7.4.3 Step 3: Extracting the target country\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam).\n\nVietnam <- air %>% \n  select(`Vietnam`, \n         month, \n         year) %>%\n  filter(year >= 2010)\n\n\n\n7.4.4 Step 4: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data <- Vietnam %>% \n  group_by(month) %>%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n7.4.5 Step 5: Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-slopegraph",
    "title": "Hands-on Exercise 7a",
    "section": "7.5 Plotting Slopegraph",
    "text": "7.5 Plotting Slopegraph\nIn this section we will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment.\n\n7.5.1 Step 1: Data Import\nImport the rice data set into R environment by using the code chunk below.\n\nrice <- read_csv(\"data/rice.csv\")\n\n\n\n7.5.2 Step 2: Plotting the slopegraph\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor.\n\nrice %>% \n  mutate(Year = factor(Year)) %>%\n  filter(Year %in% c(1961, 1980)) %>%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared using notes provided by: Dr. Kam Tin Seong\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08a.html#overview",
    "title": "Hands-on Exercise 8a",
    "section": "8.1 Overview",
    "text": "8.1 Overview\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, we will learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08a.html#getting-started",
    "title": "Hands-on Exercise 8a",
    "section": "8.2 Getting Started",
    "text": "8.2 Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that, we only need to install tidyverse instead of readr, tidyr and dplyr individually."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08a.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08a.html#importing-data-into-r",
    "title": "Hands-on Exercise 8a",
    "section": "8.3 Importing Data into R",
    "text": "8.3 Importing Data into R\n\n8.3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n8.3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Users\\p-RIP\\Documents\\leepeckkhee\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nWe can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n8.3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n8.3.4 Data Preparation\nBefore a thematic map can be prepared, we are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n8.3.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n8.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above: - left_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame. write_rds(mpsz_pop2020, “data/rds/mpszpop2020.rds”)\n\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08a.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08a.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 8a",
    "section": "8.4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "8.4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n8.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above: - tmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used. - fill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n8.4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will share the tmap functions that used to plot these elements.\n\n8.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n8.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons(): - The default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3. - The default colour scheme used is YlOrRd of ColorBrewer. We will learn more about the color scheme in sub-section 4.4. - By default, Missing value will be shaded in grey.\n\n\n8.4.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n8.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n8.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n8.4.3.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n8.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n8.4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n8.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n8.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n8.4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n8.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n8.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n8.4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n8.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n8.4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, we can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#overview",
    "title": "Hands-on Exercise 8c",
    "section": "8.1 Overview",
    "text": "8.1 Overview"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#objectives",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#objectives",
    "title": "Hands-on Exercise 8c",
    "section": "8.1.1 Objectives",
    "text": "8.1.1 Objectives\nIn this exercise, we will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n8.1.2 Learning outcome\nBy the end of this exercise, we will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map = Creating boxmap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#getting-sstarted",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#getting-sstarted",
    "title": "Hands-on Exercise 8c",
    "section": "8.2 Getting sStarted",
    "text": "8.2 Getting sStarted\n\n8.2.1 Installing and loading packages\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n8.2.2 Importing data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. We can find the data set in the rds sub-direct of the hands-on data folder.\n\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 8c",
    "section": "8.3 Basic Choropleth Mapping",
    "text": "8.3 Basic Choropleth Mapping\n\n8.3.1 Visualising distribution of non-functional water point\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 8c",
    "section": "8.4 Choropleth Map for Rates",
    "text": "8.4 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n8.4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n8.4.2 Plotting map of rate\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08c.html#extreme-value-maps",
    "title": "Hands-on Exercise 8c",
    "section": "8.5 Extreme Value Maps",
    "text": "8.5 Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n8.5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n8.5.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\nNGA_wp is a sf layer, however the function does not understand it. Therefore, we need to set st_set_geometry as null.\n\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\n8.5.1.2 Why writing functions?\nWriting a function has three big advantages over using copy-and-paste: - You can give a function an evocative name that makes your code easier to understand. - As requirements change, you only need to update code in one place, instead of many. - You eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\n\n8.5.1.3 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\narguments: - vname: variable name (as character, in quotes) - df: name of sf data frame\nreturns: - v: vector with values (without a column name)\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% \n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n8.5.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below, where df is parameter, while vnam is the input parameter.\n\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df) #get value from my dataframe\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n8.5.1.5 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n8.5.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n8.5.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\narguments: - v: vector with observations - mult: multiplier for IQR (default 1.5)\nreturns: - bb: vector with 7 break points compute quartile and fences\n\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\n\n8.5.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\narguments: - vname: variable name (as character, in quotes) - df: name of sf data frame\nreturns: - v: vector with values (without a column name)\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n8.5.2.3 Test drive the newly created function\nLet’s test the newly created function\n\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n8.5.2.4 Boxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "As illegal, unreported and unregulated fishing continues to be a major contributor to overfishing worldwide, contributing to estimated global losses of approximately $50bn, FishEye hopes to leverage on visual analytics to understand patterns and highlight anomalous groups via the usage of knowledge graph.\n\nFor quick reference to the MC3 challenge, Question 1 write-up, please refer to Section 6."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#description-of-dataset",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#description-of-dataset",
    "title": "Take-home Exercise 3",
    "section": "2. Description of Dataset",
    "text": "2. Description of Dataset\nThe dataset utilised for the below analysis consists of a total of 27,622 nodes 24,038 edges and 7,794 connected components. It is an undirected multi-graph in a json format.\n\nPossible node types include: {company and person}\nPossible node sub types include: (beneficial owner and company contacts}\nPossible edge types include: {person}\nPossible edge sub types include: {beneficial owner and company contacts}\n\nThe full details can be found on: https://vast-challenge.github.io/2023/MC3.html"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-wrangling-and-preparation",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-wrangling-and-preparation",
    "title": "Take-home Exercise 3",
    "section": "3. Data Wrangling and Preparation",
    "text": "3. Data Wrangling and Preparation\n\n3.1 Installing Requisite R packages\nThe code chunk below uses p_load() of pacman package to check if the said packages are installed in the computer. If they are, then they will be launched into R.\n\njsonlite: Enables us to import the json file for further analysis\ntidygraph: Enables us to manipulate, analyze, and visualize graphs using a consistent and tidy syntax\nggraph: An extension of the ggplot2 package with tools to create visualizations of graphs and networks\nvisNetwork: Enables us to create interactive network visualizations in R\ngraphlayouts: Provides various graph layout algorithms such as Fruchterman-Reingold, Kamada-Kawai for graph visualisation\nggforce: Extension of the ggplot2 package by providing additional plotting functions and geoms\nskimr: Provides tools for quickly summarizing and visualizing data in a tidy format, and enables one to get a quick overview of the data\ntidytext: Enables us to perform various text preprocessing tasks and provides functions for analyzing text data\ntopicmodels: Provides functions for fitting and analyzing topic models, as well as identifying representative words for each topic\ntidyverse: A collection of packages that enables a consistent and tidy data manipulation and analysis workflow in R\n\n\n\nShow code\npacman::p_load(jsonlite, tidygraph, ggraph, igraph,\n               visNetwork, graphlayouts, ggforce, \n               skimr, tidytext, topicmodels, tidyverse)\n\n\n\n\n3.2 Loading the Dataset\nWe first start off by loading the mc3.json dataset into “mc3_data” by using fromJSON() of jsonlit package below. The resulting output is “mc3_data” and is stored as a large list R object.\n\n\nShow code\n#importing json file by using jsonlite package\nmc3_data <- fromJSON(\"data/MC3.json\")\n\n\n\n\n3.3 Data Cleaning\n\n3.3.1 Edge Extraction\nThe code chunk below is used to extract the links dataframe of mc3_data and save it as tibble dataframe called mc3_edges.\n\n\nShow code\nmc3_edges <- as_tibble(mc3_data$links)\n\n\nData cleaning is performed by utilising a combination of:\n\ndistinct(): to remove duplicates records\nmutate() and as.character(): to convert the field data type from list to character\nfilter(): to remove records where source = target\n\n\n\nShow code\nmc3_edges <- as_tibble(mc3_data$links) %>% \n  distinct() %>%\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type)) %>%\n  filter(source!=target)\n\n\nUpon further inspection, we noticed that there are cells that contains a list of strings within the source column.\n\n\nShow code\nto_unpack_further <- mc3_edges[grepl(\"^c\\\\(\", mc3_edges$source), ]\nto_unpack_further\n\n\n# A tibble: 2,169 × 3\n   source                                                           target type \n   <chr>                                                            <chr>  <chr>\n 1 \"c(\\\"Assam   Limited Liability Company\\\", \\\"Assam   Limited Lia… Marcu… Bene…\n 2 \"c(\\\"Assam   Limited Liability Company\\\", \\\"Assam   Limited Lia… Keith… Bene…\n 3 \"c(\\\"Assam   Limited Liability Company\\\", \\\"Assam   Limited Lia… Thoma… Bene…\n 4 \"c(\\\"Assam   Limited Liability Company\\\", \\\"Assam   Limited Lia… Yolan… Bene…\n 5 \"c(\\\"Assam   Limited Liability Company\\\", \\\"Assam   Limited Lia… Jenni… Bene…\n 6 \"c(\\\"Assam   Limited Liability Company\\\", \\\"Assam   Limited Lia… Micha… Bene…\n 7 \"c(\\\"Assam   Limited Liability Company\\\", \\\"Assam   Limited Lia… Saman… Comp…\n 8 \"c(\\\"Oceanic Explorers Plc Salt spray\\\", \\\"The Salted Pearl Inc… Laure… Bene…\n 9 \"c(\\\"Oceanic Explorers Plc Salt spray\\\", \\\"The Salted Pearl Inc… Natal… Bene…\n10 \"c(\\\"Oceanic Explorers Plc Salt spray\\\", \\\"The Salted Pearl Inc… Ricky… Comp…\n# ℹ 2,159 more rows\n\n\nAs such, we utilised mutate() and separate_rows() to unpack the strings. Note that the key usage of separate_rows() is to handle data that are stored in a nested list format. Using separate_rows(), we can transform the data into a “tidy” format, with each value occupying its own row.\n\n\nShow code\nmc3_edges <- mc3_edges %>%\n  mutate(source = gsub(\"^c\\\\(|\\\"\\\\)$\", \"\", source)) %>%\n  separate_rows(source, sep = \"\\\", \\\"\") %>%\n  mutate(source = gsub(\"\\\"\", \"\", source)) %>%\n  group_by(source, target, type) %>%\n  distinct() %>%\n  ungroup()\n\n\n\n\n3.3.2 Nodes Extraction\nThe code chunk below is used to extract the nodes dataframe of mc3_data and save it as tibble dataframe called mc3_nodes.\n\nmutate() and as.character() are used to convert the field data type from list to character.\nas.character() is used to convert revenue_omu from list data type to numeric data type by first converting the values into character. as.numeric() will then be used thereafter to convert them into numeric data type.\nselect() is used to re-orgnanise the order of the selected fields.\n\n\n\nShow code\nmc3_nodes <- as_tibble(mc3_data$nodes) %>%\n  mutate(country = as.character(country),\n         id = as.character(id),\n         product_services = as.character(product_services),\n         revenue_omu = as.numeric(as.character(revenue_omu)),\n         type = as.character(type)) %>%\n  select(id, country, type, revenue_omu, product_services)\n\n\n\n3.3.2.1 Text Sensing with tidytext\nLet’s attempt to first perform a simple word count of the word fish. The below code chunk calculates the number of times the word fish appears in “product_services” column. We can see that there are several nodes with product_services that are not related to fish.\n\n\nShow code\nmc3_nodes %>% \n    mutate(n_fish = str_count(product_services, \"fish\")) \n\n\n# A tibble: 27,622 × 6\n   id                          country type  revenue_omu product_services n_fish\n   <chr>                       <chr>   <chr>       <dbl> <chr>             <int>\n 1 Jones LLC                   ZH      Comp…  310612303. Automobiles           0\n 2 Coleman, Hall and Lopez     ZH      Comp…  162734684. Passenger cars,…      0\n 3 Aqua Advancements Sashimi … Oceanus Comp…  115004667. Holding firm wh…      0\n 4 Makumba Ltd. Liability Co   Utopor… Comp…   90986413. Car service, ca…      0\n 5 Taylor, Taylor and Farrell  ZH      Comp…   81466667. Fully electric …      0\n 6 Harmon, Edwards and Bates   ZH      Comp…   75070435. Discount superm…      0\n 7 Punjab s Marine conservati… Riodel… Comp…   72167572. Beef, pork, chi…      0\n 8 Assam   Limited Liability … Utopor… Comp…   72162317. Power and Gas s…      0\n 9 Ianira Starfish Sagl Import Rio Is… Comp…   68832979. Light commercia…      0\n10 Moran, Lewis and Jimenez    ZH      Comp…   65592906. Automobiles, tr…      0\n# ℹ 27,612 more rows\n\n\n\n\n3.3.2.2 Tokenisation\nThe code chunk below leverages on unnest_token() of tidytext to split text in product_services column into individual words. First we have the output column name that will be created as the text is unnested into it (word, in this case), and then the input column that the text comes from (product_services, in this case).\nNote that by default, all punctuation have been stripped, and all tokens are converted to lowercase to enable easy comparison.\n\n\nShow code\ntoken_nodes <- mc3_nodes %>%\n  unnest_tokens(word, \n                product_services)\n\n\nNext, let’s leverage on ggplot() to visualise the words that were extracted via the below code chunk. We can see that there are several stopwords that are not meaningful, for example “of”, “as” and “for” etc.\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\ntoken_nodes %>%\n  count(word, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(x = word, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  xlab(NULL) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5)) +\n  coord_flip() +\n      labs(x = \"Count\",\n      y = \"Unique words\",\n      title = \"Count of unique words found in product_services field\")\n\n\n\n\n\n\n3.3.2.3 Removing stopwords\nTherefore, we proceed to remove the stopwords via a function from the tidytext package called stop_words. Note that the anti_join() function from the dplyr package is used to remove all stopwords from the analysis.\n\n\nShow code\nstopwords_removed <- token_nodes %>% \n  anti_join(stop_words)\n\n\nWe then visualise the extracted words (without stopwords) using the below code chunk. We can see that there are still several words that are not related to our scope of analysis focusing on the fishing industry.\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\nstopwords_removed %>%\n  count(word, sort = TRUE) %>%\n  top_n(30) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(x = word, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  xlab(NULL) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5)) +\n  coord_flip() +\n      labs(x = \"Count\",\n      y = \"Unique words\",\n      title = \"Count of unique words found in product_services field\")\n\n\n\n\nAs such, we proceed to remove words such as “character”, “0”, “unknown” etc. This step was done in an iterative manner to ensure that we capture as most fishery related keywords. We then visualise the top 30 words that are closely related to our analysis scope.\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\nstopwords_removed %>%\n  filter(!word %in% c(\"character\", \"0\", \"unknown\", \"products\", \n                      \"services\", \"food\", \"related\", \"equipment\",\n                      \"accessories\", \"materials\", \"including\",\n                      \"industrial\", \"meat\", \"canned\", \"systems\", \"freight\",\n                      \"offers\", \"machines\", \"range\", \"processing\",\n                      \"steel\", \"transportation\", \"supplies\", \"shoes\",\n                      \"logistics\", \"vegetables\", \"metal\", \"solutions\",\n                      \"packaging\", \"source\", \"researcher\", \"freelance\",\n                      \"footwear\", \"management\", \"chemicals\", \"machinery\",\n                      \"plastic\", \"air\", \"components\", \"manufacturing\",\n                      \"tools\", \"distribution\", \"water\", \"foods\", \"wide\",\n                      \"oil\", \"electronic\", \"fruits\", \"adhesives\",\n                      \"apparel\", \"power\", \"bags\", \"care\", \"service\",\n                      \"casting\", \"industry\", \"household\", \"oils\",\n                      \"raw\", \"cargo\", \"technology\", \"specialty\",\n                      \"aluminum\", \"home\", \"items\", \"grocery\", \"cooked\",\n                      \"transport\", \"storage\", \"specialises\", \"smoked\",\n                      \"rubber\", \"paper\", \"fabrics\", \"electrical\", \"control\", \n                      \"activities\", \"line\", \"dried\", \"production\", \"construction\", \n                      \"pharmaceutical\", \"machine\", \"clothing\", \"prepared\",\n                      \"poultry\", \"canning\", \"product\", \"forwarding\", \"development\",\n                      \"include\", \"glue\", \"furniture\", \"consumer\", \"business\", \n                      \"automotive\", \"commercial\", \"fabric\", \"dry\", \"chemical\",\n                      \"warehousing\", \"die\", \"customs\", \"sole\", \"iron\",\n                      \"packing\", \"office\", \"industries\", \"applications\",\n                      \"special\", \"preparation\", \"international\", \"beverages\",\n                      \"gelatin\", \"design\", \"based\", \"natural\", \"meats\", \"custom\", \n                      \"adhesive\",\"textile\", \"system\", \"stationery\", \"processed\", \n                      \"leather\", \"electric\", \"dairy\", \"trucking\", \"personal\", \"medical\", \n                      \"hot\", \"fats\", \"building\", \"beef\")) %>%\n  count(word, sort = TRUE) %>%\n  top_n(30) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(x = word, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  xlab(NULL) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5)) +\n  coord_flip() +\n  labs(x = \"Count\",\n       y = \"Unique words\",\n       title = \"Count of unique words found in product_services field\")\n\n\n\n\n\n\n3.3.2.4 Topic Modelling\nWe also performed topic modelling in an attempt to further bucket keywords related to fishing so that this can help aid in further refining our analysis scope for mc3_nodes subsequently. We begin by creating a document-term matrix and storing it in dtm, before applying topic modelling using LDA to obtain 5 key topics. Finally, we then extract the top 50 terms associated with each topic.\nNote that:\n\ncast_dtm(): converts the count data into a document-term matrix, with each row representing a document and each column representing a term (ie. word)\nLDA(): function from topicmodels package to create an LDA model\nterms(): extracts the most probable term associated with each topic from the LDA model\n\n\n\nShow code\n# Create a document-term matrix\ndtm <- stopwords_removed %>%\n  count(id, word) %>%\n  cast_dtm(id, word, n)\n\n# Apply topic modeling using LDA\nlda_model <- LDA(dtm, k = 5)\n\n# Extract the terms associated with each topic\ntopics <- terms(lda_model, 50)\n\n\n\n\n3.3.2.5 Filtered list of nodes related to fishing [mc3_nodes_fish]\nTherefore, mc3_nodes was then furthered refined based on the keywords we identified from the sections 3.3.2.3 and 3.3.2.4 and saved as mc3_nodes_fish\n\n\nShow code\nmc3_nodes_fish <- mc3_nodes[grepl(\"\\\\b(aquatic|clams|cod|crab|crabs|crustaceans|fillet|fillets|fish(es)?|fishing|flounder|fresh|halibut|herring|lobster|marine|octopus|oysters|pacific|pollock|salmon|sea|seafood|seafoods|shellfish|shrimp|shrimps|sockeye|sole|squid|trout|tuna)\\\\b\", mc3_nodes$product_services, ignore.case = TRUE), ]\n\nmc3_nodes_fish\n\n\n# A tibble: 1,126 × 5\n   id                                 country type  revenue_omu product_services\n   <chr>                              <chr>   <chr>       <dbl> <chr>           \n 1 Hopkins LLC                        ZH      Comp…   14667942. Operates tramp …\n 2 Caracola del Mar NV Family         Rio Is… Comp…    7085566. Canned, frozen …\n 3 Rollins, Mercado and Miller        ZH      Comp…    5672004. Pharmaceuticals…\n 4 Krause Ltd                         ZH      Comp…    4532443. Fresh and cooke…\n 5 Hawkins-Benson                     ZH      Comp…    2040575. Exports salmon,…\n 6 Sea Star LLC Shipping              Zawali… Comp…    1507514. Operation of fi…\n 7 Hensley-Martinez                   ZH      Comp…    1288524. Development of …\n 8 Jammu & Kashmir Sea  Sagl Merchan… Puerto… Comp…    1275143. Land operations…\n 9 Mar de la Vida AG                  Rio Is… Comp…    1205868. Fish and meat p…\n10 Turkish Calamari AB Marine conser… Kondan… Comp…    1167093. Standard, marin…\n# ℹ 1,116 more rows\n\n\n\n\n3.3.2.6 Further cleaning of mc3_nodes_fish\nUpon further data investigation, we noticed that there are multiple rows with same ID. Therefore, we grouped by ID, country and type to gain the summed revenue_omu by each unique record. product_services was also subsequently concatenated and updated accordingly.\n\n\nShow code\nmc3_nodes_fish <- mc3_nodes_fish %>%\n  group_by(id, country, type) %>%\n  summarise(revenue_omu = sum(revenue_omu), product_services = paste(product_services, collapse = \"; \"), .groups = \"drop\")\n\n\n\n\n\n\n3.4 Exploratory Data Analysis\n\n3.4.1 mc3_edges\nThe below code chunk leverages on skim() of skimr package to display a summary statistics of mc3_edges tibble data frame. We can observe that there are no missing values in all the fields.\n\n\nShow code\nskim(mc3_edges)\n\n\n\nData summary\n\n\nName\nmc3_edges\n\n\nNumber of rows\n24937\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsource\n0\n1\n6\n81\n0\n13162\n0\n\n\ntarget\n0\n1\n6\n28\n0\n21265\n0\n\n\ntype\n0\n1\n16\n16\n0\n2\n0\n\n\n\n\n\nThe below code chunk leverages on datatable() of DT package to display mc3_edges tibble dataframe as an interactive table on the html document.\n\n\nShow code\nDT::datatable(mc3_edges)\n\n\n\n\n\n\n\n\n3.4.1.1 Relationship categorisation\nFirst, let’s take a look at the mc3_edges type of relationship categorisation. There are two main types, namely “Beneficial Owner” and “Company Contacts”.\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data = mc3_edges, aes(x = type)) +\n  geom_bar(fill = \"steelblue\") +\n  geom_text(\n    aes(label = ..count..),\n    stat = \"count\",\n    vjust = -0.5,\n    size = 3) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.line = element_line(color = \"grey\", size = 0.5),\n    plot.title = element_text(hjust = 0.5)) +\n  labs(x = \"Relationship Type\", y = \"Frequency Count\") +\n  ggtitle(\"Distribution of Relationship Types within mc3_edges\")\n\n\n\n\n\n\n3.4.1.2 Number of companies each respective beneficial owner owns\nWithin this section, we calculated the number of companies each beneficial owner owns and save it as a new column in mc3_edges_bo as bo_target_count. Note that mc3_edges_bo only contains beneficial owner.\nWe utilised:\n\nfilter(): to filter for beneficial owner and company contact respectively\ncount(): to count the number of business a beneficial owner owns, as well as the number of companies that a company contact has access to respectively\n\n\n\nShow code\nbo_target_count <- mc3_edges %>%\n  filter(type == \"Beneficial Owner\") %>%\n  count(target, name = \"bo_target_count\")\n\nmc3_edges_bo <- mc3_edges %>%\n  filter(type == \"Beneficial Owner\") %>%\n  left_join(bo_target_count, by = \"target\") %>%\n  filter(source!=target) \n\n\nFrom the plot below, we can see that most beneficial owners usually own 1 company. We also observe that it is rare for beneficial owners to own several companies\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\nggplot(mc3_edges_bo, aes(x = bo_target_count)) +\n  geom_bar(fill = \"steelblue\") +\n  geom_text(\n    aes(label = ..count..),\n    stat = \"count\",\n    vjust = -0.5,\n    size = 3) +  \n  labs(x = \"Number of Companies owned by Beneficial Owners\", y = \"Frequency Count\") +\n  ggtitle(\"Distribution on Number of companies owned by Beneficial Owners\") +\n  scale_x_continuous(breaks = seq(min(mc3_edges_bo$bo_target_count), max(mc3_edges_bo$bo_target_count), by = 1)) + \n  theme_minimal() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.line = element_line(color = \"grey\", size = 0.5),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n3.4.1.3 Number of companies each company contact is linked to\nWithin this section, we calculated the number of companies each company contact is linked to and save it as a new column in mc3_edges_cc as cc_target_count. Likewise, we observe that company contacts are mostly linked to one companies, with a few exception where there are a minority group of company contacts that can be linked to more than 4 companies.\nNote that mc3_edges_cc only contains company contacts.\n\n\nShow code\ncc_target_count <- mc3_edges %>%\n  filter(type == \"Company Contacts\") %>%\n  count(target, name = \"cc_target_count\")\n\nmc3_edges_cc <- mc3_edges %>%\n  filter(type == \"Company Contacts\") %>%\n  left_join(cc_target_count, by = \"target\") %>%\n  filter(source!=target)\n\n\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\nggplot(mc3_edges_cc, aes(x = cc_target_count)) +\n  geom_bar(fill = \"steelblue\") +\n  geom_text(\n    aes(label = ..count..),\n    stat = \"count\",\n    vjust = -0.5,\n    size = 3) +    \n  labs(x = \"Number of companies that Company Contacts are linked to\", y = \"Frequency Count\") +\n  ggtitle(\"Distribution on Number of companies that Company Contacts are connected\") +\n  scale_x_continuous(breaks = seq(min(mc3_edges_cc$cc_target_count), max(mc3_edges_cc$cc_target_count), by = 1)) + \n  theme_minimal() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.line = element_line(color = \"grey\", size = 0.5),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n3.4.2 mc3_nodes_fish\nFrom the table below, we can see that there are approximately 89% of data available within revenue_omu variable. Hence, we need to exercise caution when using this column due to the missing data.\n\n\nShow code\nskim(mc3_nodes_fish)\n\n\n\nData summary\n\n\nName\nmc3_nodes_fish\n\n\nNumber of rows\n1116\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1\n8\n56\n0\n1108\n0\n\n\ncountry\n0\n1\n2\n15\n0\n56\n0\n\n\ntype\n0\n1\n7\n16\n0\n3\n0\n\n\nproduct_services\n0\n1\n4\n1139\n0\n719\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrevenue_omu\n122\n0.89\n6375658\n35156735\n4666.67\n16874.84\n36448.66\n85524.67\n308249623\n▇▁▁▁▁\n\n\n\n\n\nSimilar to the above, we also visualise mc3_nodes_fish via an interactive table on the html document.\n\n\nShow code\nDT::datatable(mc3_nodes_fish)\n\n\n\n\n\n\n\n\n3.4.2.1 Relationship categorisation\nFrom the below plot, we can visualise the mc3_nodes types of relationship categorisation. There are three main types, namely “Beneficial Owner”, “Company” and “Company Contacts”.\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data = mc3_nodes, aes(x = type)) +\n  geom_bar(fill = \"steelblue\") +\n  geom_text(\n    aes(label = ..count..),\n    stat = \"count\",\n    vjust = -0.5,\n    size = 3) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.line = element_line(color = \"grey\", size = 0.5),\n    plot.title = element_text(hjust = 0.5)) +\n  labs(x = \"Relationship Type\", y = \"Frequency Count\") +\n  ggtitle(\"Distribution of Relationship Types within mc3_nodes\")\n\n\n\n\nNote that after cleaning to focus only on the fishing related product_services, we land at the below plot. We can observe that most of the nodes remaining are of the type “Company”.\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\nggplot(data = mc3_nodes_fish, aes(x = type)) +\n  geom_bar(fill = \"steelblue\") +\n  geom_text(\n    aes(label = ..count..),\n    stat = \"count\",\n    vjust = -0.5,\n    size = 3) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.line = element_line(color = \"grey\", size = 0.5),\n    plot.title = element_text(hjust = 0.5)) +\n  labs(x = \"Relationship Type\", y = \"Frequency Count\") +\n  ggtitle(\"Distribution of Relationship Types within mc3_nodes\")\n\n\n\n\n\n\n3.4.2.2 Country analysis\nLet’s take a closer look at the country analysis within mc3_nodes_fish. Country ZH, followed by Oceanus and Marebak are the top 3 countries within mc3_nodes_fish.\n\nPlotCode Chunk\n\n\n\n\n\n\n\n\n\n\ncountry_counts <- mc3_nodes_fish %>%\n  count(country) %>%\n  top_n(5, n) %>%\n  arrange(desc(n))\n\nggplot(data = country_counts, aes(x = reorder(country, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(aes(label = n), vjust = -0.5, size = 3) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.line = element_line(color = \"grey\", size = 0.5),\n    plot.title = element_text(hjust = 0.5)\n  ) +\n  labs(x = \"Country\", y = \"Count\", title = \"Distribution by Country (Top 5)\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#initial-network-visualisation-and-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#initial-network-visualisation-and-analysis",
    "title": "Take-home Exercise 3",
    "section": "4. Initial Network Visualisation and Analysis",
    "text": "4. Initial Network Visualisation and Analysis\n\n4.1 Preparation\n\n4.1.1 Ensuring that mc3_edges (all/bo/cc)’s source only contains the filtered fishing related companies\nThis step is necessary to ensure that mc3_edges_all, mc3_edges_bo and mc3_edges_cc contains only the filtered fishing related companies. As a recap:\n\nmc3_edges_all contains the overall types (including beneficial owner and company contacts).\nmc3_edges_bo contains only beneficial owner type.\nmc3_edges_cc contains only company contacts type.\n\n\n\nShow code\nmc3_edges_all <- mc3_edges %>%\n  filter(source %in% mc3_nodes_fish$id)\n\nmc3_edges_bo <- mc3_edges_bo %>%\n  filter(source %in% mc3_nodes_fish$id)\n\nmc3_edges_cc <- mc3_edges_cc %>%\n  filter(source %in% mc3_nodes_fish$id)\n\n\n\n\n4.1.2 Preparing mc3_nodes to only contain the nodes found within mc3_edges\nThis step is necessary to ensure that the nodes in the mc3_nodes_all/bo/cc include all the source and target values from mc3_edges_all/bo/cc respectively.\n\n\nShow code\nid1 <- mc3_edges_all %>%\n  select(source) %>%\n  rename(id = source)\nid2 <- mc3_edges_all %>%\n  select(target) %>%\n  rename(id = target)\nmc3_nodes_all <- rbind(id1, id2) %>%\n  distinct() %>%\n  left_join(mc3_nodes_fish, by = \"id\",\n            unmatched = \"drop\")\n\nid1_bo <- mc3_edges_bo %>%\n  select(source) %>%\n  rename(id = source)\nid2_bo <- mc3_edges_bo %>%\n  select(target) %>%\n  rename(id = target)\nmc3_nodes_bo <- rbind(id1_bo, id2_bo) %>%\n  distinct() %>%\n  left_join(mc3_nodes_fish, by = \"id\",\n            unmatched = \"drop\")\n\nid1_cc <- mc3_edges_cc %>%\n  select(source) %>%\n  rename(id = source)\nid2_cc <- mc3_edges_cc %>%\n  select(target) %>%\n  rename(id = target)\nmc3_nodes_cc <- rbind(id1_cc, id2_cc) %>%\n  distinct() %>%\n  left_join(mc3_nodes_fish,\n            unmatched = \"drop\")\n\n\n\n\n4.1.3 Company_metrics calculation stored to mc3_nodes_fish\nWithin this step, we aim to calculate from the Company’s perspective before storing it back to mc3_nodes_all as two new columns, namely:\n\ncompany_contact_count: how many Company Contacts does the Company have?\nbeneficial_owner_count: how many Beneficial Owners does the Company have?\n\n\n\nShow code\ncompany_metrics <- mc3_edges_all %>%\n  group_by(source) %>%\n  summarise(\n    company_contact_count = sum(type == \"Company Contacts\"),\n    beneficial_owner_count = sum(type == \"Beneficial Owner\")\n  ) %>%\n  ungroup()\n\nmc3_nodes_all <- mc3_nodes_all %>%\n  left_join(company_metrics, by = c(\"id\" = \"source\")) %>%\n  mutate(\n    company_contact_count = ifelse(is.na(company_contact_count), 0, company_contact_count),\n    beneficial_owner_count = ifelse(is.na(beneficial_owner_count), 0, beneficial_owner_count)\n  )\n\n\n\n\n4.1.4 Revenue Quantiles within mc3_nodes_all, mc3_nodes_bo, mc3_nodes_cc\nWithin this step below, we split the revenues into four main quantiles within mc3_nodes_all, mc3_nodes_bo and mc3_nodes_cc. Quantile 1 refers to the lowest end of the revenue scale, while Quantile 4 refers to the highest end of the revenue scale. Note that the below steps assumes that if revenue_omu is not available, it will be categorised under Quantile 1.\n\n\nShow code\nmc3_nodes_all <- mc3_nodes_all %>%\n  mutate(quantile = ifelse(is.na(revenue_omu), 1, ntile(revenue_omu, 4)))\n\nquantile_counts_all <- mc3_nodes_all %>%\n  group_by(quantile) %>%\n  summarise(records = n())\n\nmc3_nodes_bo <- mc3_nodes_bo %>%\n  mutate(quantile = ifelse(is.na(revenue_omu), 1, ntile(revenue_omu, 4)))\n\nquantile_counts_bo <- mc3_nodes_bo %>%\n  group_by(quantile) %>%\n  summarise(records = n())\n\nmc3_nodes_cc <- mc3_nodes_cc %>%\n  mutate(quantile = ifelse(is.na(revenue_omu), 1, ntile(revenue_omu, 4)))\n\nquantile_counts_cc <- mc3_nodes_cc %>%\n  group_by(quantile) %>%\n  summarise(records = n())\n\n\n\n\n\n4.2 Building the tidy graph data model\n\n4.2.1 Using tbl_graph() to build a tidygraph data model for mc3_graph_all, mc3_graph_bo and mc3_graph_cc\nWithin this section, we build a basic tidygraph data model for mc3_graph_all, mc3_graph_bo and mc3_graph_cc. Thereafter, in Section 4.2.2 to 4.2.4, we will explore the threshold to use (either top 30%, 20% or 10% based on degree/closeness/betweeness centrality) via a series of static plots.\n\nA higher degree centrality indicates that the node has more connections than the average number of connections as compared to other nodes.\nA higher closeness centrality indicates a shorter distance relative to all other nodes. It helps to detect nodes who can spread information very efficiently within a network.\nA higher betweenness centrality measures the extent to which a particular node lies on the path between other nodes. Nodes with high betweenness can have significant influence within a network.\n\n\n\nShow code\nmc3_graph_all <- tbl_graph(nodes = mc3_nodes_all,\n                       edges = mc3_edges_all,\n                       directed = FALSE) %>%\n  mutate(betweenness_centrality = centrality_betweenness(),\n         closeness_centrality = centrality_closeness(),\n         degree_centrality = centrality_degree())\n\nmc3_graph_bo <- tbl_graph(nodes = mc3_nodes_bo,\n                       edges = mc3_edges_bo,\n                       directed = FALSE) %>%\n  mutate(betweenness_centrality = centrality_betweenness(),\n         closeness_centrality = centrality_closeness(),\n         degree_centrality = centrality_degree())\n\nmc3_graph_cc <- tbl_graph(nodes = mc3_nodes_cc,\n                          edges = mc3_edges_cc,\n                          directed = FALSE) %>%\n  mutate(betweenness_centrality = centrality_betweenness(),\n         closeness_centrality = centrality_closeness(),\n         degree_centrality = centrality_degree())\n\n\nThe code chunk below utilises as_tibble() to convert the three graphs (mc3_graph_all, mc3_graph_bo, mc3_graph_cc) into a tibble format.\n\n\nShow code\nmc3_graph_tibble_all <- as_tibble(mc3_graph_all)\nmc3_graph_tibble_bo <- as_tibble(mc3_graph_bo)\nmc3_graph_tibble_cc <- as_tibble(mc3_graph_cc)\n\n\n\n\n4.2.2 mc3_graph_all plot\n\n4.2.2.1 Degree Centrality (Company Contact Count)\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_all data set to the top 30%, 20% and 10% based on degree_centrality focusing on company contact count before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_all %>%\n  top_frac(0.30, wt = degree_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point(aes(\n    size = degree_centrality,\n    color = company_contact_count,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(3,10))+\n  scale_color_gradient(low = \"gray\", high = \"red\") +\n  theme_graph()\n\n\n\n\n\n\n4.2.2.2 Degree Centrality (Beneficial Owner Count)\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_all data set to the top 30%, 20% and 10% based on degree_centrality focusing on beneficial owner count before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_all %>%\n  top_frac(0.30, wt = degree_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point(aes(\n    size = degree_centrality,\n    color = beneficial_owner_count,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(3,10))+\n  scale_color_gradient(low = \"gray\", high = \"red\") +\n  theme_graph()\n\n\n\n\n\n\n4.2.2.3 Closeness Centrality (Company Contact Count)\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_all data set to the top 30%, 20% and 10% based on closeness_centrality focusing on company contact count before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_all %>%\n  top_frac(0.30, wt = closeness_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point(aes(\n    size = closeness_centrality,\n    color = company_contact_count,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(3,10))+\n  scale_color_gradient(low = \"gray\", high = \"red\") +\n  theme_graph()\n\n\n\n\n\n\n4.2.2.4 Closeness Centrality (Beneficial Owner Count)\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_all data set to the top 30%, 20% and 10% based on closeness_centrality focusing on beneficial owner count before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_all %>%\n  top_frac(0.30, wt = closeness_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point(aes(\n    size = closeness_centrality,\n    color = beneficial_owner_count,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(3,10))+\n  scale_color_gradient(low = \"gray\", high = \"red\") +\n  theme_graph()\n\n\n\n\n\n\n4.2.2.5 Betweenness Centrality (Company Contact Count)\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_all data set to the top 30%, 20% and 10% based on betweenness_centrality focusing on company contact count before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_all %>%\n  top_frac(0.30, wt = betweenness_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point(aes(\n    size = betweenness_centrality,\n    color = company_contact_count,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(3,10))+\n  scale_color_gradient(low = \"gray\", high = \"red\") +\n  theme_graph()\n\n\n\n\n\n\n4.2.2.6 Betweenness Centrality (Beneficial Owner Count)\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_all data set to the top 30%, 20% and 10% based on betweenness_centrality focusing on beneficial owner count before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_all %>%\n  top_frac(0.30, wt = betweenness_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point(aes(\n    size = betweenness_centrality,\n    color = beneficial_owner_count,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(3,10))+\n  scale_color_gradient(low = \"gray\", high = \"red\") +\n  theme_graph()\n\n\n\n\n\n\n\n4.2.3 mc3_graph_bo plot\n\n4.2.3.1 Degree Centrality\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_bo data set to the top 30%, 20% and 10% based on degree_centrality focusing on revenue quantile before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_bo %>%\n  top_frac(0.30, wt = degree_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link(aes(width = bo_target_count),\n                 alpha=0.5) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(\n    size = degree_centrality,\n    color = quantile,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()\n\n\n\n\n\n\n4.2.3.2 Closeness Centrality\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_bo data set to the top 30%, 20% and 10% based on closeness_centrality focusing on revenue quantile before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_bo %>%\n  top_frac(0.30, wt = closeness_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link(aes(width = bo_target_count),\n                 alpha=0.5) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(\n    size = closeness_centrality,\n    color = quantile,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()\n\n\n\n\n\n\n4.2.3.3 Betweenness Centrality\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_bo data set to the top 30%, 20% and 10% based on betweenness_centrality focusing on revenue quantile before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_bo %>%\n  top_frac(0.30, wt = betweenness_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link(aes(width = bo_target_count),\n                 alpha=0.5) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(\n    size = betweenness_centrality,\n    color = quantile,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()\n\n\n\n\n\n\n\n4.2.4 mc3_graph_cc plot\n\n4.2.4.1 Degree Centrality\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_cc data set to the top 30%, 20% and 10% based on degree_centrality focusing on revenue quantile before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_cc %>%\n  top_frac(0.30, wt = degree_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link(aes(width = cc_target_count),\n                 alpha=0.5) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(\n    size = degree_centrality,\n    color = quantile,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()\n\n\n\n\n\n\n4.2.4.2 Closeness Centrality\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_cc data set to the top 30%, 20% and 10% based on closeness_centrality focusing on revenue quantile before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_cc %>%\n  top_frac(0.30, wt = closeness_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link(aes(width = cc_target_count),\n                 alpha=0.5) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(\n    size = closeness_centrality,\n    color = quantile,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()\n\n\n\n\n\n\n4.2.4.3 Betweenness Centrality\nWithin this section, we leveraged on top_frac to further filter the mc3_graph_cc data set to the top 30%, 20% and 10% based on betweenness_centrality focusing on revenue quantile before determining the appropriate threshold for our subsequent analysis.\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_cc %>%\n  top_frac(0.30, wt = betweenness_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link(aes(width = cc_target_count),\n                 alpha=0.5) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(\n    size = betweenness_centrality,\n    color = quantile,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()\n\n\n\n\n\n\n\n\n4.3 Computing Degree, Betweenness & Closeness Centrality to store within mc3_nodes_all\nWithin this section, we compute degree, betweenness and closeness centrality metrics and store it within mc3_nodes_all for easy reference in our subsequent analysis.\n\n\nShow code\nclosenesscentrality <- closeness(mc3_graph_all, mode = \"all\")\nmc3_nodes_all <- mc3_nodes_all %>%\n  mutate(closenesscentrality = closenesscentrality)\n\ndegreecentrality <- degree(mc3_graph_all, mode = \"all\")\nmc3_nodes_all <- mc3_nodes_all %>%\n  mutate(degreecentrality = degreecentrality)\n\nbetweennesscentrality <- betweenness(mc3_graph_all, directed = FALSE)\nmc3_nodes_all <- mc3_nodes_all %>%\n  mutate(betweennesscentrality = betweennesscentrality)\n\n\n\n\n4.4 Filtered Thresholds (by degree centrality)\nAfter reviewing the above plots in section 4.2, we are of the view to gain a deeper insight via the top 20% nodes in terms of degree centrality.\nThis is because nodes with a higher degree centrality will have a high number of interacting neighbours. This might potentially help us to identify anomalies within the knowledge graph as companies who are involved in illegal fishing are more likely to have more beneficial owners.\nAccording to an article “Fishy networks: Uncovering the companies and individuals behind illegal fishing globally”, it indicated that unscrupulous operators of vessels involved in IUU fishing takes advantage of a lack of regulations by using complex ownership structures to hide the identities of their ultimate beneficial owners (UBOs). Therefore, a company with more beneficial owners (and correspondingly a higher degree centrality) might be worth looking into further.\nLikewise, illegal fishing typically involve complex network amongst various actors such as fishing companies, wholesalers and suppliers etc. Therefore, a higher number of company contacts can provide illegal fishing companies with access to valuable resources, markets and revenues to enable their illegal activtities. Therefore, it is worthwhile looking a high degree centrality perspective in relation to company contacts.\nThe below code chunk defines the various threshold for mc3_graph_all, mc3_graph_bo and mc3_graph_cc by the top 20% degree centrality.\n\n\nShow code\nmc3_graph_all_degree <- mc3_graph_all %>%\n  top_frac(0.20, wt = degree_centrality)\n\nmc3_graph_bo_degree <- mc3_graph_bo %>%\n  top_frac(0.20, wt = degree_centrality)\n\nmc3_graph_cc_degree <- mc3_graph_cc %>%\n  top_frac(0.20, wt = degree_centrality)\n\n\nThe below code chunk saves the respective graphs to a RDS file for subsequent usage.\n\n\nShow code\nwrite_rds(mc3_graph_all_degree, \"data/mc3_graph_all_degree.rds\")\nwrite_rds(mc3_graph_bo_degree, \"data/mc3_graph_bo_degree.rds\")\nwrite_rds(mc3_graph_cc_degree, \"data/mc3_graph_cc_degree.rds\")\n\n\n\n\n4.5 Preparing the edge and nodes for graph plotting\nNote that tidygraph model is in R list format. The code chunk below will be used to extract and convert the edges into a tibble data frame.\n\nactivate() is used to make the edges of mc3_graph_all/bo/cc_degree active. This is necessary in order to extract the correct component from the list object.\nas.tibble() is used to convert the edges list into tibble data frame.\n\n\n\nShow code\nedges_df_all <- mc3_graph_all_degree %>%\n  activate(edges) %>%\n  as.tibble()\n\nedges_df_bo <- mc3_graph_bo_degree %>%\n  activate(edges) %>%\n  as.tibble()\n\nedges_df_cc <- mc3_graph_cc_degree %>%\n  activate(edges) %>%\n  as.tibble()\n\n\nThe below code chunk serves to prepare a nodes tibble data frame.\n\nactivate() is used to make the edges of mc3_graph_all/bo/cc_degree active. This is necessary in order to extract the correct component from the list object.\nas.tibble() is used to convert the edges list into tibble data frame.\nrename() is used to rename the field name id to label.\nmutate() is used to create a new field called id and row_number() is used to assign the row number into id values.\nselect() is used to re-organised the field name. This is because visNetwork is expecting the first field is called id and the second field is called label.\n\n\n\nShow code\nnodes_df_all <- mc3_graph_all_degree %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, country, type, product_services, revenue_omu, company_contact_count, beneficial_owner_count, quantile)\n\nnodes_df_bo <- mc3_graph_bo_degree %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, country, type, product_services, revenue_omu, quantile)\n\nnodes_df_cc <- mc3_graph_cc_degree %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, country, type, product_services, revenue_omu, quantile)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#references",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#references",
    "title": "Take-home Exercise 3",
    "section": "8. References",
    "text": "8. References\n\nhttps://financialtransparency.org/fishy-networks-challenges-uncovering-beneficial-owners-behind-illegal-fishing-global-south-countries/\nhttps://rpubs.com/neloe/ggraph_intro\nhttps://combattb.org/combat-tb-neodb/graph-algorithms/\nhttps://www.theguardian.com/environment/2022/oct/26/illegal-fishing-billions-losses-developing-countries\nhttps://maritime-executive.com/article/report-the-corporate-owners-behind-illegal-fishing\nhttps://financialtransparency.org/reports/fishy-networks-uncovering-companies-individuals-behind-illegal-fishing-globally"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#filter-in-the-edges-to-contain-only-those-in-nodes",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#filter-in-the-edges-to-contain-only-those-in-nodes",
    "title": "Take-home Exercise 3",
    "section": "filter in the edges to contain only those in nodes",
    "text": "filter in the edges to contain only those in nodes\nThis step is necessary to ensure that the nodes in the mc3_nodes_bo include all the source and target values from mc3_edges_bo.\n\nid1_bo <- mc3_edges_bo %>%\n  select(source) %>%\n  rename(id = source)\nid2_bo <- mc3_edges_bo %>%\n  select(target) %>%\n  rename(id = target)\nmc3_nodes_bo <- rbind(id1_bo, id2_bo) %>%\n  distinct() %>%\n  left_join(mc3_nodes_fish, by = \"id\",\n            unmatched = \"drop\")\n\n\n# Find the range of revenue_omu\nrevenue_range <- range(mc3_nodes_bo$revenue_omu, na.rm = TRUE)\ncat(\"Range of revenue_omu:\", revenue_range[1], \"-\", revenue_range[2])\n\nRange of revenue_omu: 4666.673 - 291426453\n\n\n\n# Plot histogram of revenue_omu\nlibrary(scales)\nggplot(mc3_nodes_bo, aes(x = revenue_omu)) +\n  geom_histogram(fill = \"lightblue\", color = \"black\", bins = 20) +\n  xlab(\"Revenue (OMU)\") +\n  ylab(\"Count\") +\n  ggtitle(\"Distribution of Revenue (OMU)\")+\n  scale_x_continuous(labels = comma) \n\n\n\n\n\n# Count records in each quantile\nmc3_nodes_bo <- mc3_nodes_bo %>%\n  mutate(quantile = ntile(revenue_omu, 4))\n\nquantile_counts_bo <- mc3_nodes_bo %>%\n  group_by(quantile) %>%\n  summarise(records = n())\n\nquantile_counts_bo\n\n# A tibble: 5 × 2\n  quantile records\n     <int>   <int>\n1        1     102\n2        2     102\n3        3     101\n4        4     101\n5       NA    1830\n\n\n\n4.2 Preparing mc3_nodes_cc to only contain the nodes found within mc3_edges_cc\nThis step is necessary to ensure that the nodes in the mc3_nodes_cc include all the source and target values from mc3_edges_cc.\n\nid1_cc <- mc3_edges_cc %>%\n  select(source) %>%\n  rename(id = source)\nid2_cc <- mc3_edges_cc %>%\n  select(target) %>%\n  rename(id = target)\nmc3_nodes_cc <- rbind(id1_cc, id2_cc) %>%\n  distinct() %>%\n  left_join(mc3_nodes_fish,\n            unmatched = \"drop\")\n\n\n# Find the range of revenue_omu\nrevenue_range <- range(mc3_nodes_cc$revenue_omu, na.rm = TRUE)\ncat(\"Range of revenue_omu:\", revenue_range[1], \"-\", revenue_range[2])\n\nRange of revenue_omu: 4666.673 - 131450837\n\n\n\n# Plot histogram of revenue_omu\nlibrary(scales)\nggplot(mc3_nodes_cc, aes(x = revenue_omu)) +\n  geom_histogram(fill = \"lightblue\", color = \"black\", bins = 20) +\n  xlab(\"Revenue (OMU)\") +\n  ylab(\"Count\") +\n  ggtitle(\"Distribution of Revenue (OMU)\")+\n  scale_x_continuous(labels = comma) \n\n\n\n\n\n# Count records in each quantile\nmc3_nodes_cc <- mc3_nodes_cc %>%\n  mutate(quantile = ntile(revenue_omu, 4))\n\nquantile_counts_cc <- mc3_nodes_cc %>%\n  group_by(quantile) %>%\n  summarise(records = n())\n\nquantile_counts_cc\n\n# A tibble: 5 × 2\n  quantile records\n     <int>   <int>\n1        1     122\n2        2     122\n3        3     122\n4        4     122\n5       NA     661\n\n\n\n\n4.3 Building the tidy graph data model\n\n4.3.1 Using tbl_graph() to build a tidygraph data model for mc3_graph_bo\n\nmc3_graph_bo <- tbl_graph(nodes = mc3_nodes_bo,\n                       edges = mc3_edges_bo,\n                       directed = FALSE) %>%\n  mutate(betweenness_centrality = centrality_betweenness(),\n         closeness_centrality = centrality_closeness(),\n         degree_centrality = centrality_degree())\n\n\n\n4.3.2 Using tbl_graph() to build a tidygraph data model for mc3_graph_cc\n\nmc3_graph_cc <- tbl_graph(nodes = mc3_nodes_cc,\n                          edges = mc3_edges_cc,\n                          directed = FALSE) %>%\n  mutate(betweenness_centrality = centrality_betweenness(),\n         closeness_centrality = centrality_closeness(),\n         degree_centrality = centrality_degree())\n\n\nmc3_graph_tibble_bo <- as_tibble(mc3_graph_bo)\nmc3_graph_tibble_cc <- as_tibble(mc3_graph_cc)\n\n\n\n4.3.3 mc3_graph_bo plot based on degree centrality\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_bo %>%\n  top_frac(0.30, wt = degree_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link(aes(width = bo_target_count),\n                 alpha=0.5) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(\n    size = degree_centrality,\n    color = revenue_omu,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()\n\n\n\n\n\n\n4.3.4 mc3_graph_cc plot based on betweeness centrality\n\nTop 30%Top 20%Top 10%Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nmc3_graph_cc %>%\n  top_frac(0.30, wt = degree_centrality) %>% \nggraph(layout = \"fr\") +\n  geom_edge_link(aes(width = cc_target_count),\n                 alpha=0.5) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(\n    size = degree_centrality,\n    color = revenue_omu,\n    alpha = 0.1)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()\n\n\n\n\n\nmc3_graph_bo <- mc3_graph_bo %>%\n  top_frac(0.20, wt = degree_centrality)\n\nmc3_graph_cc <- mc3_graph_cc %>%\n  top_frac(0.20, wt = betweenness_centrality)\n\n\nwrite_rds(mc3_graph_bo, \"data/mc3_graph_bo.rds\")\nwrite_rds(mc3_graph_cc, \"data/mc3_graph_cc.rds\")\n\n\n\n\n4.4 Preparing the edge and nodes for graph plotting\n\nedges_df_bo <- mc3_graph_bo %>%\n  activate(edges) %>%\n  as.tibble()\n\n\nnodes_df_bo <- mc3_graph_bo %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, country, type, product_services, revenue_omu, quantile)\n\n\nedges_df_cc <- mc3_graph_cc %>%\n  activate(edges) %>%\n  as.tibble()\n\n\nnodes_df_cc <- mc3_graph_cc %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  rename(label = id) %>%\n  mutate(id=row_number()) %>%\n  select(id, label, country, type, product_services, revenue_omu, quantile)\n\n\n\n5. Interactive Network Visualisation\n\n5.1 Company by beneficial owners\n\nBy Revenue QuantilesBy countriesCode Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nid_node <- sort(nodes_df_bo$id) # for the id nodes dropdown box\n\nvis_plot_interactive_country_bo <- visNetwork(nodes = nodes_df_bo, edges = edges_df_bo) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,\n                  physics = TRUE       \n                ) %>%\nvisNodes(color = list(highlight = list(border = 'red', background = 'yellow', size = 50))) %>%\n  visEdges(color = list(highlight = \"black\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"country\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = id_node)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), \n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 1) #0.25\n\nvis_plot_interactive_country_bo\n\n\n\n\n\n\n5.2 Company by company contacts\n\nBy Revenue QuantilesBy CountryCode Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nid_node <- sort(nodes_df_cc$id) # for the id nodes dropdown box\n\nvis_plot_interactive_country_cc <- visNetwork(nodes = nodes_df_cc, edges = edges_df_cc) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,\n                  physics = TRUE       \n                ) %>%\nvisNodes(color = list(highlight = list(border = 'red', background = 'yellow', size = 50))) %>%\n  visEdges(color = list(highlight = \"black\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"country\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = id_node)) %>%\n  visLegend(width = 0.1) %>%\n  visPhysics(repulsion = list(springlength = 50), \n             maxVelocity = 2,\n             solver = \"forceAtlas2Based\",\n             forceAtlas2Based = list(gravitationalConstant = -1000),\n             timestep = 1) #0.25\n\nvis_plot_interactive_country_cc\n\n\n\n\n#zoom into ego network by company perspective (to see who owns it and how many contacts it has) #zoom into owner perspective (to see how diversified the ownership is)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#mc3-challenge-question-1s-answer",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#mc3-challenge-question-1s-answer",
    "title": "Take-home Exercise 3",
    "section": "6. MC3 Challenge Question 1’s Answer",
    "text": "6. MC3 Challenge Question 1’s Answer\nQuestion: Use visual analytics to identify anomalies in the business groups present in the knowledge graph.\nFrom the image below, we observe that:\n\nmost beneficial owners usually own only one company\nmost company contacts are usually only linked to one company.\n\n\nIt is useful to deep dive into countries with a high number of companies involved in fishing related activities (ZH and Oceanus from the below plot). In a study by Financial Transparency Coalition, fishing vessels flagged to Asia (particularly China), were found to have the world's largest distant water fleet, with 54.7% tagged as IUU fishing (The Guardian, 2022). Hence, focusing on a higher number of fleet occurrence in a country might potentially help detect IUU fishing.\n\nWhile our interactive network visualisation focuses on the top 20% degree centrality, it is important/relevant for us to also compute closeness and betweenness centrality.\n\nHigher degree centrality indicates that the node has more connections than the average number of connections as compared to other nodes.\nHigher closeness centrality indicates a shorter distance relative to all other nodes.\nHigher betweenness centrality measures the extent to which a particular node lies on the path between other nodes.\n\nRevenue was divided into four main quantiles, with a scale of 1 (lowest revenue) to 4 (highest revenue).\nAccording to an article “Fishy networks: Uncovering the companies and individuals behind illegal fishing globally”, vessels operators involved in IUU fishing were taking advantage on the lack of regulations by using complex ownership structures to hide the identities of their ultimate beneficial owners. Therefore, a company with more beneficial owners (and correspondingly a higher degree centrality) is worthy to investigate.\nLikewise, illegal fishing typically involves various actors such as fishing companies, wholesalers and suppliers etc. A higher number of company contacts can provide illegal fishing companies with access to valuable resources and revenues to enable their illegal activities. Therefore, it is worthwhile to look from a high degree centrality perspective in relation to company contacts.\nLeveraging on the interactive plots in Section 5, we focused on companies’ connection to beneficial owners and company contacts respectively. Using “Congo Rapids   Ltd. Corporation” as an example, we can see that it has connections to several beneficial owners.\n\n\n\nShow code\nDT::datatable(mc3_nodes_all)\n\n\n\n\n\n\n\nHowever, when we utilised DT::datatable(mc3_nodes_all) to gain an insight into the other centrality measures. It appears to have relatively low closeness centrality (0.0166667) but high betweenness centrality (1770). This suspicious company is owned by several beneficial owners (possibly to mask the ultimate beneficial owner), with low direct proximity to other companies (indicating a “relatively closed off network”). Yet, it serves as a significant intermediary between other companies in the flow of illegal fishing in the broader network. Hence, the said company can potentially be acting as a bridge to facilitate the illegal fishing network.\n\nNext, we look at “By Country” perspective. Zooming into Oceanus, which has the second highest count in terms of fishing related companies, we detected “Aqua Aura SE Marine life” as having a lot of company contacts. Companies involved in illegal fishing might potentially be more prone to having a few company contacts to enable a higher revenue stream.\n\n\n\nShow code\nDT::datatable(mc3_nodes_all)\n\n\n\n\n\n\n\nUtilising DT::datatable(mc3_nodes_all), we found that the said company has presence in two different countries. The first record in the table below was the one picked up via the network graph. Likewise, it has a high degree and betweenness centrality but low closeness centrality, hence making it a suspicious company potentially involved in illegal fishing.\n\nFinally, focusing on “Revenue Quantile - 4”, this initially pointed our attention to”Zambezi Gorge Incorporated Consulting”. A closer look reveals that Adam Johnson is a rather suspicious node as it owns several companies (unlike most beneficial owner who usually owns 1 company)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#reflections",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#reflections",
    "title": "Take-home Exercise 3",
    "section": "7. Reflections",
    "text": "7. Reflections\nSimilar to Take Home Exercise 2, the most difficult aspect of working with this graph is in relation to the steep learning curve in relation to coding and computing resources. While I have an idea of going about analyzing or wrangling the data, it is difficult for me to put it into action as I am just beginning to be more familiar with R.\nThankfully, Professor Kam extended the submission due date and he was always open to taking time out for consultation. Professor Kam also helped me and the class to get up to speed by explaining data wrangling concepts and guiding me along the way in terms of framing my thoughts to enable me to complete the assignment.\nI learnt a lot from this take-home exercise but as always, I’m sure there is more to learn in this journey."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#interactive-network-visualisation",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#interactive-network-visualisation",
    "title": "Take-home Exercise 3",
    "section": "5. Interactive Network Visualisation",
    "text": "5. Interactive Network Visualisation\nWithin Section 5, we have:\n\nSection 5.1: This section focuses on looking at the interactive network visualisation on an overall basis by revenue quantiles and country\nSection 5.2: This section focuses on looking at the interactive network visualisation specifically on beneficial owner relationship with Company by revenue quantiles and country\nSection 5.3: This section focuses on looking at the interactive network visualisation specifically on company contact relationship with Company by revenue quantiles and country\n\nNote that the analysis can be done either by countries or revenue quantiles, where the objective is to find the node with a higher degree centrality.\n\n5.1 Company by Overall View\n\nBy Revenue QuantilesBy CountryCode Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nid_node <- sort(nodes_df_all$id) # for the id nodes dropdown box\n\nvis_plot_interactive_quantiles_all <- visNetwork(nodes = nodes_df_all, edges = edges_df_all) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,\n                  physics = TRUE       \n                ) %>%\nvisNodes(color = list(highlight = list(border = 'red', background = 'yellow', size = 50))) %>%\n  visEdges(color = list(highlight = \"black\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"quantile\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = id_node)) %>%\n  visLegend(width = 0.1)\n\nvis_plot_interactive_quantiles_all\n\n\n\n\n\n\n5.2 Company by Beneficial Owners\n\nBy Revenue QuantilesBy CountryCode Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nid_node <- sort(nodes_df_bo$id) # for the id nodes dropdown box\n\nvis_plot_interactive_quantiles_bo <- visNetwork(nodes = nodes_df_bo, edges = edges_df_bo) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,\n                  physics = TRUE       \n                ) %>%\nvisNodes(color = list(highlight = list(border = 'red', background = 'yellow', size = 50))) %>%\n  visEdges(color = list(highlight = \"black\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"quantile\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = id_node)) %>%\n  visLegend(width = 0.1)\n\nvis_plot_interactive_quantiles_bo\n\n\n\n\n\n\n5.3 Company by Company Contacts\n\nBy Revenue QuantilesBy CountryCode Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\nid_node <- sort(nodes_df_cc$id) # for the id nodes dropdown box\n\nvis_plot_interactive_quantiles_cc <- visNetwork(nodes = nodes_df_cc, edges = edges_df_cc) %>%\n  visIgraphLayout(layout = \"layout_with_fr\", \n                  smooth = FALSE,\n                  physics = TRUE       \n                ) %>%\nvisNodes(color = list(highlight = list(border = 'red', background = 'yellow', size = 50))) %>%\n  visEdges(color = list(highlight = \"black\"), arrows = 'to', \n           smooth = list(enabled = TRUE, type = \"curvedCW\")) %>%\n  visOptions(selectedBy = \"quantile\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = list(enabled = TRUE,\n                                     values = id_node)) %>%\n  visLegend(width = 0.1)\n\nvis_plot_interactive_quantiles_cc"
  }
]